\input{header}


\begin{document}

\title{Sparse Coding of Images}
% \subtitle{...progress report}
\author{Sebastian Szczepanski}
\institute[TU Berlin]{TU Berlin Computer Graphics}
\date{\today}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents %[pausesections]
\end{frame}


\section{motivation}
\subsection{improve compression quality}
\begin{frame}
\begin{itemize}
\item basic idea for lossy compression
	\begin{itemize}
	\item  conversion, sparse base transform, quantization, encoding
	\end{itemize}
\pause
\item improve sparse representation quality of natural images 
\pause
\item how?
\pause
	\begin{itemize}
	\item rather than a small designed dictionary (DCT, DWT, other orthonormal bases)	
	\pause
	\item we select a small number of atoms from a large redundant dictionary (sparse coding)
	\end{itemize}
\end{itemize}
\end{frame}

\section{sparse coding}

%\begin{frame}
%for orthonomal bases it's the same but for dict. based redundant overcomplete/overdetermined?
%it gets computational compelex
%\end{frame}


\subsection{the idea}
\begin{frame}
% why not PCA? leads to sparse dictionary but not to sparse decomposition
(picture of matrix an sparse code vector)
\[
\underbrace{\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}}_{signal} \approx \underbrace{\begin{pmatrix} d_1  d_2 \cdots d_n \end{pmatrix}}_{\textrm{dictionary}}
\underbrace{\begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n \end{pmatrix}}_{\textrm{keep sparse}}
\]
\begin{itemize}
\item non arthonormal base/atom transform with a sparse coeficent vector
\item compression, noisefind a linear combination of a “few” atoms from
D that is “close” to the signal x
\item usage reduction, classification, impainting,  superresolution
\end{itemize}

\end{frame}

%\subsection{origin}
\begin{frame}
..Sparse coding was originally developed for studying how neurons in the brain responded to visuals. It works by breaking down an image—for simplicity's sake, usually one in grayscale—into mathematical functions, pixel by pixel. The images that are broken down are just small patches of whole works, not much more than a dozen pixels square...
>>Olshausen and Field [391] observed that these
dictionary vectors learned from natural images also look similar to the impulse
response of simple cell neurons in the visual cortical area V1. It supports the idea
of a biological adaptation to process visual scenes. <<
\end{frame}

\begin{frame}
\begin{itemize}
\item \cite{Chen1999} and \cite{Aharon2006} have shown that learnd dictionaries show better compression quality
than small designed dictionaries (wavelets etc.)
\pause
		\begin{itemize}
		\item if dictionary is good enough :)
  		\end{itemize}

% as shown in >Sparse and redundant modeling of image content using an image-signature-dictionary<
\item but choosing atoms from large dictionaries can lead to high computational complexity
\end{itemize}
\end{frame}

\subsection{a closer look}
\begin{frame}
Small othonormal bases like wavelets,cosin,bandelets are >easy< to solve but what about big redundant dictionarys?
>>Finding such an exact decomposition for any , if it exists, is an NP-hard three-sets
covering problem. Indeed,the choice of one element in the solution influences the
choice of all others,which essentially requires us to try all possibilities.<<
%\end{block}
\end{frame}






\subsection{the problem}
\begin{frame}
sparse decomposition problem -> np-hard
\[ 
\min_{\alpha\in\mathbb{R}^{p}} f(a) + \underbrace{\psi(\alpha)}_{regularization} 
\]
\[
\min_{\alpha\in\mathbb{R}^{p}} \frac{1}{2} \lVert x - D\alpha \rVert^{2}_{2} + \underbrace{\psi(\alpha)}_{regularization}
\] 

how does this regularization look like? \\
l0-norm l1-norm

\end{frame}

\subsection{the solutions}
\begin{frame}
heuristics
\begin{itemize}
% \item cholesky decomposition
\item (orthogonal) matching pursuit
\item LARS-Lasso
\end{itemize}
\end{frame}


\begin{frame}
(matching persuit desc.)
\end{frame}

\subsubsection*{LARS}
\begin{frame}
(LARS-Lasso desc.)
\end{frame}



\subsection{problems (still)}
\begin{frame}
color bleeding as in >Learning High-Order MRF Priors of Color Images< and >Sparse learned representations for image restoration<
(picture from paper)
Why?
\end{frame}

\section{dictionaries}
\subsection{building}
\begin{frame}
\begin{itemize}
\item But what about a universal basis for natural images? 
\item How many elements for a 'good' sparse representation?
\item take all possible elements (fixed compression rate, gigantic data)
\item learn a dictionary until convergence of psnr of new images
\item how to find this basis? (online training with constant addition of new images)
\end{itemize}
\end{frame}

\begin{frame}
\begin{itemize}
\item neural network (see >Learning Multiple Layers of Features from Tiny Images<)
\item trainig algorithms
\end{itemize}
\end{frame}

\begin{frame}
(dict learn pseudocode)
\end{frame}

\subsubsection*{convergence?}
\begin{frame}
\end{frame}

\subsection{results}
\begin{frame}
(sample pictures compression) \\
(psnr graph with 100 / 1000 / 10.000 images) \\
(diff block sizes 8 12 16) \\
\end{frame}

\begin{frame}
(conclusion)
thanks to modern greedy algos. and online learning
sparse coding became a solvable problem

\end{frame}


\section{refenrences}
\begin{frame}
\bibliographystyle{alpha}
\bibliography{bibtex/dersebu}
\nocite{*}


\end{frame}

\end{document}