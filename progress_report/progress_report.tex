\input{header}


\begin{document}

\title{Sparse Coding of Images}
% \subtitle{...progress report}
\author{Sebastian Szczepanski}
\institute[TU Berlin]{TU Berlin Computer Graphics}
\date{\today}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents %[pausesections]
\end{frame}


\section{motivation}
\subsection{signal compression}
\begin{frame}
how? 

\begin{itemize}
\item loseless (clever encoding) 
\item lossy (approximation) (conversion, basis transform, quantization, encoding) \
\pause
	\begin{itemize}
	\item  keep the number of bases vector small (DCT, DWT, other orthonormal bases)	
	\pause
	\item constrain the number of elements in large redundant dictionaries (sparse coding)
	\end{itemize}
\end{itemize}
\end{frame}

\section{sparse coding}

%\begin{frame}
%for orthonomal bases it's the same but for dict. based redundant overcomplete/overdetermined?
%it gets computational compelex
%\end{frame}


\subsection{the idea}
\begin{frame}
chen1999 and erlandAharon2006 have shown that learn bases show better compression quality
than ... wavelets 
% as shown in >Sparse and redundant modeling of image content using an image-signature-dictionary<
\end{frame}

\subsection{basics}
\begin{frame}
% why not PCA? leads to sparse dictionary but not to sparse decomposition
(picture of matrix an sparse code vector)
\[
\underbrace{\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}}_{signal} \approx \underbrace{\begin{pmatrix} d_1  d_2 \cdots d_n \end{pmatrix}}_{\textrm{dictionary}}
\underbrace{\begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n \end{pmatrix}}_{\textrm{keep sparse}}
\]
non arthonormal base/atom transform with a sparse coeficent vector
compression, noise reduction, classification, impainting,  superresolution

\end{frame}

\subsection{origin}
\begin{frame}
..Sparse coding was originally developed for studying how neurons in the brain responded to visuals. It works by breaking down an image—for simplicity's sake, usually one in grayscale—into mathematical functions, pixel by pixel. The images that are broken down are just small patches of whole works, not much more than a dozen pixels square...
>>Olshausen and Field [391] observed that these
dictionary vectors learned from natural images also look similar to the impulse
response of simple cell neurons in the visual cortical area V1. It supports the idea
of a biological adaptation to process visual scenes. <<
\end{frame}

\begin{frame}
Small othonormal bases like wavelets,cosin,bandelets are >easy< to solve but what about big redundant dictionarys?
>>Finding such an exact decomposition for any , if it exists, is an NP-hard three-sets
covering problem. Indeed,the choice of one element in the solution influences the
choice of all others,which essentially requires us to try all possibilities.<<

%\end{block}
\end{frame}

\subsection{the problem}
\begin{frame}
\[ 
\min_{\alpha\in\mathbb{R}^{p}} f(a) + \underbrace{\psi(\alpha)}_{regularization} 
\]
\[
\min_{\alpha\in\mathbb{R}^{p}} \frac{1}{2} \lVert x - D\alpha \rVert^{2}_{2} + \underbrace{\psi(\alpha)}_{regularization}
\] 
how does this regularization look like? \\
l1-norm

\end{frame}

\subsection{the solutions}
\begin{frame}
heuristics 
\begin{itemize}
% \item cholesky decomposition
\item (orthogonal) matching pursuit
\item homotopy, LARS-lasso
\item K-SVD
\end{itemize}
\end{frame}


\begin{frame}
(matching persuit desc.)
\end{frame}

\subsubsection*{LARS}
\begin{frame}
(LARS-lasso desc.)
\end{frame}


\subsection{problems (still)}
\begin{frame}
color bleeding as in >Learning High-Order MRF Priors of Color Images< and >Sparse learned representations for image restoration<
(picture from paper)
Why?
\end{frame}

\section{dictionaries}
\subsection{building}
\begin{frame}
* But what about a universal basis for natural images? 
* How many elements for a 'good' sparse representation?
** take all possible elements (fixed compression rate, gigantic data)
** learn a dictionary until approximation of new images is good enough
* how to find this basis?
\end{frame}

\begin{frame}
*neural network (see >Learning Multiple Layers of Features from Tiny Images<)
*trainig algorithms
\end{frame}

\subsection{results}
\begin{frame}
\end{frame}

\subsection{problems}

\section{refenrences}
\begin{frame}
\bibliographystyle{alpha}
\bibliography{bibtex/dersebu}
\nocite{}


\end{frame}

\end{document}