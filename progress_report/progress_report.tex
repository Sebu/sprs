\input{header}


\begin{document}

\title{Sparse Coding of Images}
% \subtitle{...progress report}
\author{Sebastian Szczepanski}
\institute[TU Berlin]{TU Berlin Computer Graphics}
\date{\today}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Outline}
\tableofcontents %[pausesections]
\end{frame}


\section{motivation}
\begin{frame}
\begin{itemize}
\item improve lossy compression quality
\item basic idea for lossy compression of natural images
	\begin{itemize}
	\item  conversion, sparse base transform, quantization, encoding
	\end{itemize}
\pause
\item improve sparse representation quality of natural images 
\pause
\item how?
\pause
	\begin{itemize}
	\item rather than a small designed dictionary (DCT, DWT, other orthonormal bases)	
	\pause
	\item select a small number of atoms from a large redundant dictionary (sparse coding)
	\end{itemize}
\end{itemize}
\end{frame}

\section{sparse coding}

%\begin{frame}
%for orthonomal bases it's the same but for dict. based redundant overcomplete/overdetermined?
%it gets computational compelex
%\end{frame}


\subsection{the idea}
\begin{frame}
% why not PCA? leads to sparse dictionary but not to sparse decomposition
%(picture of matrix an sparse code vector)
\[
\underbrace{\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}}_{signal} \approx \underbrace{\begin{pmatrix} d_1  d_2 \cdots d_n \end{pmatrix}}_{\textrm{dictionary}}
\underbrace{\begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n \end{pmatrix}}_{\textrm{keep sparse}}
\]
\begin{itemize}
\item a linear combination of a “few” non orthonormal atoms from
D that is “close” to the signal X
\item try to keep coeficent vector sparse
\pause
\item usage: reduction, classification, impainting,  superresolution
\end{itemize}

\end{frame}

%\subsection{origin}
\begin{frame}
\begin{itemize}
\item originally developed for studying how neurons in the brain responded to visuals
\pause
\item break image down into patches
\pause
\item decompose patch into activities of a few neurons
%\item see Olshausen and Field 
\end{itemize}
%..Sparse coding was originally developed for studying how neurons in the brain responded to visuals. It works by breaking down an image—for simplicity's sake, usually one in grayscale—into mathematical functions, pixel by pixel. The images that are broken down are just small patches of whole works, not much more than a dozen pixels square...
%>>Olshausen and Field [391] observed that these dictionary vectors learned from natural images also look similar to the impulse response of simple cell neurons in the visual cortical area V1. It supports the idea of a biological adaptation to process visual scenes. <<
\end{frame}

\begin{frame}
\begin{itemize}
\item \cite{Chen1998Atomic} and \cite{Aharon2006KSVD} have shown that learnd dictionaries show better compression quality
than small designed dictionaries (cosin, wavelets, bandelets, curvelets etc.)
\pause
		\begin{itemize}
		\item if dictionary is good enough :)
  		\end{itemize}
\pause
% as shown in >Sparse and redundant modeling of image content using an image-signature-dictionary<
\item but choosing atoms from large dictionaries can lead to high computational complexity
\end{itemize}
\end{frame}

\subsection{a closer look}
\begin{frame}
\begin{itemize}
\item small othonormal bases like wavelets,cosin,bandelets are 'easy' to solve but what about big redundant dictionarys?
\pause
\item sparse decomposition with redundant dictionaries is a NP-hard problem
\begin{itemize}
\item to get best result you need to test every combination
\end{itemize}
%>>Finding such an exact decomposition for any , if it exists, is an NP-hard three-sets covering problem. Indeed,the choice of one element in the solution influences the choice of all others,which essentially requires us to try all possibilities.<<
\end{itemize}
\end{frame}






\subsection{the problem}
\begin{frame}
keep alpha sparse
\[ 
\min_{\alpha\in\mathbb{R}^{p}} \underbrace{f(\alpha)}_{approximation} + \underbrace{\psi(\alpha)}_{regularization} 
\]
\pause
linear least squares with regularization for sparsity 
\[
\min_{\alpha\in\mathbb{R}^{p}} \frac{1}{2} \lVert x - D\alpha \rVert^{2}_{2} + \underbrace{\psi(\alpha)}_{regularization}
\] 
how does this regularization look like? 
\begin{itemize}
\item $\ell_0$-Norm 
\item $\ell_1$-Norm
\end{itemize}


\end{frame}

\subsection{the solutions}
\begin{frame}
heuristics
\begin{itemize}
% \item cholesky decomposition
\item (orthogonal) matching pursuit
\item LARS-Lasso \cite{Osborne2000New}
\end{itemize}
\end{frame}


\begin{frame}
(matching persuit description)
\end{frame}

\subsubsection*{LARS}
\begin{frame}
(LARS-Lasso description)
\end{frame}



\subsection{(still) problems}
\begin{frame}
multi-channel signals
\begin{itemize}
\item code every channel separate or combine them into single signal
\pause
\item  can lead to color bleeding as shown in \cite{mairal08sparse} %and \cite{Learning High-Order MRF Priors of Color Images}
\item (picture from paper)
\pause
\item Why?
	\begin{itemize}
	\item color channels can be highly correlated 
	\item combined channels show better results
	\end{itemize}
\end{itemize}
\end{frame}


\section{dictionaries}
\begin{frame}
\begin{itemize}
\item what about a universal basis for natural images? 
\pause
\item how many elements for a 'good' sparse representation?
\pause
	\begin{itemize}
	\item take all possible elements (fixed compression rate, gigantic data)
	\pause
	\item learn a dictionary until convergence of error for new images
	\end{itemize}
\end{itemize}
\end{frame}

\subsection{building}
\begin{frame}
how to find this basis? 
\pause
\begin{itemize}
\item neural network \cite{Krizhevsky2009Learning}
\item online training with constant addition of new images \cite{Mairal2010Online}
\end{itemize}
\end{frame}

\begin{frame}
(dict learn pseudocode)
\end{frame}

%\subsubsection*{convergence?}
%\begin{frame}
%\end{frame}

\subsection{results}
\begin{frame}
(compression of sample pictures vs. DCT, DWT) \\
(graph: average psnr vs. Nr. of training batches 100 / 1000 / 10.000) \\
(graph: psnr for different block sizes 8 / 12  / 16) \\
\end{frame}

\begin{frame}
(conclusion)
%thanks to modern greedy algos. and online learning
%sparse coding became a solvable problem

\end{frame}


\section{refenrences}
%\begin{frame}
\bibliographystyle{alpha}
\bibliography{../thesis/bibtex/dersebu}
% \nocite{*}
%\end{frame}

\end{document}