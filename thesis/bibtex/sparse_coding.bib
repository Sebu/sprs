Automatically generated by Mendeley 0.9.8.1
Any changes to this file will be lost if it is regenerated by Mendeley.

@inproceedings{Mairal2008,
abstract = {Sparse signal models have been the focus of much recent research, leading to (or improving upon) state-of-the-art results in signal, image, and video restoration. This article extends this line of research into a novel framework for local image discrimination tasks, proposing an energy formulation with both sparse reconstruction and class discrimination components, jointly optimized during dictionary learning. This approach improves over the state of the art in texture segmentation experiments using the Brodatz database, and it paves the way for a novel scene analysis and recognition framework based on simultaneously learning discriminative and reconstructive dictionaries. Preliminary results in this direction using examples from the Pascal VOC06 and Graz02 datasets are presented as well.},
address = {Anchorage, AK, USA},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo and Zisserman, Andrew},
booktitle = {Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on},
doi = {10.1109/CVPR.2008.4587652},
file = {:home/seb/git/diplom/papers/Mairal et al.\_2008\_Discriminative learned dictionaries for local image analysis.pdf:pdf},
isbn = {978-1-4244-2242-5},
month = jun,
pages = {1--8},
publisher = {IEEE},
title = {{Discriminative learned dictionaries for local image analysis}},
type = {Conference proceedings (article)},
url = {http://www.di.ens.fr/\~{}fbach/cvpr08.pdf http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4587652},
year = {2008}
}
@article{Delgado2003,
abstract = {Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur-concave (CSC) negative log priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen (environmentally matched) dictionary. The elements of the dictionary can be interpreted as concepts, features, or words capable of succinct expression of events encountered in the environment (the source of the measured signals). This is a generalization of vector quantization in that one is interested in a description involving a few dictionary entries (the proverbial "25 words or less"), but not necessarily as succinct as one entry. To learn an environmentally adapted dictionary capable of concise expression of signals generated by the environment, we develop algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS and an update of the dictionary using these sparse representations.Experiments were performed using synthetic data and natural images. For complete dictionaries, we demonstrate that our algorithms have improved performance over other independent component analysis (ICA) methods, measured in terms of signal-to-noise ratios of separated sources. In the overcomplete case, we show that the true underlying dictionary and sparse sources can be accurately recovered. In tests with natural images, learned overcomplete dictionaries are shown to have higher coding efficiency than complete dictionaries; that is, images encoded with an overcomplete dictionary have both higher compression (fewer bits per pixel) and higher accuracy (lower mean square error).},
author = {Delgado, Kenneth Kreutz and Murray, Joseph F. and Rao, Bhaskar D. and Engan, Kjersti and Lee, Te Won and Sejnowski, Terrence J.},
doi = {10.1162/089976603762552951},
issn = {0899-7667},
journal = {Neural Comput.},
keywords = {dictionary,learning,sparse},
mendeley-tags = {dictionary,learning,sparse},
number = {2},
pages = {349--396},
publisher = {MIT Press},
title = {{Dictionary learning algorithms for sparse representation}},
type = {Journal article},
url = {http://portal.acm.org/citation.cfm?id=643340},
volume = {15},
year = {2003}
}
@article{Rubinstein2010,
abstract = {Sparse and redundant representation modeling of data assumes an ability to describe signals as linear combinations of a few atoms from a pre-specified dictionary. As such, the choice of the dictionary that sparsifies the signals is crucial for the success of this model. In general, the choice of a proper dictionary can be done using one of two ways: i) building a sparsifying dictionary based on a mathematical model of the data, or ii) learning a dictionary to perform best on a training set. In this paper we describe the evolution of these two paradigms. As manifestations of the first approach, we cover topics such as wavelets, wavelet packets, contourlets, and curvelets, all aiming to exploit 1-D and 2-D mathematical models for constructing effective dictionaries for signals and images. Dictionary learning takes a different route, attaching the dictionary to a set of examples it is supposed to serve. From the seminal work of Field and Olshausen, through the MOD, the K-SVD, the Generalized PCA and others, this paper surveys the various options such training has to offer, up to the most recent contributions and structures.},
author = {Rubinstein, Ron and Bruckstein, Alfred M. and Elad, Michael},
doi = {10.1109/JPROC.2010.2040551},
file = {:home/seb/git/diplom/papers/Rubinstein, Bruckstein, Elad\_2010\_Dictionaries for Sparse Representation Modeling.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {dictionaries,sparse\_coding},
mendeley-tags = {dictionaries,sparse\_coding},
month = jun,
number = {6},
pages = {1045--1057},
title = {{Dictionaries for Sparse Representation Modeling}},
type = {Journal article},
url = {http://www.cs.technion.ac.il/\~{}ronrubin/Publications/dictdesign.pdf http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5452966},
volume = {98},
year = {2010}
}
@inproceedings{Lee_NIPS_2007,
abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons. 1},
author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y.},
file = {:home/seb/git/diplom/papers/Lee et al.\_2007\_Efficient sparse coding algorithms.pdf:pdf},
keywords = {coding,sparse},
mendeley-tags = {coding,sparse},
pages = {801--808},
title = {{Efficient sparse coding algorithms}},
type = {Conference proceedings (article)},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.69.2112},
year = {2007}
}
@article{mairal08sparse,
abstract = {Sparse representations of signals have drawn considerable interest in recent years. The assumption that natural signals, such as images, admit a sparse decomposition over a redundant dictionary leads to efficient algorithms for handling such sources of data. In particular, the design of well adapted dictionaries for images has been a major challenge. The K-SVD has been recently proposed for this task and shown to perform very well for various grayscale image processing tasks. In this paper, we address the problem of learning dictionaries for color images and extend the K-SVD-based grayscale image denoising algorithm that appears in. This work puts forward ways for handling nonhomogeneous noise and missing information, paving the way to state-of-the-art results in applications such as color image denoising, demosaicing, and inpainting, as demonstrated in this paper.},
author = {Mairal, Julien and Elad, Michael and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Mairal, Elad, Sapiro\_2008\_Sparse representation for color image restoration.pdf:pdf},
issn = {1057-7149},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
keywords = {coding,sparse},
mendeley-tags = {coding,sparse},
month = jan,
number = {1},
pages = {53--69},
pmid = {18229804},
title = {{Sparse representation for color image restoration.}},
type = {Journal article},
volume = {17},
year = {2008}
}
@article{Bach2009c,
author = {Bach, Francis and Mairal, Julien and Ponce, Jean and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Bach et al.\_2009\_Sparse Coding and Dictionary Learning for Image Analysis IV.pdf:pdf},
number = {September},
pages = {1--19},
title = {{Sparse Coding and Dictionary Learning for Image Analysis IV}},
year = {2009}
}
@article{Aharon2006,
abstract = {In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method—the<tex>\$ K\$</tex>-SVD algorithm—generalizing the<tex>\$ K\$</tex>-means clustering process.<tex>\$ K\$</tex>-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The<tex>\$ K\$</tex>-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data.},
author = {Aharon, M. and Elad, M. and Bruckstein, A.},
doi = {10.1109/TSP.2006.881199},
file = {:home/seb/git/diplom/papers/Aharon, Elad, Bruckstein\_2006\_K-SVD An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation.pdf:pdf},
journal = {Signal Processing, IEEE Transactions on [see also Acoustics, Speech, and Signal Processing, IEEE Transactions on]},
number = {11},
pages = {4311--4322},
title = {{K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation}},
type = {Journal article},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1710377 http://adsabs.harvard.edu/cgi-bin/nph-bib\_query?bibcode=2006ITSP...54.4311A},
volume = {54},
year = {2006}
}
@article{Mallat1993,
abstract = {The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992)},
author = {Mallat, S.G. and Zhang, Zhifeng},
doi = {10.1109/78.258082},
file = {:home/seb/git/diplom/papers/Mallat, Zhang\_1993\_Matching pursuits with time-frequency dictionaries.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {dictionary,matching\_pursuit,overcomplete,redd,redundancy,sparse},
mendeley-tags = {dictionary,matching\_pursuit,overcomplete,redd,redundancy,sparse},
number = {12},
pages = {3397--3415},
title = {{Matching pursuits with time-frequency dictionaries}},
type = {Journal article},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=258082},
volume = {41},
year = {1993}
}
@article{Mairal2010,
abstract = {Sparse coding--that is, modelling data vectors as sparse linear combinations
of basis elements--is widely used in machine learning, neuroscience, signal
processing, and statistics. This paper focuses on the large-scale matrix
factorization problem that consists of learning the basis set, adapting it to
specific data. Variations of this problem include dictionary learning in signal
processing, non-negative matrix factorization and sparse principal component
analysis. In this paper, we propose to address these tasks with a new online
optimization algorithm, based on stochastic approximations, which scales up
gracefully to large datasets with millions of training samples, and extends
naturally to various matrix factorization formulations, making it suitable for
a wide range of learning problems. A proof of convergence is presented, along
with experiments with natural images and genomic data demonstrating that it
leads to state-of-the-art performance in terms of speed and optimization for
both small and large datasets.},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Mairal et al.\_2010\_Online Learning for Matrix Factorization and Sparse Coding.pdf:pdf},
keywords = {coding,computer,dictionary,learning,sparse,vision},
mendeley-tags = {coding,computer,dictionary,learning,sparse,vision},
title = {{Online Learning for Matrix Factorization and Sparse Coding}},
type = {Journal article},
url = {http://arxiv.org/abs/0908.0050 http://arxiv.org/pdf/0908.0050},
year = {2010}
}
@phdthesis{Krizhevsky2009,
author = {Krizhevsky, Alex},
file = {:home/seb/git/diplom/papers/Krizhevsky\_2009\_Learning Multiple Layers of Features from Tiny Images.pdf:pdf},
keywords = {learning,sparse},
mendeley-tags = {learning,sparse},
title = {{Learning Multiple Layers of Features from Tiny Images}},
type = {Thesis (Master's)},
url = {http://www.cs.toronto.edu/\~{}kriz/learning-features-2009-TR.pdf},
year = {2009}
}
@article{Rubinstein2009,
abstract = {An efficient and flexible dictionary structure is proposed for sparse and redundant signal representation. The proposed sparse dictionary is based on a sparsity model of the dictionary atoms over a base dictionary, and takes the form D = \~{A}‚\^{A}¿ A, where \~{A}‚\^{A}¿ is a fixed base dictionary and A is sparse. The sparse dictionary provides efficient forward and adjoint operators, has a compact representation, and can be effectively trained from given example data. In this, the sparse structure bridges the gap between implicit dictionaries, which have efficient implementations yet lack adaptability, and explicit dictionaries, which are fully adaptable but non-efficient and costly to deploy. In this paper, we discuss the advantages of sparse dictionaries, and present an efficient algorithm for training them. We demonstrate the advantages of the proposed structure for 3-D image denoising.},
author = {Rubinstein, Ron and Zibulevsky and Elad},
doi = {10.1109/TSP.2009.2036477},
file = {:home/seb/git/diplom/papers/Rubinstein, Zibulevsky, Elad\_2009\_Double Sparsity Learning Sparse Dictionaries for Sparse Signal Approximation.pdf:pdf},
issn = {1053-587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {atoms,compact\_representation,learning,sparse\_coding,sparsity},
mendeley-tags = {atoms,compact\_representation,learning,sparse\_coding,sparsity},
title = {{Double Sparsity: Learning Sparse Dictionaries for Sparse Signal Approximation}},
type = {Journal article},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5325694},
year = {2009}
}
@article{Bach2009b,
author = {Bach, Francis and Mairal, Julien and Ponce, Jean and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Bach et al.\_2009\_Sparse Coding and Dictionary Learning for Image Analysis III.pdf:pdf},
journal = {Optimization},
number = {September},
title = {{Sparse Coding and Dictionary Learning for Image Analysis III}},
year = {2009}
}
@article{Bach,
author = {Bach, Francis and Mairal, Julien},
file = {:home/seb/git/diplom/papers/Bach, Mairal\_Unknown\_Sparse Coding and Dictionary Learning for Image Analysis 0.pdf:pdf},
title = {{Sparse Coding and Dictionary Learning for Image Analysis 0}}
}
@article{Mailhe2009,
author = {Mailhe, B. and Gribonval, R. and Bimbot, F. and Lemay, M. and Vandergheynst, P. and Vesin, J.-M.},
doi = {10.1109/ICASSP.2009.4959621},
file = {:home/seb/git/diplom/papers/Mailhe et al.\_2009\_Dictionary learning for the sparse modelling of atrial fibrillation in ECG signals.pdf:pdf},
isbn = {978-1-4244-2353-8},
journal = {2009 IEEE International Conference on Acoustics, Speech and Signal Processing},
month = apr,
pages = {465--468},
publisher = {Ieee},
title = {{Dictionary learning for the sparse modelling of atrial fibrillation in ECG signals}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4959621},
year = {2009}
}
@inproceedings{Hoyer2002,
abstract = {[Non-negative sparse coding, non-negative matrix factorization, sparseness] Non-negative sparse coding is a method for decomposing multivariate data into non-negative sparse components. We briefly describe the motivation behind this type of data representation and its relation to standard sparse coding and non-negative matrix factorization. We then give a simple yet efficient multiplicative algorithm for finding the optimal values of the hidden components. In addition, we show how the basis vectors can be learned from the observed data. Simulations demonstrate the effectiveness of the proposed method.},
author = {Hoyer, P.O.},
booktitle = {Neural Networks for Signal Processing, 2002. Proceedings of the 2002 12th IEEE Workshop on},
keywords = {coding,sparse},
mendeley-tags = {coding,sparse},
pages = {557--565},
title = {{Non-negative sparse coding}},
type = {Conference proceedings (whole)},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1030067},
year = {2002}
}
@inproceedings{Mairal2009,
address = {Montreal, Canada},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Mairal et al.\_2009\_Online Dictionary Learning for Sparse Coding.pdf:pdf},
keywords = {base,coding,dictionary,image,learning,online,sparse},
mendeley-tags = {base,coding,dictionary,image,learning,online,sparse},
title = {{Online Dictionary Learning for Sparse Coding.}},
type = {Conference proceedings (article)},
url = {http://www.di.ens.fr/willow/pdfs/icml09.pdf},
year = {2009}
}
@article{Lewicki2000,
abstract = {In an overcomplete basis, the number of basis vectors is greater than the dimensionality of the input, and the representation of an input is not a unique combination of basis vectors. Overcomplete representations have been advocated because they have greater robustness in the presence of noise, can be more sparse, and can have greater flexibility in matching structure in the data. Overcomplete codes have also been proposed as a model of some of the response properties of neurons in primary...},
author = {Lewicki, Michael S. and Sejnowski, Terrence J.},
file = {:home/seb/git/diplom/papers/Lewicki, Sejnowski\_2000\_Learning Overcomplete Representations.pdf:pdf},
journal = {Neural Computation},
keywords = {dictionary,overcomplete,redundancy},
mendeley-tags = {dictionary,overcomplete,redundancy},
number = {2},
pages = {337--365},
title = {{Learning Overcomplete Representations}},
type = {Journal article},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.8986},
volume = {12},
year = {2000}
}
@book{Mallat2008,
abstract = {\_Mallat's book is the undisputed reference in this field - it is the only one
that covers the essential material in such breadth and depth. - Laurent
Demanet, Stanford University\_


The new edition of this classic book gives all the major concepts, techniques
and applications of sparse representation, reflecting the key role the subject
plays in today's signal processing. The book clearly presents the standard
representations with Fourier, wavelet and time-frequency transforms, and the
construction of orthogonal bases with fast algorithms. The central concept of
sparsity is explained and applied to signal compression, noise reduction, and
inverse problems, while coverage is given to sparse representations in
redundant dictionaries, super-resolution and compressive sensing applications.


Features:


* Balances presentation of the mathematics with applications to signal
processing

* Algorithms and numerical examples are implemented in WaveLab, a MATLAB
toolbox

* Companion website for instructors and selected solutions and code available
for students


New in this edition


* Sparse signal representations in dictionaries

* Compressive sensing, super-resolution and source separation

* Geometric image processing with curvelets and bandlets

* Wavelets for computer graphics with lifting on surfaces

* Time-frequency audio processing and denoising

* Image compression with JPEG-2000

* New and updated exercises


**A Wavelet Tour of Signal Processing: The Sparse Way**, third edition, is an
invaluable resource for researchers and R\&D engineers wishing to apply the
theory in fields such as image processing, video processing and compression,
bio-sensing, medical imaging, machine vision and communications engineering.


Stephane Mallat is Professor in Applied Mathematics at \'{E}cole Polytechnique,
Paris, France. From 1986 to 1996 he was a Professor at the Courant Institute
of Mathematical Sciences at New York University, and between 2001 and 2007, he
co-founded and became CEO of an image processing semiconductor company.


* Includes all the latest developments since the book was published in 1999,
including its

application to JPEG 2000 and MPEG-4

* Algorithms and numerical examples are implemented in Wavelab, a MATLAB
toolbox

* Balances presentation of the mathematics with applications to signal
processing},
author = {Mallat, Stephane},
edition = {3},
isbn = {0123743702},
keywords = {dictionary,sparse,wavelet},
mendeley-tags = {dictionary,sparse,wavelet},
month = dec,
publisher = {Academic Press},
title = {{A Wavelet Tour of Signal Processing, 3rd ed., Third Edition: The Sparse Way}},
type = {Book},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0123743702 http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0123743702 http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0123743702 http://www.amazon.jp/exec/obidos/ASIN/0123743702 http://www.amazon.co.uk/exec/obidos/ASIN/0123743702/citeulike00-21 http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0123743702 http://www.worldcat.org/isbn/0123743702 htt},
year = {2008}
}
@article{Bach2009,
author = {Bach, Francis and Mairal, Julien and Ponce, Jean and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Bach et al.\_2009\_Sparse Coding and Dictionary Learning for Image Analysis I.pdf:pdf},
journal = {Optimization},
number = {September},
pages = {1--41},
title = {{Sparse Coding and Dictionary Learning for Image Analysis I}},
year = {2009}
}
@article{Rubenstein2008,
author = {Rubinstein, Ron},
doi = {10.1142/S0219467808002952},
file = {:home/seb/git/diplom/papers/Rubinstein\_2008\_K-SVD for dummies.pdf:pdf},
journal = {International Journal of Image and Graphics},
number = {01},
pages = {25},
title = {{K-SVD for dummies}},
volume = {08},
year = {2008}
}
@article{Bach2009a,
author = {Bach, Francis and Mairal, Julien and Ponce, Jean and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Bach et al.\_2009\_Sparse Coding and Dictionary Learning for Image Analysis II.pdf:pdf},
journal = {Learning},
number = {September},
title = {{Sparse Coding and Dictionary Learning for Image Analysis II}},
year = {2009}
}
@article{Chen1998,
author = {Chen, Scott Shaobing and Donoho, David L. and Saunders, Michael A.},
journal = {SIAM Journal on Scientific Computing},
number = {1},
pages = {33--61},
publisher = {SIAM},
title = {{Atomic Decomposition by Basis Pursuit}},
type = {Journal article},
url = {http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal\&id=SJOCE3000020000001000033000001\&idtype=cvips\&gifs=yes http://link.aip.org/link/?SCE/20/33},
volume = {20},
year = {1998}
}
@misc{Bruckstein2007,
abstract = {A full-rank matrix A ∈ IR n×m with n \&lt; m generates an underdetermined system of linear equations Ax = b having infinitely many solutions. Suppose we seek the sparsest solution, i.e., the one with the fewest nonzero entries: can it ever be unique? If so, when? As optimization of sparsity is combinatorial in nature, are there efficient methods for finding the sparsest solution? These questions have been answered positively and constructively in recent years, exposing a wide variety of surprising phenomena; in particular, the existence of easily-verifiable conditions under which optimally-sparse solutions can be found by concrete, effective computational methods. Such theoretical results inspire a bold perspective on some important practical problems in signal and image processing. Several well-known signal and image processing problems can be cast as demanding solutions of undetermined systems of equations. Such problems have previously seemed, to many, intractable. There is considerable evidence that these problems often have sparse solutions. Hence, advances in finding sparse solutions to underdetermined systems energizes research on such signal and image processing problems – to striking effect. In this paper we review the theoretical results on sparse solutions of linear systems, empirical},
author = {Bruckstein, Alfred M. and Donoho, David L. and Elad, Michael},
keywords = {sparse},
mendeley-tags = {sparse},
title = {{From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images ∗}},
type = {Electronic citation},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.102.4697},
year = {2007}
}
@article{Aharon2006a,
author = {Aharon, Michal},
file = {:home/seb/git/diplom/papers/Aharon\_2006\_Overcomplete Dictionaries for Sparse Representation of Signals.pdf:pdf},
journal = {cs.technion.ac.il},
title = {{Overcomplete Dictionaries for Sparse Representation of Signals}},
url = {http://ftp.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2006/PHD/PHD-2006-15.pdf},
year = {2006}
}
@article{Mairal2007,
author = {Mairal, Julien and Sapiro, Guillermo and Elad, Michael},
file = {:home/seb/git/diplom/papers/Mairal, Sapiro, Elad\_2007\_Multiscale sparse image representation with learned dictionaries.pdf:pdf},
journal = {Proc. ICIP},
number = {4},
pages = {2--5},
title = {{Multiscale sparse image representation with learned dictionaries}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.133.7148\&amp;rep=rep1\&amp;type=pdf},
year = {2007}
}
@article{Efron2004,
abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection, and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived. (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of Ordinary Least Squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as Ordinary Least Squares applied to the full set of covariates.},
arxivId = {math/0406474v1},
author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
doi = {10.1214/009053604000000067},
file = {:home/seb/git/diplom/papers/Efron et al.\_2004\_Least angle regression.pdf:pdf},
institution = {Statistics Department, Stanford University},
issn = {00905364},
journal = {Annals of Statistics},
number = {2},
pages = {407--499},
publisher = {Institute of Mathematical Statistics},
title = {{Least angle regression}},
url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.aos/1083178935/},
volume = {32},
year = {2004}
}
@article{Bar2009,
abstract = {Sparse representation theory has been increasingly used in the fields of signal processing and machine learning. The standard sparse models are not invariant to spatial transformations such as image rotations, and the representation is very sensitive even under small such distortions. Most studies addressing this problem proposed algorithms which either use transformed data as part of the training set, or are invariant or robust only under minor transformations. In this paper we suggest a framework which extracts sparse features invariant under significant rotations and scalings. The algorithm is based on a hierarchical architecture of dictionary learning for sparse coding in a cortical (log-polar) space. The proposed model is tested in supervised classification applications and proved to be robust under transformed data.},
author = {Bar, L and Sapiro, G},
file = {:home/seb/git/diplom/papers/Bar, Sapiro\_2009\_HIERARCHICAL DICTIONARY LEARNING FOR INVARIANT CLASSIFICATION.pdf:pdf},
journal = {eceumnedu},
pages = {3578--3581},
title = {{HIERARCHICAL DICTIONARY LEARNING FOR INVARIANT CLASSIFICATION}},
url = {http://oai.dtic.mil/oai/oai?verb=getRecord\&amp;metadataPrefix=html\&amp;identifier=ADA513255},
volume = {Proceeding},
year = {2009}
}
@article{Bach2009d,
author = {Bach, Francis and Mairal, Julien},
journal = {Learning},
number = {6},
pages = {1--19},
title = {{Dictionary Learning}},
year = {2009}
}
@article{Jenatton2010,
abstract = {Sparse coding consists in representing signals as sparse linear combinations of atoms selected from a dictionary. We consider an extension of this framework where the atoms are further assumed to be embedded in a tree. This is achieved using a recently introduced tree-structured sparse regularization norm, which has proven useful in several applications. This norm leads to regularized problems that are difficult to optimize, and we propose in this paper efficient algorithms for solving them. More precisely, we show that the proximal operator associated with this norm is computable exactly via a dual approach that can be viewed as the composition of elementary proximal operators. Our procedure has a complexity linear, or close to linear, in the number of atoms, and allows the use of accelerated gradient techniques to solve the tree-structured sparse approximation problem at the same computational cost as traditional ones using the L1-norm. Our method is efficient and scales gracefully to millions of variables, which we illustrate in two types of applications: first, we consider fixed hierarchical dictionaries of wavelets to denoise natural images. Then, we apply our optimization tools in the context of dictionary learning, where learned dictionary elements naturally organize in a prespecified arborescent structure, leading to a better performance in reconstruction of natural image patches. When applied to text documents, our method learns hierarchies of topics, thus providing a competitive alternative to probabilistic topic models.},
author = {Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume and Bach, Francis},
journal = {Work},
title = {{Proximal Methods for Hierarchical Sparse Coding}},
url = {http://arxiv.org/abs/1009.2139},
year = {2010}
}
@misc{OpenCV,
author = {OpenCV},
booktitle = {http://opencv.willowgarage.com},
title = {{OpenCV}},
url = {http://opencv.willowgarage.com}
}
@misc{OpenMP,
author = {OpenMP},
title = {{OpenMP}},
url = {http://openmp.org/wp}
}
@misc{Eigen,
author = {Eigen},
title = {{Eigen}},
url = {http://eigen.tuxfamily.org}
}
@misc{Rubinstein,
author = {Rubinstein, Ron},
booktitle = {bla},
title = {{OMPBox}},
url = {http://www.cs.technion.ac.il/\~{}ronrubin/software.html}
}
@misc{Strand2005,
abstract = {There are a number of interesting variable selection methods available beside the regular forward selection and stepwise selection methods. Such approaches include LASSO (Least Absolute Shrinkage and Selection Operator), least angle regression (LARS) and elastic net (LARS-EN) regression. There also exists a method for calculating principal components with sparse loadings. This software package contains Matlab implementations of these functions. The standard implementations of these functions are available as add-on packages in S-Plus and R.},
author = {Sjoestrandd, K},
keywords = {{elastic net,sparse,sparsity,variable selection,\{lasso,\} matlab,\} \{lars,\} \{spca}},
publisher = {Informatics and Mathematical Modelling, Technical University of Denmark, \{DTU\}},
title = {{Matlab implementation of LASSO, LARS, the elastic net and SPCA}},
url = {http://octopus.cs.drexel.edu/\~{}gao25/References/},
year = {2005}
}
@article{Olshausen1996,
abstract = {The receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
author = {Olshausen, B A and Field, D J},
file = {:home/seb/git/diplom/papers/Olshausen, Field\_1996\_Emergence of simple-cell receptive field properties by learning a sparse code for natural images.pdf:pdf},
institution = {Department of Psychology, Cornell University, Ithaca, New York 14853, USA. bruno@ai.mit.edu},
journal = {Nature},
number = {6583},
pages = {607--609},
pmid = {8637596},
publisher = {Nature Publishing Group},
title = {{Emergence of simple-cell receptive field properties by learning a sparse code for natural images.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8637596},
volume = {381},
year = {1996}
}
@article{Bryt2008,
author = {Bryt, O and Elad, M},
doi = {10.1016/j.jvcir.2008.03.001},
file = {:home/seb/git/diplom/papers/Bryt, Elad\_2008\_Compression of facial images using the K-SVD algorithm.pdf:pdf},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
number = {4},
pages = {270--282},
publisher = {Elsevier},
title = {{Compression of facial images using the K-SVD algorithm}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1047320308000254},
volume = {19},
year = {2008}
}
@article{Pati1993,
abstract = {We describe a recursive algorithm to compute representations of functions with respect to nonorthogonal and possibly overcomplete dictionaries of elementary building blocks e.g. affine (wavelet) frames. We propose a modification to the matching pursuit algorithm of Mallat and Zhang (1992) that maintains full backward orthogonality of the residual (error) at every step and thereby leads to improved convergence. We refer to this modified algorithm as orthogonal matching pursuit (OMP). It is shown that all additional computation required for the OMP algorithm may be performed recursively},
author = {Pati, Y C and Rezaiifar, R and Krishnaprasad, P S},
doi = {10.1109/ACSSC.1993.342465},
isbn = {0818641207},
issn = {10586393},
journal = {Proceedings of 27th Asilomar Conference on Signals Systems and Computers},
pages = {40--44},
publisher = {IEEE Comput. Soc. Press},
title = {{Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=342465},
volume = {1},
year = {1993}
}
@article{Rubinstein2008,
author = {Rubinstein, Ron and Zibulevsky, Michael and Elad, Michael},
file = {:home/seb/git/diplom/papers/Rubinstein, Zibulevsky, Elad\_2008\_Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit.pdf:pdf},
institution = {Israel Institute of Technology},
pages = {1--15},
publisher = {Technical Report–CS Technion, 2008},
title = {{Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit}},
url = {http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2008/CS/CS-2008-08.revised.pdf},
year = {2008}
}
@article{Elad2006,
abstract = {We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods.},
author = {Elad, Michael and Aharon, Michal},
institution = {Department of Computer Science, The Technion-Israel Institute of Technology, Haifa 32000, Israel. elad@cs.technion.ac.il},
journal = {IEEE Transactions on Image Processing},
number = {12},
pages = {3736--3745},
pmid = {17153947},
publisher = {IEEE},
title = {{Image denoising via sparse and redundant representations over learned dictionaries.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17153947},
volume = {15},
year = {2006}
}
@article{Mairal2008b,
abstract = {Sparse signal models learned from data are widely used in audio, image, and video restoration. They have recently been generalized to discriminative image understanding tasks such as texture segmentation and feature selection. This paper extends this line of research by proposing a multiscale method to minimize least-squares reconstruction errors and discriminative cost functions under 0 or 1 regularization constraints. It is applied to edge detection, category-based edge selection and image classification tasks. Experiments on the Berkeley edge detection benchmark and the PASCAL VOC05 and VOC07 datasets demonstrate the computational efficiency of our algorithm and its ability to learn local image descriptions that effectively support demanding computer vision tasks.},
author = {Mairal, Julien and Leordeanu, Marius and Bach, Francis and Hebert, Martial and Ponce, Jean},
file = {:home/seb/git/diplom/papers/Mairal et al.\_2008\_Discriminative Sparse Image Models for Class-Specific Edge Detection and Image Interpretation.pdf:pdf},
journal = {Computer Vision–ECCV 2008},
keywords = {machine vision},
pages = {43--56},
publisher = {Springer},
series = {LNCS},
title = {{Discriminative Sparse Image Models for Class-Specific Edge Detection and Image Interpretation}},
url = {http://eprints.pascal-network.org/archive/00004520/},
volume = {5304},
year = {2008}
}
