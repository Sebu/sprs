Automatically generated by Mendeley 0.9.8.2
Any changes to this file will be lost if it is regenerated by Mendeley.

@article{Mairal2009c,
abstract = {We propose in this paper to unify two different approaches to image restoration: On the one hand, learning a basis set (dictionary) adapted to sparse signal descriptions has proven to be very effective in image reconstruction and classification tasks. On the other hand, explicitly exploiting the self-similarities of natural images has led to the successful non-local means approach to image restoration. We propose simultaneous sparse coding as a framework for combining these two approaches in a natural manner. This is achieved by jointly decomposing groups of similar signals on subsets of the learned dictionary. Experimental results in image denoising and demosaicking tasks with synthetic and real noise show that the proposed method outperforms the state of the art, making it possible to effectively restore raw images from digital cameras at a reasonable speed and memory cost.},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo and Zisserman, Andrew},
doi = {10.1109/ICCV.2009.5459452},
isbn = {9781424444205},
issn = {15505499},
journal = {2009 IEEE 12th International Conference on Computer Vision},
number = {Iccv},
pages = {2272--2279},
publisher = {Ieee},
title = {{Non-local sparse models for image restoration}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459452},
volume = {2},
year = {2009}
}
@article{Bryt2008b,
abstract = {The use of sparse representations in signal processing is gradually increasing in the past several years. In a previous work we proposed a new method for compressing facial images using the K-SVD algorithm, which is a novel algorithm for training overcomplete dictionaries that lead to sparse signal representations. This method was shown to be most efficient, surpassing the JPEG2000 performance significantly. In this paper we present a significant addition to our compression algorithm in the form of image deblocking. Since the encoding is done in patches, a visually disturbing artifacts of blockiness appear in the reconstructed images. We eliminate these artifacts using a linear deblocking technique, which is based on local image filters. We construct a linear filter for each relevant pixel independently, and apply these filters as post-processing. This method is limited, but nevertheless it improves the PSNR of the reconstructed images and gives visually appealing results.},
author = {Bryt, Ori and Elad, Michael},
doi = {10.1109/EEEI.2008.4736586},
isbn = {9781424424818},
journal = {2008 IEEE 25th Convention of Electrical and Electronics Engineers in Israel},
pages = {533--537},
publisher = {Ieee},
title = {{Improving the k-svd facial image compression using a linear deblocking method}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4736586},
year = {2008}
}
@article{Rubinstein2008,
author = {Rubinstein, Ron and Zibulevsky, Michael and Elad, Michael},
file = {:home/seb/git/diplom/papers/Rubinstein, Zibulevsky, Elad\_2008\_Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit.pdf:pdf},
institution = {Israel Institute of Technology},
pages = {1--15},
publisher = {Technical Report–CS Technion, 2008},
title = {{Efficient Implementation of the K-SVD Algorithm using Batch Orthogonal Matching Pursuit}},
url = {http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2008/CS/CS-2008-08.revised.pdf},
year = {2008}
}
@article{Rubenstein2008,
author = {Rubinstein, Ron},
doi = {10.1142/S0219467808002952},
file = {:home/seb/git/diplom/papers/Rubinstein\_2008\_K-SVD for dummies.pdf:pdf},
journal = {International Journal of Image and Graphics},
number = {01},
pages = {25},
title = {{K-SVD for dummies}},
volume = {08},
year = {2008}
}
@inproceedings{Mairal2009,
address = {Montreal, Canada},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Mairal et al.\_2009\_Online Dictionary Learning for Sparse Coding.pdf:pdf},
keywords = {base,coding,dictionary,image,learning,online,sparse},
mendeley-tags = {base,coding,dictionary,image,learning,online,sparse},
title = {{Online Dictionary Learning for Sparse Coding.}},
type = {Conference proceedings (article)},
url = {http://www.di.ens.fr/willow/pdfs/icml09.pdf},
year = {2009}
}
@article{Olshausen1997a,
abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete-i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.},
author = {Olshausen, B A and Field, D J},
institution = {Department of Psychology, Cornell University, Ithaca, NY 14853, USA. bruno@redwood.ucdavis.edu},
journal = {Vision Research},
number = {23},
pages = {3311--3325},
pmid = {9425546},
publisher = {Elsevier},
title = {{Sparse coding with an overcomplete basis set: a strategy employed by V1?}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0042698997001697},
volume = {37},
year = {1997}
}
@article{Cooley1965,
abstract = {Description of FFT},
author = {Cooley, J W and Tukey, J W},
chapter = {297},
doi = {10.2307/2003354},
file = {:home/seb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cooley, Tukey - 1965 - An Algorithm for the Machine Calculation of Complex Fourier Series.pdf:pdf},
issn = {00255718},
journal = {Mathematics of Computation},
number = {90},
pages = {297--301},
publisher = {American Mathematical Society},
title = {{An Algorithm for the Machine Calculation of Complex Fourier Series}},
url = {http://www.jstor.org/stable/2003354?origin=crossref},
volume = {19},
year = {1965}
}
@article{Bach2009b,
author = {Bach, Francis and Mairal, Julien and Ponce, Jean and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Bach et al.\_2009\_Sparse Coding and Dictionary Learning for Image Analysis III.pdf:pdf},
journal = {Optimization},
number = {September},
title = {{Sparse Coding and Dictionary Learning for Image Analysis III}},
year = {2009}
}
@inproceedings{Yee2004a,
abstract = {This paper describes a perceptually-based image comparison process that can be used to tell when images are perceptually identical even though they contain some numerical differences. The technique has shown much utility in the production testing of rendering software.},
address = {Los Angeles, California},
author = {Yee, Yangli Hector and Newman, Anna},
doi = {10.1145/1186223.1186374},
isbn = {1-59593-896-2},
publisher = {ACM},
title = {{A perceptual metric for production testing}},
type = {Conference proceedings (article)},
url = {http://portal.acm.org/citation.cfm?id=1186374},
year = {2004}
}
@misc{Rubinstein,
author = {Rubinstein, Ron},
booktitle = {bla},
title = {{OMPBox}},
url = {http://www.cs.technion.ac.il/\~{}ronrubin/software.html}
}
@article{Peyre2008,
abstract = {This paper presents a generative model for textures that uses a local sparse description of the image content. This model enforces the sparsity of the expansion of local texture patches on adapted atomic elements. The analysis of a given texture within this framework performs the sparse coding of all the patches of the texture into the dictionary of atoms. Conversely, the synthesis of a new texture is performed by solving an optimization problem that seeks for a texture whose patches are sparse in the dictionary. This paper explores several strategies to choose this dictionary. A set of hand crafted dictionaries composed of edges, oscillations, lines or crossings elements allows to synthesize synthetic images with geometric features. Another option is to define the dictionary as the set of all the patches of an input exemplar. This leads to computer graphics methods for synthesis and shares some similarities with non-local means filtering. The last method we explore learns the dictionary by an optimization process that maximizes the sparsity of a set of exemplar patches. Applications of all these methods to texture synthesis, inpainting and classification shows the efficiency of the proposed texture model.},
author = {Peyr\'{e}, Gabriel},
doi = {10.1007/s10851-008-0120-3},
issn = {09249907},
journal = {Journal of Mathematical Imaging and Vision},
number = {1},
pages = {17--31},
title = {{Sparse Modeling of Textures}},
url = {http://www.springerlink.com/index/10.1007/s10851-008-0120-3},
volume = {34},
year = {2008}
}
@misc{OpenCV,
author = {OpenCV},
booktitle = {http://opencv.willowgarage.com},
title = {{OpenCV}},
url = {http://opencv.willowgarage.com}
}
@article{Yang2010,
abstract = {This paper presents a new approach to single-image super-resolution, based on sparse signal representation. Research on image statistics suggests that image patches can be well-represented as a sparse linear combination of elements from an appropriately chosen over-complete dictionary. Inspired by this observation, we seek a sparse representation for each patch of the low-resolution input, and then use the coefficients of this representation to generate the high-resolution output. Theoretical results from compressed sensing suggest that under mild conditions, the sparse representation can be correctly recovered from the downsampled signals. By jointly training two dictionaries for the low- and high-resolution image patches, we can enforce the similarity of sparse representations between the low resolution and high resolution image patch pair with respect to their own dictionaries. Therefore, the sparse representation of a low resolution image patch can be applied with the high resolution image patch dictionary to generate a high resolution image patch. The learned dictionary pair is a more compact representation of the patch pairs, compared to previous approaches, which simply sample a large amount of image patch pairs 1, reducing the computational cost substantially. The effectiveness of such a sparsity prior is demonstrated for both general image super-resolution and the special case of face hallucination. In both cases, our algorithm generates high-resolution images that are competitive or even superior in quality to images produced by other similar SR methods. In addition, the local sparse modeling of our approach is naturally robust to noise, and therefore the proposed algorithm can handle super-resolution with noisy inputs in a more unified framework.},
author = {Yang, Jianchao and Wright, John and Huang, Thomas and Ma, Yi},
doi = {10.1109/TIP.2010.2050625},
issn = {19410042},
journal = {IEEE Transactions on Image Processing},
number = {11},
pages = {1--13},
pmid = {20483687},
publisher = {IEEE},
title = {{Image Super-Resolution via Sparse Representation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20483687},
volume = {19},
year = {2010}
}
@article{Engan2007,
abstract = {The use of overcomplete dictionaries, or frames, for sparse signal representation has been given considerable attention in recent years. The major challenges are good algorithms for sparse approximations, i.e., vector selection algorithms, and good methods for choosing or designing dictionaries/frames. This work is concerned with the latter. We present a family of iterative least squares based dictionary learning algorithms (ILS-DLA), including algorithms for design of signal dependent block based dictionaries and overlapping dictionaries, as generalizations of transforms and filter banks, respectively. In addition different constraints can be included in the ILS-DLA, thus we present different constrained design algorithms. Experiments show that ILS-DLA is capable of reconstructing (most of) the generating dictionary vectors from a sparsely generated data set, with and without noise. The dictionaries are shown to be useful in applications like signal representation and compression where experiments demonstrate that our ILS-DLA dictionaries substantially improve compression results compared to traditional signal expansions such as transforms and filter banks/wavelets.},
author = {Engan, Kjersti and Skretting, Karl and Husoy, John Hakon},
doi = {10.1016/j.dsp.2006.02.002},
issn = {10512004},
journal = {Digital Signal Processing},
keywords = {basis selection,compression,design,dictionary design,dictionary learning,expansions,frame design,least squares,matching pursuit,minimization,noise,overcomplete dictionary,overcomplete representations,set,signal representation,sparse approximation},
number = {1},
pages = {32--49},
title = {{Family of iterative LS-based dictionary learning algorithms, ILS-DLA, for sparse signal representation}},
url = {http://www.sciencedirect.com/science/article/B6WDJ-4JG469S-1/2/37b677ca42492554d5bf0003e1e8d9ad},
volume = {17},
year = {2007}
}
@article{Mairal2008b,
abstract = {Sparse signal models learned from data are widely used in audio, image, and video restoration. They have recently been generalized to discriminative image understanding tasks such as texture segmentation and feature selection. This paper extends this line of research by proposing a multiscale method to minimize least-squares reconstruction errors and discriminative cost functions under 0 or 1 regularization constraints. It is applied to edge detection, category-based edge selection and image classification tasks. Experiments on the Berkeley edge detection benchmark and the PASCAL VOC05 and VOC07 datasets demonstrate the computational efficiency of our algorithm and its ability to learn local image descriptions that effectively support demanding computer vision tasks.},
author = {Mairal, Julien and Leordeanu, Marius and Bach, Francis and Hebert, Martial and Ponce, Jean},
file = {:home/seb/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mairal et al. - 2008 - Discriminative Sparse Image Models for Class-Specific Edge Detection and Image Interpretation.pdf:pdf},
journal = {Computer Vision–ECCV 2008},
keywords = {machine vision},
pages = {43--56},
publisher = {Springer},
series = {LNCS},
title = {{Discriminative Sparse Image Models for Class-Specific Edge Detection and Image Interpretation}},
url = {http://eprints.pascal-network.org/archive/00004520/},
volume = {5304},
year = {2008}
}
@article{Aharon2006,
abstract = {In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method—the<tex>\$ K\$</tex>-SVD algorithm—generalizing the<tex>\$ K\$</tex>-means clustering process.<tex>\$ K\$</tex>-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The<tex>\$ K\$</tex>-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data.},
author = {Aharon, M. and Elad, M. and Bruckstein, A.},
doi = {10.1109/TSP.2006.881199},
file = {:home/seb/git/diplom/papers/Aharon, Elad, Bruckstein\_2006\_K-SVD An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation.pdf:pdf},
journal = {Signal Processing, IEEE Transactions on [see also Acoustics, Speech, and Signal Processing, IEEE Transactions on]},
number = {11},
pages = {4311--4322},
title = {{K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation}},
type = {Journal article},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1710377 http://adsabs.harvard.edu/cgi-bin/nph-bib\_query?bibcode=2006ITSP...54.4311A},
volume = {54},
year = {2006}
}
@article{Mairal2010,
abstract = {Sparse coding--that is, modelling data vectors as sparse linear combinations
of basis elements--is widely used in machine learning, neuroscience, signal
processing, and statistics. This paper focuses on the large-scale matrix
factorization problem that consists of learning the basis set, adapting it to
specific data. Variations of this problem include dictionary learning in signal
processing, non-negative matrix factorization and sparse principal component
analysis. In this paper, we propose to address these tasks with a new online
optimization algorithm, based on stochastic approximations, which scales up
gracefully to large datasets with millions of training samples, and extends
naturally to various matrix factorization formulations, making it suitable for
a wide range of learning problems. A proof of convergence is presented, along
with experiments with natural images and genomic data demonstrating that it
leads to state-of-the-art performance in terms of speed and optimization for
both small and large datasets.},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Mairal et al.\_2010\_Online Learning for Matrix Factorization and Sparse Coding.pdf:pdf},
keywords = {coding,computer,dictionary,learning,sparse,vision},
mendeley-tags = {coding,computer,dictionary,learning,sparse,vision},
title = {{Online Learning for Matrix Factorization and Sparse Coding}},
type = {Journal article},
url = {http://arxiv.org/abs/0908.0050 http://arxiv.org/pdf/0908.0050},
year = {2010}
}
@article{Mairal2007,
author = {Mairal, Julien and Sapiro, Guillermo and Elad, Michael},
file = {:home/seb/git/diplom/papers/Mairal, Sapiro, Elad\_2007\_Multiscale sparse image representation with learned dictionaries.pdf:pdf},
journal = {Proc. ICIP},
number = {4},
pages = {2--5},
title = {{Multiscale sparse image representation with learned dictionaries}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.133.7148\&amp;rep=rep1\&amp;type=pdf},
year = {2007}
}
@article{Engan2010,
author = {Engan, Kjersti},
doi = {10.1021/jp912157g},
file = {:home/seb/git/diplom/papers/Engan\_2010\_Recursive Least Squares Dictionary Learning Algorithm.pdf:pdf},
issn = {1520-5207},
pmid = {20232924},
title = {{Recursive Least Squares Dictionary Learning Algorithm}},
year = {2010}
}
@article{Rubinstein2009,
abstract = {An efficient and flexible dictionary structure is proposed for sparse and redundant signal representation. The proposed sparse dictionary is based on a sparsity model of the dictionary atoms over a base dictionary, and takes the form D = \~{A}‚\^{A}¿ A, where \~{A}‚\^{A}¿ is a fixed base dictionary and A is sparse. The sparse dictionary provides efficient forward and adjoint operators, has a compact representation, and can be effectively trained from given example data. In this, the sparse structure bridges the gap between implicit dictionaries, which have efficient implementations yet lack adaptability, and explicit dictionaries, which are fully adaptable but non-efficient and costly to deploy. In this paper, we discuss the advantages of sparse dictionaries, and present an efficient algorithm for training them. We demonstrate the advantages of the proposed structure for 3-D image denoising.},
author = {Rubinstein, Ron and Zibulevsky and Elad},
doi = {10.1109/TSP.2009.2036477},
file = {:home/seb/git/diplom/papers/Rubinstein, Zibulevsky, Elad\_2009\_Double Sparsity Learning Sparse Dictionaries for Sparse Signal Approximation.pdf:pdf},
issn = {1053-587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {atoms,compact\_representation,learning,sparse\_coding,sparsity},
mendeley-tags = {atoms,compact\_representation,learning,sparse\_coding,sparsity},
title = {{Double Sparsity: Learning Sparse Dictionaries for Sparse Signal Approximation}},
type = {Journal article},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5325694},
year = {2009}
}
@article{Alon2009a,
abstract = {Choosing good problems is essential for being a good scientist. But what is a good problem, and how do you choose one? The subject is not usually discussed explicitly within our profession. Scientists are expected to be smart enough to figure it out on their own and through the observation of their teachers. This lack of explicit discussion leaves a vacuum that can lead to approaches such as choosing problems that can give results that merit publication in valued journals, resulting in a job and tenure.},
author = {Alon, Uri},
doi = {10.1016/j.molcel.2009.09.013},
issn = {10972765},
journal = {Molecular Cell},
month = sep,
number = {6},
pages = {726--728},
pmid = {19782018},
title = {{How To Choose a Good Scientific Problem}},
type = {Journal article},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1097276509006418 http://www.f1000.com/article/id/1165779},
volume = {35},
year = {2009}
}
@inproceedings{Wang2008a,
abstract = {We reduce transmission bandwidth and memory space for images by factoring their repeated content. A transform map and a condensed epitome are created such that all image blocks can be reconstructed from transformed epitome patches. The transforms may include affine deformation and color scaling to account for perspective and tonal variations across the image. The factored representation allows efficient random-access through a simple indirection, and can therefore be used for real-time texture mapping without expansion in memory. Our scheme is orthogonal to traditional image compression, in the sense that the epitome is amenable to further compression such as DXT. Moreover it allows a new mode of progressivity, whereby generic features appear before unique detail. Factoring is also effective across a collection of images, particularly in the context of image-based rendering. Eliminating redundant content lets us include textures that are several times as large in the same memory space.},
address = {Los Angeles, California},
author = {Wang, Huamin and Wexler, Yonatan and Ofek, Eyal and Hoppe, Hugues},
doi = {10.1145/1399504.1360613},
file = {:home/seb/git/diplom/papers/Wang et al.\_2008\_Factoring repeated content within and among images.pdf:pdf},
isbn = {978-1-4503-0112-1},
keywords = {affine,compression,computer,deformation,epitome,vision},
mendeley-tags = {affine,compression,computer,deformation,epitome,vision},
pages = {1--10},
publisher = {ACM},
title = {{Factoring repeated content within and among images}},
type = {Conference proceedings (article)},
url = {http://research.microsoft.com/en-us/um/people/hoppe/factorimage.pdf http://portal.acm.org/citation.cfm?id=1399504.1360613},
year = {2008}
}
@article{Bryt2008,
author = {Bryt, O and Elad, M},
doi = {10.1016/j.jvcir.2008.03.001},
file = {:home/seb/git/diplom/papers/Bryt, Elad\_2008\_Compression of facial images using the K-SVD algorithm.pdf:pdf},
issn = {10473203},
journal = {Journal of Visual Communication and Image Representation},
number = {4},
pages = {270--282},
publisher = {Elsevier},
title = {{Compression of facial images using the K-SVD algorithm}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1047320308000254},
volume = {19},
year = {2008}
}
@article{Bar2009,
abstract = {Sparse representation theory has been increasingly used in the fields of signal processing and machine learning. The standard sparse models are not invariant to spatial transformations such as image rotations, and the representation is very sensitive even under small such distortions. Most studies addressing this problem proposed algorithms which either use transformed data as part of the training set, or are invariant or robust only under minor transformations. In this paper we suggest a framework which extracts sparse features invariant under significant rotations and scalings. The algorithm is based on a hierarchical architecture of dictionary learning for sparse coding in a cortical (log-polar) space. The proposed model is tested in supervised classification applications and proved to be robust under transformed data.},
author = {Bar, L and Sapiro, G},
file = {:home/seb/git/diplom/papers/Bar, Sapiro\_2009\_HIERARCHICAL DICTIONARY LEARNING FOR INVARIANT CLASSIFICATION.pdf:pdf},
journal = {eceumnedu},
pages = {3578--3581},
title = {{HIERARCHICAL DICTIONARY LEARNING FOR INVARIANT CLASSIFICATION}},
url = {http://oai.dtic.mil/oai/oai?verb=getRecord\&amp;metadataPrefix=html\&amp;identifier=ADA513255},
volume = {Proceeding},
year = {2009}
}
@article{Olshausen1996,
abstract = {The receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs.},
author = {Olshausen, B A and Field, D J},
file = {:home/seb/git/diplom/papers/Olshausen, Field\_1996\_Emergence of simple-cell receptive field properties by learning a sparse code for natural images.pdf:pdf},
institution = {Department of Psychology, Cornell University, Ithaca, New York 14853, USA. bruno@ai.mit.edu},
journal = {Nature},
number = {6583},
pages = {607--609},
pmid = {8637596},
publisher = {Nature Publishing Group},
title = {{Emergence of simple-cell receptive field properties by learning a sparse code for natural images.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8637596},
volume = {381},
year = {1996}
}
@article{Mallat1993,
abstract = {The authors introduce an algorithm, called matching pursuit, that decomposes any signal into a linear expansion of waveforms that are selected from a redundant dictionary of functions. These waveforms are chosen in order to best match the signal structures. Matching pursuits are general procedures to compute adaptive signal representations. With a dictionary of Gabor functions a matching pursuit defines an adaptive time-frequency transform. They derive a signal energy distribution in the time-frequency plane, which does not include interference terms, unlike Wigner and Cohen class distributions. A matching pursuit isolates the signal structures that are coherent with respect to a given dictionary. An application to pattern extraction from noisy signals is described. They compare a matching pursuit decomposition with a signal expansion over an optimized wavepacket orthonormal basis, selected with the algorithm of Coifman and Wickerhauser see (IEEE Trans. Informat. Theory, vol. 38, Mar. 1992)},
author = {Mallat, S.G. and Zhang, Zhifeng},
doi = {10.1109/78.258082},
file = {:home/seb/git/diplom/papers/Mallat, Zhang\_1993\_Matching pursuits with time-frequency dictionaries.pdf:pdf},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
keywords = {dictionary,matching\_pursuit,overcomplete,redd,redundancy,sparse},
mendeley-tags = {dictionary,matching\_pursuit,overcomplete,redd,redundancy,sparse},
number = {12},
pages = {3397--3415},
title = {{Matching pursuits with time-frequency dictionaries}},
type = {Journal article},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=258082},
volume = {41},
year = {1993}
}
@article{Duarte2009,
abstract = {Sparse signal representation, analysis, and sensing have received a lot of attention in recent years from the signal processing, optimization, and learning communities. On one hand, learning overcomplete dictionaries that facilitate a sparse representation of the data as a liner combination of a few atoms from such dictionary leads to state-of-the-art results in image and video restoration and classification. On the other hand, the framework of compressed sensing (CS) has shown that sparse signals can be recovered from far less samples than those required by the classical Shannon-Nyquist Theorem. The samples used in CS correspond to linear projections obtained by a sensing projection matrix. It has been shown that, for example, a nonadaptive random sampling matrix satisfies the fundamental theoretical requirements of CS, enjoying the additional benefit of universality. On the other hand, a projection sensing matrix that is optimally designed for a certain class of signals can further improve the reconstruction accuracy or further reduce the necessary number of samples. In this paper, we introduce a framework for the joint design and optimization, from a set of training images, of the nonparametric dictionary and the sensing matrix. We show that this joint optimization outperforms both the use of random sensing matrices and those matrices that are optimized independently of the learning of the dictionary. Particular cases of the proposed framework include the optimization of the sensing matrix for a given dictionary as well as the optimization of the dictionary for a predefined sensing environment. The presentation of the framework and its efficient numerical optimization is complemented with numerous examples on classical image datasets.},
author = {Duarte-Carvajalino, Julio Martin and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Duarte-Carvajalino, Sapiro\_2009\_Learning to sense sparse signals simultaneous sensing matrix and sparsifying dictionary optimization.pdf:pdf},
institution = {Department of Electrical and Computer Engineering, University of Minnesota, Minneapolis, MN 55455-0436, USA.},
journal = {IEEE Transactions on Image Processing},
number = {7},
pages = {1395--1408},
pmid = {19497818},
title = {{Learning to sense sparse signals: simultaneous sensing matrix and sparsifying dictionary optimization.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19497818},
volume = {18},
year = {2009}
}
@inproceedings{Mallat1995,
author = {Bergeaud, Francois and Mallat, S},
booktitle = {Proc IEEE Int Conf Acoustics Speech Signal Process},
doi = {10.1109/ICIP.1995.529037},
isbn = {0780331222},
pages = {53--56},
publisher = {Academic Pr},
title = {{Matching pursuit of images}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=529037},
year = {1995}
}
@inproceedings{Mairal2008,
abstract = {Sparse signal models have been the focus of much recent research, leading to (or improving upon) state-of-the-art results in signal, image, and video restoration. This article extends this line of research into a novel framework for local image discrimination tasks, proposing an energy formulation with both sparse reconstruction and class discrimination components, jointly optimized during dictionary learning. This approach improves over the state of the art in texture segmentation experiments using the Brodatz database, and it paves the way for a novel scene analysis and recognition framework based on simultaneously learning discriminative and reconstructive dictionaries. Preliminary results in this direction using examples from the Pascal VOC06 and Graz02 datasets are presented as well.},
address = {Anchorage, AK, USA},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo and Zisserman, Andrew},
booktitle = {Computer Vision and Pattern Recognition, 2008. CVPR 2008. IEEE Conference on},
doi = {10.1109/CVPR.2008.4587652},
file = {:home/seb/git/diplom/papers/Mairal et al.\_2008\_Discriminative learned dictionaries for local image analysis.pdf:pdf},
isbn = {978-1-4244-2242-5},
month = jun,
pages = {1--8},
publisher = {IEEE},
title = {{Discriminative learned dictionaries for local image analysis}},
type = {Conference proceedings (article)},
url = {http://www.di.ens.fr/\~{}fbach/cvpr08.pdf http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=4587652},
year = {2008}
}
@article{Osborne2000a,
abstract = {The title Lasso has been suggested by Tibshirani (1996) as a colourful name for a technique of variable selection which requires the minimization of a sum of squares subject to an l1 bound \{kappa\} on the solution. This forces zero components in the minimizing solution for small values of \{kappa\}. Thus this bound can function as a selection parameter. This paper makes two contributions to computational problems associated with implementing the Lasso: (1) a compact descent method for solving the constrained problem for a particular value of \{kappa\} is formulated, and (2) a homotopy method, in which the constraint bound \{kappa\} becomes the homotopy parameter, is developed to completely describe the possible selection regimes. Both algorithms have a finite termination property. It is suggested that modified Gram-Schmidt orthogonalization applied to an augmented design matrix provides an effective basis for implementing the algorithms. 10.1093/imanum/20.3.389},
author = {Osborne, MR and Presnell, B and Turlach, BA},
doi = {10.1093/imanum/20.3.389},
journal = {IMA J Numer Anal},
month = jul,
number = {3},
pages = {389--403},
title = {{A new approach to variable selection in least squares problems}},
type = {Journal article},
url = {http://imajna.oxfordjournals.org/cgi/content/abstract/20/3/389},
volume = {20},
year = {2000}
}
@article{Bach,
author = {Bach, Francis and Mairal, Julien},
file = {:home/seb/git/diplom/papers/Bach, Mairal\_Unknown\_Sparse Coding and Dictionary Learning for Image Analysis 0.pdf:pdf},
title = {{Sparse Coding and Dictionary Learning for Image Analysis 0}}
}
@article{Bach2009d,
author = {Bach, Francis and Mairal, Julien},
journal = {Learning},
number = {6},
pages = {1--19},
title = {{Dictionary Learning}},
year = {2009}
}
@article{Bach2009,
author = {Bach, Francis and Mairal, Julien and Ponce, Jean and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Bach et al.\_2009\_Sparse Coding and Dictionary Learning for Image Analysis I.pdf:pdf},
journal = {Optimization},
number = {September},
pages = {1--41},
title = {{Sparse Coding and Dictionary Learning for Image Analysis I}},
year = {2009}
}
@article{Mikolajczyk2005a,
abstract = {Abstract\&nbsp;\&nbsp;The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris\&nbsp; (Mikolajczyk and \&nbsp;Schmid, 2002; Schaffalitzky and \&nbsp;Zisserman, 2002) and Hessian points\&nbsp; (Mikolajczyk and \&nbsp;Schmid, 2002), a detector of ‘maximally stable extremal regions', proposed by Matas et al.\&nbsp;(2002); an edge-based region detector\&nbsp; (Tuytelaars and Van\&nbsp;Gool, 1999) and a detector based on intensity extrema (Tuytelaars and Van\&nbsp;Gool, 2000), and a detector of ‘salient regions', proposed by Kadir, Zisserman and Brady\&nbsp;(2004). The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression. The objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework.},
author = {Mikolajczyk, K. and Tuytelaars, T. and Schmid, C. and Zisserman, A. and Matas, J. and Schaffalitzky, F. and Kadir, T. and Gool, L.},
doi = {10.1007/s11263-005-3848-x},
file = {:home/seb/git/diplom/papers/Mikolajczyk et al.\_2005\_A Comparison of Affine Region Detectors.pdf:pdf},
issn = {0920-5691},
journal = {International Journal of Computer Vision},
keywords = {affine,matching},
mendeley-tags = {affine,matching},
month = nov,
number = {1},
pages = {43--72},
publisher = {Kluwer Academic Publishers},
title = {{A Comparison of Affine Region Detectors}},
type = {Journal article},
url = {http://portal.acm.org/citation.cfm?id=1117156.1117169 http://www.springerlink.com/content/744376233t370532},
volume = {65},
year = {2005}
}
@inproceedings{Shi1994a,
abstract = {No feature-based vision system can work unless good features can be identified and tracked from frame to frame. Although tracking itself is by and large a solved problem, selecting features that can be tracked well and correspond to physical points in the world is still hard. We propose a feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not...},
author = {Shi, Jianbo and Tomasi, Carlo},
file = {:home/seb/git/diplom/papers/Shi, Tomasi\_1994\_Good Features to Track.pdf:pdf},
keywords = {computer,features,klt,vision},
mendeley-tags = {computer,features,klt,vision},
title = {{Good Features to Track}},
type = {Conference proceedings (article)},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.36.2669},
year = {1994}
}
@inbook{Bay2006a,
abstract = {In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF’s strong performance.},
author = {Bay, Herbert and Tuytelaars, Tinne and {Van Gool}, Luc},
booktitle = {Computer Vision – ECCV 2006},
chapter = {32},
doi = {10.1007/11744023\_32},
editor = {Leonardis, Ale\v{s} and Bischof, Horst and Pinz, Axel},
isbn = {978-3-540-33832-1},
pages = {404--417},
publisher = {Springer Berlin Heidelberg},
series = {Lecture Notes in Computer Science},
title = {{SURF: Speeded Up Robust Features}},
type = {Book part (with own title)},
url = {http://www.springerlink.com/content/e580h2k58434p02k},
volume = {3951},
year = {2006}
}
@inproceedings{Hoyer2002,
abstract = {[Non-negative sparse coding, non-negative matrix factorization, sparseness] Non-negative sparse coding is a method for decomposing multivariate data into non-negative sparse components. We briefly describe the motivation behind this type of data representation and its relation to standard sparse coding and non-negative matrix factorization. We then give a simple yet efficient multiplicative algorithm for finding the optimal values of the hidden components. In addition, we show how the basis vectors can be learned from the observed data. Simulations demonstrate the effectiveness of the proposed method.},
author = {Hoyer, P.O.},
booktitle = {Neural Networks for Signal Processing, 2002. Proceedings of the 2002 12th IEEE Workshop on},
keywords = {coding,sparse},
mendeley-tags = {coding,sparse},
pages = {557--565},
title = {{Non-negative sparse coding}},
type = {Conference proceedings (whole)},
url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1030067},
year = {2002}
}
@article{Mailhe2009,
author = {Mailhe, B. and Gribonval, R. and Bimbot, F. and Lemay, M. and Vandergheynst, P. and Vesin, J.-M.},
doi = {10.1109/ICASSP.2009.4959621},
file = {:home/seb/git/diplom/papers/Mailhe et al.\_2009\_Dictionary learning for the sparse modelling of atrial fibrillation in ECG signals.pdf:pdf},
isbn = {978-1-4244-2353-8},
journal = {2009 IEEE International Conference on Acoustics, Speech and Signal Processing},
month = apr,
pages = {465--468},
publisher = {Ieee},
title = {{Dictionary learning for the sparse modelling of atrial fibrillation in ECG signals}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4959621},
year = {2009}
}
@article{Pati1993,
abstract = {We describe a recursive algorithm to compute representations of functions with respect to nonorthogonal and possibly overcomplete dictionaries of elementary building blocks e.g. affine (wavelet) frames. We propose a modification to the matching pursuit algorithm of Mallat and Zhang (1992) that maintains full backward orthogonality of the residual (error) at every step and thereby leads to improved convergence. We refer to this modified algorithm as orthogonal matching pursuit (OMP). It is shown that all additional computation required for the OMP algorithm may be performed recursively},
author = {Pati, Y C and Rezaiifar, R and Krishnaprasad, P S},
doi = {10.1109/ACSSC.1993.342465},
isbn = {0818641207},
issn = {10586393},
journal = {Proceedings of 27th Asilomar Conference on Signals Systems and Computers},
pages = {40--44},
publisher = {IEEE Comput. Soc. Press},
title = {{Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=342465},
volume = {1},
year = {1993}
}
@misc{OpenMP,
author = {OpenMP},
title = {{OpenMP}},
url = {http://openmp.org/wp}
}
@inproceedings{Lee_NIPS_2007,
abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons. 1},
author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y.},
file = {:home/seb/git/diplom/papers/Lee et al.\_2007\_Efficient sparse coding algorithms.pdf:pdf},
keywords = {coding,sparse},
mendeley-tags = {coding,sparse},
pages = {801--808},
title = {{Efficient sparse coding algorithms}},
type = {Conference proceedings (article)},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.69.2112},
year = {2007}
}
@article{Lewicki2000,
abstract = {In an overcomplete basis, the number of basis vectors is greater than the dimensionality of the input, and the representation of an input is not a unique combination of basis vectors. Overcomplete representations have been advocated because they have greater robustness in the presence of noise, can be more sparse, and can have greater flexibility in matching structure in the data. Overcomplete codes have also been proposed as a model of some of the response properties of neurons in primary...},
author = {Lewicki, Michael S. and Sejnowski, Terrence J.},
file = {:home/seb/git/diplom/papers/Lewicki, Sejnowski\_2000\_Learning Overcomplete Representations.pdf:pdf},
journal = {Neural Computation},
keywords = {dictionary,overcomplete,redundancy},
mendeley-tags = {dictionary,overcomplete,redundancy},
number = {2},
pages = {337--365},
title = {{Learning Overcomplete Representations}},
type = {Journal article},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.8986},
volume = {12},
year = {2000}
}
@article{Rubinstein2010,
abstract = {Sparse and redundant representation modeling of data assumes an ability to describe signals as linear combinations of a few atoms from a pre-specified dictionary. As such, the choice of the dictionary that sparsifies the signals is crucial for the success of this model. In general, the choice of a proper dictionary can be done using one of two ways: i) building a sparsifying dictionary based on a mathematical model of the data, or ii) learning a dictionary to perform best on a training set. In this paper we describe the evolution of these two paradigms. As manifestations of the first approach, we cover topics such as wavelets, wavelet packets, contourlets, and curvelets, all aiming to exploit 1-D and 2-D mathematical models for constructing effective dictionaries for signals and images. Dictionary learning takes a different route, attaching the dictionary to a set of examples it is supposed to serve. From the seminal work of Field and Olshausen, through the MOD, the K-SVD, the Generalized PCA and others, this paper surveys the various options such training has to offer, up to the most recent contributions and structures.},
author = {Rubinstein, Ron and Bruckstein, Alfred M. and Elad, Michael},
doi = {10.1109/JPROC.2010.2040551},
file = {:home/seb/git/diplom/papers/Rubinstein, Bruckstein, Elad\_2010\_Dictionaries for Sparse Representation Modeling.pdf:pdf},
issn = {0018-9219},
journal = {Proceedings of the IEEE},
keywords = {dictionaries,sparse\_coding},
mendeley-tags = {dictionaries,sparse\_coding},
month = jun,
number = {6},
pages = {1045--1057},
title = {{Dictionaries for Sparse Representation Modeling}},
type = {Journal article},
url = {http://www.cs.technion.ac.il/\~{}ronrubin/Publications/dictdesign.pdf http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=5452966},
volume = {98},
year = {2010}
}
@article{Jenatton2010,
abstract = {Sparse coding consists in representing signals as sparse linear combinations of atoms selected from a dictionary. We consider an extension of this framework where the atoms are further assumed to be embedded in a tree. This is achieved using a recently introduced tree-structured sparse regularization norm, which has proven useful in several applications. This norm leads to regularized problems that are difficult to optimize, and we propose in this paper efficient algorithms for solving them. More precisely, we show that the proximal operator associated with this norm is computable exactly via a dual approach that can be viewed as the composition of elementary proximal operators. Our procedure has a complexity linear, or close to linear, in the number of atoms, and allows the use of accelerated gradient techniques to solve the tree-structured sparse approximation problem at the same computational cost as traditional ones using the L1-norm. Our method is efficient and scales gracefully to millions of variables, which we illustrate in two types of applications: first, we consider fixed hierarchical dictionaries of wavelets to denoise natural images. Then, we apply our optimization tools in the context of dictionary learning, where learned dictionary elements naturally organize in a prespecified arborescent structure, leading to a better performance in reconstruction of natural image patches. When applied to text documents, our method learns hierarchies of topics, thus providing a competitive alternative to probabilistic topic models.},
author = {Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume and Bach, Francis},
journal = {Work},
title = {{Proximal Methods for Hierarchical Sparse Coding}},
url = {http://arxiv.org/abs/1009.2139},
year = {2010}
}
@article{Delgado2003,
abstract = {Algorithms for data-driven learning of domain-specific overcomplete dictionaries are developed to obtain maximum likelihood and maximum a posteriori dictionary estimates based on the use of Bayesian models with concave/Schur-concave (CSC) negative log priors. Such priors are appropriate for obtaining sparse representations of environmental signals within an appropriately chosen (environmentally matched) dictionary. The elements of the dictionary can be interpreted as concepts, features, or words capable of succinct expression of events encountered in the environment (the source of the measured signals). This is a generalization of vector quantization in that one is interested in a description involving a few dictionary entries (the proverbial "25 words or less"), but not necessarily as succinct as one entry. To learn an environmentally adapted dictionary capable of concise expression of signals generated by the environment, we develop algorithms that iterate between a representative set of sparse representations found by variants of FOCUSS and an update of the dictionary using these sparse representations.Experiments were performed using synthetic data and natural images. For complete dictionaries, we demonstrate that our algorithms have improved performance over other independent component analysis (ICA) methods, measured in terms of signal-to-noise ratios of separated sources. In the overcomplete case, we show that the true underlying dictionary and sparse sources can be accurately recovered. In tests with natural images, learned overcomplete dictionaries are shown to have higher coding efficiency than complete dictionaries; that is, images encoded with an overcomplete dictionary have both higher compression (fewer bits per pixel) and higher accuracy (lower mean square error).},
author = {Delgado, Kenneth Kreutz and Murray, Joseph F. and Rao, Bhaskar D. and Engan, Kjersti and Lee, Te Won and Sejnowski, Terrence J.},
doi = {10.1162/089976603762552951},
issn = {0899-7667},
journal = {Neural Comput.},
keywords = {dictionary,learning,sparse},
mendeley-tags = {dictionary,learning,sparse},
number = {2},
pages = {349--396},
publisher = {MIT Press},
title = {{Dictionary learning algorithms for sparse representation}},
type = {Journal article},
url = {http://portal.acm.org/citation.cfm?id=643340},
volume = {15},
year = {2003}
}
@inproceedings{Engan1999a,
abstract = {A frame design technique for use with vector selection algorithms, for example matching pursuits (MP), is presented. The design algorithm is iterative and requires a training set of signal vectors. The algorithm, called method of optimal directions (MOD), is an improvement of the algorithm presented by Engan, Aase and Husoy see (Proc. ICASSP '98, Seattle, USA, p.1817-20, 1998). The MOD is applied to speech and electrocardiogram (ECG) signals, and the designed frames are tested on signals outside the training sets. Experiments demonstrate that the approximation capabilities, in terms of mean squared error (MSE), of the optimized frames are significantly better than those obtained using frames designed by the algorithm of Engan et. al. Experiments show typical reduction in MSE by 20-50\%},
author = {Engan, K and Aase, S O and {Hakon Husoy}, J},
booktitle = {Acoustics Speech and Signal Processing 1999 ICASSP 99 Proceedings 1999 IEEE International Conference on},
doi = {10.1109/ICASSP.1999.760624},
isbn = {0780350413},
pages = {2443--2446},
publisher = {IEEE Comput. Soc. Press},
title = {{Method of optimal directions for frame design}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=760624},
volume = {5},
year = {1999}
}
@article{Lewicki1999,
abstract = {Not Available},
author = {Lewicki, Michael S and Olshausen, Bruno A},
doi = {10.1364/JOSAA.16.001587},
file = {:home/seb/git/diplom/papers/Lewicki, Olshausen\_1999\_Probabilistic framework for the adaptation and comparison of image codes.pdf:pdf},
issn = {10847529},
journal = {Journal of the Optical Society of America A},
number = {7},
pages = {1587},
publisher = {OSA},
title = {{Probabilistic framework for the adaptation and comparison of image codes}},
url = {http://www.opticsinfobase.org/abstract.cfm?URI=JOSAA-16-7-1587},
volume = {16},
year = {1999}
}
@misc{Tomasi1991a,
abstract = {The factorization method described in this series of reports requires an algorithm to track the motion of features in an image stream. Given the small inter-frame displacement made possible by the factorization approach, the best tracking method turns out to be the one proposed by Lucas and Kanade in 1981. The method defines the measure of match between fixed-size feature windows in the past and current frame as the sum of squared intensity differences over the windows. The displacement is then ...},
author = {Tomasi, Carlo and Kanade, Takeo},
institution = {Carnegie Mellon University},
keywords = {feature,flow,motion,tracking},
mendeley-tags = {feature,flow,motion,tracking},
number = {CMU-CS-91-132},
title = {{Detection and Tracking of Point Features}},
type = {Technical report},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.5770},
year = {1991}
}
@article{Elad2006,
abstract = {We address the image denoising problem, where zero-mean white and homogeneous Gaussian additive noise is to be removed from a given image. The approach taken is based on sparse and redundant representations over trained dictionaries. Using the K-SVD algorithm, we obtain a dictionary that describes the image content effectively. Two training options are considered: using the corrupted image itself, or training on a corpus of high-quality image database. Since the K-SVD is limited in handling small image patches, we extend its deployment to arbitrary image sizes by defining a global image prior that forces sparsity over patches in every location in the image. We show how such Bayesian treatment leads to a simple and effective denoising algorithm. This leads to a state-of-the-art denoising performance, equivalent and sometimes surpassing recently published leading alternative denoising methods.},
author = {Elad, Michael and Aharon, Michal},
institution = {Department of Computer Science, The Technion-Israel Institute of Technology, Haifa 32000, Israel. elad@cs.technion.ac.il},
journal = {IEEE Transactions on Image Processing},
number = {12},
pages = {3736--3745},
pmid = {17153947},
publisher = {IEEE},
title = {{Image denoising via sparse and redundant representations over learned dictionaries.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17153947},
volume = {15},
year = {2006}
}
@article{Chen1998,
author = {Chen, Scott Shaobing and Donoho, David L. and Saunders, Michael A.},
journal = {SIAM Journal on Scientific Computing},
number = {1},
pages = {33--61},
publisher = {SIAM},
title = {{Atomic Decomposition by Basis Pursuit}},
type = {Journal article},
url = {http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal\&id=SJOCE3000020000001000033000001\&idtype=cvips\&gifs=yes http://link.aip.org/link/?SCE/20/33},
volume = {20},
year = {1998}
}
@article{mairal08sparse,
abstract = {Sparse representations of signals have drawn considerable interest in recent years. The assumption that natural signals, such as images, admit a sparse decomposition over a redundant dictionary leads to efficient algorithms for handling such sources of data. In particular, the design of well adapted dictionaries for images has been a major challenge. The K-SVD has been recently proposed for this task and shown to perform very well for various grayscale image processing tasks. In this paper, we address the problem of learning dictionaries for color images and extend the K-SVD-based grayscale image denoising algorithm that appears in. This work puts forward ways for handling nonhomogeneous noise and missing information, paving the way to state-of-the-art results in applications such as color image denoising, demosaicing, and inpainting, as demonstrated in this paper.},
author = {Mairal, Julien and Elad, Michael and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Mairal, Elad, Sapiro\_2008\_Sparse representation for color image restoration.pdf:pdf},
issn = {1057-7149},
journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
keywords = {coding,sparse},
mendeley-tags = {coding,sparse},
month = jan,
number = {1},
pages = {53--69},
pmid = {18229804},
title = {{Sparse representation for color image restoration.}},
type = {Journal article},
volume = {17},
year = {2008}
}
@article{Bach2009c,
author = {Bach, Francis and Mairal, Julien and Ponce, Jean and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Bach et al.\_2009\_Sparse Coding and Dictionary Learning for Image Analysis IV.pdf:pdf},
number = {September},
pages = {1--19},
title = {{Sparse Coding and Dictionary Learning for Image Analysis IV}},
year = {2009}
}
@misc{Aharon2007,
abstract = {Modeling signals by a sparse and redundant representations is drawing a consider-able attention in recent years. Coupled with the ability to train the dictionary using signal examples, these techniques have been shown to lead to state-of-the-art results in a series of recent applications. In this paper we propose a novel structure of such a model for representing image content. The new dictionary is itself a small image, such that every patch in it (in varying location and size) is a possible atom in the representation. We refer to this as the Image-Signature-Dictionary (ISD), and show how it can be trained from image examples. This novel structure enjoys several important features, such as shift and scale flexibilities, and smaller memory and computational requirements, compared to the classical dictionary approach. As a demonstration of these benefits, we present high-quality image denoising results based on this new model.},
author = {Aharon, Michal and Elad, Michael},
file = {:home/seb/git/diplom/papers/Aharon, Elad\_2007\_Sparse and redundant modeling of image content using an image-signature-dictionary.pdf:pdf},
keywords = {better,coding,dictionary,sparse,wavelet},
mendeley-tags = {better,coding,dictionary,sparse,wavelet},
title = {{Sparse and redundant modeling of image content using an image-signature-dictionary}},
type = {Electronic citation},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.106.1807},
year = {2007}
}
@misc{Garage2010a,
author = {Garage, Willow},
title = {{OpenCV Libraries}},
url = {http://opencv.willowgarage.com/},
year = {2010}
}
@article{Wright2008,
abstract = {This paper addresses the problem of generating a super-resolution (SR) image from a single low-resolution input image. We approach this problem from the perspective of compressed sensing. The low-resolution image is viewed as downsampled version of a high-resolution image, whose patches are assumed to have a sparse representation with respect to an over-complete dictionary of prototype signal-atoms. The principle of compressed sensing ensures that under mild conditions, the sparse representation can be correctly recovered from the downsampled signal. We will demonstrate the effectiveness of sparsity as a prior for regularizing the otherwise ill-posed super-resolution problem. We further show that a small set of randomly chosen raw patches from training images of similar statistical nature to the input image generally serve as a good dictionary, in the sense that the computed representation is sparse and the recovered high-resolution image is competitive or even superior in quality to images produced by other SR methods.},
author = {Wright, John and Huang, Thomas},
doi = {10.1109/CVPR.2008.4587647},
isbn = {9781424422425},
issn = {10636919},
journal = {IEEE Conference on Computer Vision and Pattern Recognition (2008)},
pages = {1--8},
publisher = {Ieee},
title = {{Image super-resolution as sparse representation of raw image patches}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4587647},
volume = {0},
year = {2008}
}
@book{Mallat2008,
abstract = {\_Mallat's book is the undisputed reference in this field - it is the only one
that covers the essential material in such breadth and depth. - Laurent
Demanet, Stanford University\_


The new edition of this classic book gives all the major concepts, techniques
and applications of sparse representation, reflecting the key role the subject
plays in today's signal processing. The book clearly presents the standard
representations with Fourier, wavelet and time-frequency transforms, and the
construction of orthogonal bases with fast algorithms. The central concept of
sparsity is explained and applied to signal compression, noise reduction, and
inverse problems, while coverage is given to sparse representations in
redundant dictionaries, super-resolution and compressive sensing applications.


Features:


* Balances presentation of the mathematics with applications to signal
processing

* Algorithms and numerical examples are implemented in WaveLab, a MATLAB
toolbox

* Companion website for instructors and selected solutions and code available
for students


New in this edition


* Sparse signal representations in dictionaries

* Compressive sensing, super-resolution and source separation

* Geometric image processing with curvelets and bandlets

* Wavelets for computer graphics with lifting on surfaces

* Time-frequency audio processing and denoising

* Image compression with JPEG-2000

* New and updated exercises


**A Wavelet Tour of Signal Processing: The Sparse Way**, third edition, is an
invaluable resource for researchers and R\&D engineers wishing to apply the
theory in fields such as image processing, video processing and compression,
bio-sensing, medical imaging, machine vision and communications engineering.


Stephane Mallat is Professor in Applied Mathematics at \'{E}cole Polytechnique,
Paris, France. From 1986 to 1996 he was a Professor at the Courant Institute
of Mathematical Sciences at New York University, and between 2001 and 2007, he
co-founded and became CEO of an image processing semiconductor company.


* Includes all the latest developments since the book was published in 1999,
including its

application to JPEG 2000 and MPEG-4

* Algorithms and numerical examples are implemented in Wavelab, a MATLAB
toolbox

* Balances presentation of the mathematics with applications to signal
processing},
author = {Mallat, Stephane},
edition = {3},
isbn = {0123743702},
keywords = {dictionary,sparse,wavelet},
mendeley-tags = {dictionary,sparse,wavelet},
month = dec,
publisher = {Academic Press},
title = {{A Wavelet Tour of Signal Processing, 3rd ed., Third Edition: The Sparse Way}},
type = {Book},
url = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0123743702 http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0123743702 http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0123743702 http://www.amazon.jp/exec/obidos/ASIN/0123743702 http://www.amazon.co.uk/exec/obidos/ASIN/0123743702/citeulike00-21 http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0123743702 http://www.worldcat.org/isbn/0123743702 htt},
year = {2008}
}
@phdthesis{Krizhevsky2009,
author = {Krizhevsky, Alex},
file = {:home/seb/git/diplom/papers/Krizhevsky\_2009\_Learning Multiple Layers of Features from Tiny Images.pdf:pdf},
keywords = {learning,sparse},
mendeley-tags = {learning,sparse},
title = {{Learning Multiple Layers of Features from Tiny Images}},
type = {Thesis (Master's)},
url = {http://www.cs.toronto.edu/\~{}kriz/learning-features-2009-TR.pdf},
year = {2009}
}
@article{Aharon2006a,
author = {Aharon, Michal},
file = {:home/seb/git/diplom/papers/Aharon\_2006\_Overcomplete Dictionaries for Sparse Representation of Signals.pdf:pdf},
journal = {cs.technion.ac.il},
title = {{Overcomplete Dictionaries for Sparse Representation of Signals}},
url = {http://ftp.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2006/PHD/PHD-2006-15.pdf},
year = {2006}
}
@article{Murray2006,
author = {Murray, Joseph F and Kreutz-Delgado, Kenneth},
doi = {10.1007/s11265-006-0003-z},
file = {:home/seb/git/diplom/papers/Murray, Kreutz-Delgado\_2006\_Learning Sparse Overcomplete Codes for Images.pdf:pdf},
issn = {09225773},
journal = {The Journal of VLSI Signal Processing Systems for Signal Image and Video Technology},
number = {1-2},
pages = {97--110},
title = {{Learning Sparse Overcomplete Codes for Images}},
url = {http://www.springerlink.com/index/10.1007/s11265-006-9774-5},
volume = {45},
year = {2006}
}
@article{Grosse2007,
abstract = {Page 1. - for Roger Grosse Computer Science Dept. Stanford University Stanford, CA 94305},
author = {Grosse, R and Raina, R and Kwong, H and Ng, A Y},
journal = {Cortex},
pages = {8},
publisher = {Citeseer},
title = {{Shift-invariant sparse coding for audio classification}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.2954\&amp;rep=rep1\&amp;type=pdf},
volume = {9},
year = {2007}
}
@article{Efron2004,
abstract = {The purpose of model selection algorithms such as All Subsets, Forward Selection, and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived. (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of Ordinary Least Squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising new model selection method; this connection explains the similar numerical results previously observed for the Lasso and Stagewise, and helps understand the properties of both methods, which are seen as constrained versions of the simpler LARS algorithm. (3) A simple approximation for the degrees of freedom of a LARS estimate is available, from which we derive a Cp estimate of prediction error; this allows a principled choice among the range of possible LARS estimates. LARS and its variants are computationally efficient: the paper describes a publicly available algorithm that requires only the same order of magnitude of computational effort as Ordinary Least Squares applied to the full set of covariates.},
arxivId = {math/0406474v1},
author = {Efron, Bradley and Hastie, Trevor and Johnstone, Iain and Tibshirani, Robert},
doi = {10.1214/009053604000000067},
file = {:home/seb/git/diplom/papers/Efron et al.\_2004\_Least angle regression.pdf:pdf},
institution = {Statistics Department, Stanford University},
issn = {00905364},
journal = {Annals of Statistics},
number = {2},
pages = {407--499},
publisher = {Institute of Mathematical Statistics},
title = {{Least angle regression}},
url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.aos/1083178935/},
volume = {32},
year = {2004}
}
@misc{Strand2005,
abstract = {There are a number of interesting variable selection methods available beside the regular forward selection and stepwise selection methods. Such approaches include LASSO (Least Absolute Shrinkage and Selection Operator), least angle regression (LARS) and elastic net (LARS-EN) regression. There also exists a method for calculating principal components with sparse loadings. This software package contains Matlab implementations of these functions. The standard implementations of these functions are available as add-on packages in S-Plus and R.},
author = {Sjoestrandd, K},
keywords = {{elastic net,sparse,sparsity,variable selection,\{lasso,\} matlab,\} \{lars,\} \{spca}},
publisher = {Informatics and Mathematical Modelling, Technical University of Denmark, \{DTU\}},
title = {{Matlab implementation of LASSO, LARS, the elastic net and SPCA}},
url = {http://octopus.cs.drexel.edu/\~{}gao25/References/},
year = {2005}
}
@article{Bengio2009,
abstract = {Bag-of-words document representations are often used in text, image and video processing. While it is relatively easy to determine a suitable word dictionary for text documents, there is no simple mapping from raw images or videos to dictionary terms. The classical approach builds a dictionary using vector quantization over a large set of useful visual descriptors extracted from a training set, and uses a nearest-neighbor algorithm to count the number of occurrences of each dictionary word in documents to be encoded. More robust approaches have been proposed recently that represent each visual descriptor as a sparse weighted combination of dictionary words. While favoring a sparse representation at the level of visual descriptors, those methods however do not ensure that images have sparse representation. In this work, we use mixed-norm regularization to achieve sparsity at the image level as well as a small overall dictionary. This approach can also be used to encourage using the same dictionary words for all the images in a class, providing a discriminative signal in the construction of image on a benchmark image classification dataset show that when compact image or dictionary representations are needed for computational efficiency, the proposed approach yields better mean average precision in classification.},
author = {Bengio, Samy and Pereira, Fernando and Singer, Yoram and Strelow, Dennis},
editor = {Bengio, Y and Schuurmans, D and Lafferty, J and Williams, C K I and Culotta, A},
journal = {Pattern Recognition},
pages = {1--8},
publisher = {Citeseer},
title = {{Group Sparse Coding}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.153.8544\&amp;rep=rep1\&amp;type=pdf},
year = {2009}
}
@phdthesis{Chen1995,
author = {Chen, Shaobing},
booktitle = {Carbon},
pages = {41--44},
school = {Department of Statistics},
title = {{Basis Pursuit}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.3.960\&amp;rep=rep1\&amp;type=pdf},
volume = {1},
year = {1995}
}
@misc{Eigen,
author = {Eigen},
title = {{Eigen}},
url = {http://eigen.tuxfamily.org}
}
@inproceedings{Jojic2003a,
abstract = {We present novel simple appearance and shape models that we call
 epitomes. The epitome of an image is its miniature, condensed
 version containing the essence of the textural and shape properties
 of the image. As opposed to previously used simple image models,
 such as templates or basis functions, the size of the epitome is
 considerably smaller than the size of the image or object it
 represents, but the epitome still contains most constitute elements
 needed to reconstruct the image (Fig. 1). A collection of images
 often shares an epitome, e.g., when images are a few consecutive
 frames from a video sequence, or when they are photographs of
 similar objects.A particular image in a collection is defined by
 its epitome and a smooth mapping from the epitome to the image
 pixels. When the epitomic representation is used within a
 hierarchical generative model, appropriate inference algorithms can
 be derived to extract the epitome from a single image or a
 collection of images and at the same time perform various inference
 tasks, such as image segmentation, motion estimation, object
 removal and super-resolution.},
author = {Jojic, Nebojsa and Frey, Brendan J. and Kannan, Anitha},
isbn = {0-7695-1950-4},
keywords = {epitome},
mendeley-tags = {epitome},
publisher = {IEEE Computer Society},
title = {{Epitomic analysis of appearance and shape}},
type = {Conference proceedings (article)},
url = {http://portal.acm.org/citation.cfm?id=946676},
year = {2003}
}
@article{Mairal2009b,
abstract = {We propose in this paper to unify two different approaches to image restoration: On the one hand, learning a basis set (dictionary) adapted to sparse signal descriptions has proven to be very effective in image reconstruction and classification tasks. On the other hand, explicitly exploiting the self-similarities of natural images has led to the successful non-local means approach to image restoration. We propose simultaneous sparse coding as a framework for combining these two approaches in a natural manner. This is achieved by jointly decomposing groups of similar signals on subsets of the learned dictionary. Experimental results in image denoising and demosaicking tasks with synthetic and real noise show that the proposed method outperforms the state of the art, making it possible to effectively restore raw images from digital cameras at a reasonable speed and memory cost.},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo and Zisserman, Andrew},
doi = {10.1109/ICCV.2009.5459452},
isbn = {9781424444205},
issn = {15505499},
journal = {2009 IEEE 12th International Conference on Computer Vision},
number = {Iccv},
pages = {2272--2279},
publisher = {Ieee},
title = {{Non-local sparse models for image restoration}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=5459452},
volume = {2},
year = {2009}
}
@article{Jenatton2010b,
author = {Jenatton, R and Mairal, J and Obozinski, G and Bach, Francis},
file = {:home/seb/git/diplom/papers/Jenatton et al.\_2010\_Proximal methods for sparse hierarchical dictionary learning.pdf:pdf},
journal = {ICML},
title = {{Proximal methods for sparse hierarchical dictionary learning}},
url = {http://snowbird.djvuzone.org/2010/abstracts/103.pdf},
year = {2010}
}
@misc{Bruckstein2007,
abstract = {A full-rank matrix A ∈ IR n×m with n \&lt; m generates an underdetermined system of linear equations Ax = b having infinitely many solutions. Suppose we seek the sparsest solution, i.e., the one with the fewest nonzero entries: can it ever be unique? If so, when? As optimization of sparsity is combinatorial in nature, are there efficient methods for finding the sparsest solution? These questions have been answered positively and constructively in recent years, exposing a wide variety of surprising phenomena; in particular, the existence of easily-verifiable conditions under which optimally-sparse solutions can be found by concrete, effective computational methods. Such theoretical results inspire a bold perspective on some important practical problems in signal and image processing. Several well-known signal and image processing problems can be cast as demanding solutions of undetermined systems of equations. Such problems have previously seemed, to many, intractable. There is considerable evidence that these problems often have sparse solutions. Hence, advances in finding sparse solutions to underdetermined systems energizes research on such signal and image processing problems – to striking effect. In this paper we review the theoretical results on sparse solutions of linear systems, empirical},
author = {Bruckstein, Alfred M. and Donoho, David L. and Elad, Michael},
keywords = {sparse},
mendeley-tags = {sparse},
title = {{From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images ∗}},
type = {Electronic citation},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.102.4697},
year = {2007}
}
@misc{Blumensath2007,
author = {Blumensath, Thomas and Davies, M.E.},
booktitle = {Proceedings of the IEEE},
file = {:home/seb/git/diplom/papers/Blumensath, Davies\_2007\_On the difference between orthogonal matching pursuit and orthogonal least squares.pdf:pdf},
institution = {Citeseer},
number = {0},
pages = {1--3},
title = {{On the difference between orthogonal matching pursuit and orthogonal least squares}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.63.3258\&amp;rep=rep1\&amp;type=pdf},
volume = {44},
year = {2007}
}
@article{Tibshirani1996,
abstract = {We propose a new method for estimation in linear models. The `lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.},
author = {Tibshirani, R},
doi = {10.1111/j.1553-2712.2009.0451c.x},
file = {:home/seb/git/diplom/papers/Tibshirani\_1996\_Regression shrinkage and selection via the lasso.pdf:pdf},
institution = {Department of Statistics, University of Toronto},
issn = {00359246},
journal = {Journal of the Royal Statistical Society Series B Methodological},
number = {1},
pages = {267--288},
publisher = {Blackwell Publishing; Royal Statistical Society},
series = {B},
title = {{Regression shrinkage and selection via the lasso}},
url = {http://www.jstor.org/stable/2346178},
volume = {58},
year = {1996}
}
@article{Sand-Jensen2007a,
author = {Sand-Jensen, Kaj},
doi = {10.1111/j.2007.0030-1299.15674.x},
issn = {0030-1299},
journal = {Oikos},
month = may,
number = {5},
pages = {723--727},
publisher = {Blackwell Publishing},
title = {{How to write consistently boring scientific literature}},
type = {Journal article},
url = {http://www.blackwell-synergy.com/doi/abs/10.1111/j.2007.0030-1299.15674.x http://www.ingentaconnect.com/content/mksg/oki/2007/00000116/00000005/art00001},
volume = {116},
year = {2007}
}
@article{Mairal2010b,
abstract = {Modeling data with linear combinations of a few elements from a learned dictionary has been the focus of much recent research in machine learning, neuroscience and signal processing. For signals such as natural images that admit such sparse representations, it is now well established that these models are well suited to restoration tasks. In this context, learning the dictionary amounts to solving a large-scale matrix factorization problem, which can be done efficiently with classical optimization tools. The same approach has also been used for learning features from data for other purposes, e.g., image classification, but tuning the dictionary in a supervised way for these tasks has proven to be more difficult. In this paper, we present a general formulation for supervised dictionary learning adapted to a wide variety of tasks, and present an efficient algorithm for solving the corresponding optimization problem. Experiments on handwritten digit classification, digital art identification, nonlinear inverse image problems, and compressed sensing demonstrate that our approach is effective in large-scale settings, and is well suited to supervised and semi-supervised classification, as well as regression tasks for data that admit sparse representations.},
author = {Mairal, Julien and Bach, Francis and Ponce, Jean},
journal = {Bach},
number = {September},
title = {{Task-Driven Dictionary Learning}},
url = {http://arxiv.org/abs/1009.5358},
volume = {stat.ML},
year = {2010}
}
@article{Bach2009a,
author = {Bach, Francis and Mairal, Julien and Ponce, Jean and Sapiro, Guillermo},
file = {:home/seb/git/diplom/papers/Bach et al.\_2009\_Sparse Coding and Dictionary Learning for Image Analysis II.pdf:pdf},
journal = {Learning},
number = {September},
title = {{Sparse Coding and Dictionary Learning for Image Analysis II}},
year = {2009}
}
