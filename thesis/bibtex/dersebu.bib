@article{Osborne2000New,
    abstract = {The title Lasso has been suggested by Tibshirani (1996) as a colourful name for a technique of variable selection which requires the minimization of a sum of squares subject to an l1 bound {kappa} on the solution. This forces zero components in the minimizing solution for small values of {kappa}. Thus this bound can function as a selection parameter. This paper makes two contributions to computational problems associated with implementing the Lasso: (1) a compact descent method for solving the constrained problem for a particular value of {kappa} is formulated, and (2) a homotopy method, in which the constraint bound {kappa} becomes the homotopy parameter, is developed to completely describe the possible selection regimes. Both algorithms have a finite termination property. It is suggested that modified Gram-Schmidt orthogonalization applied to an augmented design matrix provides an effective basis for implementing the algorithms. 10.1093/imanum/20.3.389},
    author = {Osborne, M. R. and Presnell, B. and Turlach, B. A.},
    citeulike-article-id = {7675119},
    citeulike-linkout-0 = {http://dx.doi.org/10.1093/imanum/20.3.389},
    citeulike-linkout-1 = {http://imajna.oxfordjournals.org/cgi/content/abstract/20/3/389},
    day = {1},
    doi = {10.1093/imanum/20.3.389},
    journal = {IMA J Numer Anal},
    month = {July},
    number = {3},
    pages = {389--403},
    posted-at = {2010-08-18 14:44:31},
    priority = {2},
    title = {A new approach to variable selection in least squares problems},
    url = {http://dx.doi.org/10.1093/imanum/20.3.389},
    volume = {20},
    year = {2000}
}

@article{Aharon2006KSVD,
    abstract = {In recent years there has been a growing interest in the study of sparse representation of signals. Using an overcomplete dictionary that contains prototype signal-atoms, signals are described by sparse linear combinations of these atoms. Applications that use sparse representation are many and include compression, regularization in inverse problems, feature extraction, and more. Recent activity in this field has concentrated mainly on the study of pursuit algorithms that decompose signals with respect to a given dictionary. Designing dictionaries to better fit the above model can be done by either selecting one from a prespecified set of linear transforms or adapting the dictionary to a set of training signals. Both of these techniques have been considered, but this topic is largely still open. In this paper we propose a novel algorithm for adapting dictionaries in order to achieve sparse signal representations. Given a set of training signals, we seek the dictionary that leads to the best representation for each member in this set, under strict sparsity constraints. We present a new method—the<tex>\$ K\$</tex>-SVD algorithm—generalizing the<tex>\$ K\$</tex>-means clustering process.<tex>\$ K\$</tex>-SVD is an iterative method that alternates between sparse coding of the examples based on the current dictionary and a process of updating the dictionary atoms to better fit the data. The update of the dictionary columns is combined with an update of the sparse representations, thereby accelerating convergence. The<tex>\$ K\$</tex>-SVD algorithm is flexible and can work with any pursuit method (e.g., basis pursuit, FOCUSS, or matching pursuit). We analyze this algorithm and demonstrate its results both on synthetic tests and in applications on real image data.},
    author = {Aharon, M. and Elad, M. and Bruckstein, A.},
    booktitle = {Signal Processing, IEEE Transactions on [see also Acoustics, Speech, and Signal Processing, IEEE Transactions on]},
    citeulike-article-id = {1930392},
    citeulike-linkout-0 = {http://dx.doi.org/10.1109/TSP.2006.881199},
    citeulike-linkout-1 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1710377},
    citeulike-linkout-2 = {http://adsabs.harvard.edu/cgi-bin/nph-bib\_query?bibcode=2006ITSP...54.4311A},
    doi = {10.1109/TSP.2006.881199},
    journal = {Signal Processing, IEEE Transactions on [see also Acoustics, Speech, and Signal Processing, IEEE Transactions on]},
    number = {11},
    pages = {4311--4322},
    posted-at = {2010-08-18 14:37:39},
    priority = {2},
    title = {K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation},
    url = {http://dx.doi.org/10.1109/TSP.2006.881199},
    volume = {54},
    year = {2006}
}

@article{Chen1998Atomic,
    author = {Chen, Scott S. and Donoho, David L. and Saunders, Michael A.},
    citeulike-article-id = {3441263},
    citeulike-linkout-0 = {http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal\&id=SJOCE3000020000001000033000001\&idtype=cvips\&gifs=yes},
    citeulike-linkout-1 = {http://link.aip.org/link/?SCE/20/33},
    journal = {SIAM Journal on Scientific Computing},
    number = {1},
    pages = {33--61},
    posted-at = {2010-08-18 14:37:11},
    priority = {2},
    publisher = {SIAM},
    title = {Atomic Decomposition by Basis Pursuit},
    url = {http://scitation.aip.org/getabs/servlet/GetabsServlet?prog=normal\&id=SJOCE3000020000001000033000001\&idtype=cvips\&gifs=yes},
    volume = {20},
    year = {1998}
}

@article{Lewicki2000Learning,
    abstract = {In an overcomplete basis, the number of basis vectors is greater than the dimensionality
of the input, and the representation of an input is not a unique combination of basis
vectors. Overcomplete representations have been advocated because they have greater
robustness in the presence of noise, can be more sparse, and can have greater flexibility
in matching structure in the data. Overcomplete codes have also been proposed as a
model of some of the response properties of neurons in primary...},
    author = {Lewicki, Michael S. and Sejnowski, Terrence J.},
    citeulike-article-id = {1597267},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.8986},
    journal = {Neural Computation},
    keywords = {dictionary, overcomplete, redundancy},
    number = {2},
    pages = {337--365},
    posted-at = {2010-08-10 18:06:11},
    priority = {2},
    title = {Learning Overcomplete Representations},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.43.8986},
    volume = {12},
    year = {2000}
}

@electronic{Aharon2007Sparse,
    abstract = {Modeling signals by a sparse and redundant representations is drawing a consider-able attention in recent years. Coupled with the ability to train the dictionary using signal examples, these techniques have been shown to lead to state-of-the-art results in a series of recent applications. In this paper we propose a novel structure of such a model for representing image content. The new dictionary is itself a small image, such that every patch in it (in varying location and size) is a possible atom in the representation. We refer to this as the Image-Signature-Dictionary (ISD), and show how it can be trained from image examples. This novel structure enjoys several important features, such as shift and scale flexibilities, and smaller memory and computational requirements, compared to the classical dictionary approach. As a demonstration of these benefits, we present high-quality image denoising results based on this new model.},
    author = {Aharon, Michal and Elad, Michael},
    citeulike-article-id = {7571400},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.106.1807},
    keywords = {better, coding, dictionary, sparse, wavelet},
    posted-at = {2010-08-04 14:13:39},
    priority = {2},
    title = {Sparse and redundant modeling of image content using an image-signature-dictionary},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.106.1807},
    year = {2007}
}

@techreport{Tomasi1991Detection,
    abstract = {The factorization method described in this series of reports requires an algorithm to track the motion of features in an image stream. Given the small inter-frame displacement made possible by the factorization approach, the best tracking method turns out to be the one proposed by Lucas and Kanade in 1981. The method defines the measure of match between fixed-size feature windows in the past and current frame as the sum of squared intensity differences over the windows. The displacement is then ...},
    author = {Tomasi, Carlo and Kanade, Takeo},
    citeulike-article-id = {1283567},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.5770},
    institution = {Carnegie Mellon University},
    keywords = {feature, flow, motion, tracking},
    month = {April},
    number = {CMU-CS-91-132},
    posted-at = {2010-08-04 13:45:57},
    priority = {2},
    title = {Detection and Tracking of Point Features},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.5770},
    year = {1991}
}

@proceedings{Hoyer2002Nonnegative,
    abstract = {[Non-negative sparse coding, non-negative matrix factorization, sparseness] Non-negative sparse coding is a method for decomposing multivariate data into non-negative sparse components. We briefly describe the motivation behind this type of data representation and its relation to standard sparse coding and non-negative matrix factorization. We then give a simple yet efficient multiplicative algorithm for finding the optimal values of the hidden components. In addition, we show how the basis vectors can be learned from the observed data. Simulations demonstrate the effectiveness of the proposed method.},
    author = {Hoyer, P. O.},
    citeulike-article-id = {423538},
    citeulike-linkout-0 = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1030067},
    journal = {Neural Networks for Signal Processing, 2002. Proceedings of the 2002 12th IEEE Workshop on},
    keywords = {coding, sparse},
    pages = {557--565},
    posted-at = {2010-08-04 13:40:26},
    priority = {2},
    title = {Non-negative sparse coding},
    url = {http://ieeexplore.ieee.org/xpls/abs\_all.jsp?arnumber=1030067},
    year = {2002}
}

@inproceedings{Lee_NIPS_2007,
    abstract = {Sparse coding provides a class of algorithms for finding succinct representations of stimuli; given only unlabeled input data, it discovers basis functions that capture higher-level features in the data. However, finding sparse codes remains a very difficult computational problem. In this paper, we present efficient sparse coding algorithms that are based on iteratively solving two convex optimization problems: an L1-regularized least squares problem and an L2-constrained least squares problem. We propose novel algorithms to solve both of these optimization problems. Our algorithms result in a significant speedup for sparse coding, allowing us to learn larger sparse codes than possible with previously described algorithms. We apply these algorithms to natural images and demonstrate that the inferred sparse codes exhibit end-stopping and non-classical receptive field surround suppression and, therefore, may provide a partial explanation for these two phenomena in V1 neurons. 1},
    author = {Lee, Honglak and Battle, Alexis and Raina, Rajat and Ng, Andrew Y.},
    booktitle = {In NIPS},
    citeulike-article-id = {6878193},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.69.2112},
    keywords = {coding, sparse},
    pages = {801--808},
    posted-at = {2010-08-03 22:39:52},
    priority = {2},
    title = {Efficient sparse coding algorithms},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.69.2112},
    year = {2007}
}

@electronic{Bruckstein2007From,
    abstract = {A full-rank matrix A ∈ IR n×m with n \&lt; m generates an underdetermined system of linear equations Ax = b having infinitely many solutions. Suppose we seek the sparsest solution, i.e., the one with the fewest nonzero entries: can it ever be unique? If so, when? As optimization of sparsity is combinatorial in nature, are there efficient methods for finding the sparsest solution? These questions have been answered positively and constructively in recent years, exposing a wide variety of surprising phenomena; in particular, the existence of easily-verifiable conditions under which optimally-sparse solutions can be found by concrete, effective computational methods. Such theoretical results inspire a bold perspective on some important practical problems in signal and image processing. Several well-known signal and image processing problems can be cast as demanding solutions of undetermined systems of equations. Such problems have previously seemed, to many, intractable. There is considerable evidence that these problems often have sparse solutions. Hence, advances in finding sparse solutions to underdetermined systems energizes research on such signal and image processing problems – to striking effect. In this paper we review the theoretical results on sparse solutions of linear systems, empirical},
    author = {Bruckstein, Alfred M. and Donoho, David L. and Elad, Michael},
    citeulike-article-id = {3975044},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.102.4697},
    keywords = {sparse},
    posted-at = {2010-08-03 22:11:06},
    priority = {2},
    title = {From Sparse Solutions of Systems of Equations to Sparse Modeling of Signals and Images},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.102.4697},
    year = {2007}
}

@inproceedings{Shi1994Good,
    abstract = {No feature-based vision system can work unless good features can be identified and tracked from frame to frame. Although tracking itself is by and large a solved problem, selecting features that can be tracked well and correspond to physical points in the world is still hard. We propose a feature selection criterion that is optimal by construction because it is based on how the tracker works, and a feature monitoring method that can detect occlusions, disocclusions, and features that do not...},
    address = {Seattle},
    author = {Shi, Jianbo and Tomasi, Carlo},
    booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR'94)},
    citeulike-article-id = {317333},
    citeulike-linkout-0 = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.36.2669},
    keywords = {computer, features, klt, vision},
    month = {June},
    posted-at = {2010-08-03 22:04:55},
    priority = {2},
    title = {Good Features to Track},
    url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.36.2669},
    year = {1994}
}

@article{mairal08sparse,
    abstract = {Sparse representations of signals have drawn considerable interest in recent years. The assumption that natural signals, such as images, admit a sparse decomposition over a redundant dictionary leads to efficient algorithms for handling such sources of data. In particular, the design of well adapted dictionaries for images has been a major challenge. The K-SVD has been recently proposed for this task and shown to perform very well for various grayscale image processing tasks. In this paper, we address the problem of learning dictionaries for color images and extend the K-SVD-based grayscale image denoising algorithm that appears in. This work puts forward ways for handling nonhomogeneous noise and missing information, paving the way to state-of-the-art results in applications such as color image denoising, demosaicing, and inpainting, as demonstrated in this paper.},
    author = {Mairal, Julien and Elad, Michael and Sapiro, Guillermo},
    citeulike-article-id = {4755426},
    citeulike-linkout-0 = {http://view.ncbi.nlm.nih.gov/pubmed/18229804},
    citeulike-linkout-1 = {http://www.hubmed.org/display.cgi?uids=18229804},
    issn = {1057-7149},
    journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
    keywords = {coding, sparse},
    month = {January},
    number = {1},
    pages = {53--69},
    posted-at = {2010-08-03 21:06:12},
    priority = {2},
    title = {Sparse representation for color image restoration.},
    url = {http://view.ncbi.nlm.nih.gov/pubmed/18229804},
    volume = {17},
    year = {2008}
}

@book{Mallat2008Wavelet,
    abstract = {\_Mallat's book is the undisputed reference in this field - it is the only one
that covers the essential material in such breadth and depth. - Laurent
Demanet, Stanford University\_


The new edition of this classic book gives all the major concepts, techniques
and applications of sparse representation, reflecting the key role the subject
plays in today's signal processing. The book clearly presents the standard
representations with Fourier, wavelet and time-frequency transforms, and the
construction of orthogonal bases with fast algorithms. The central concept of
sparsity is explained and applied to signal compression, noise reduction, and
inverse problems, while coverage is given to sparse representations in
redundant dictionaries, super-resolution and compressive sensing applications.


Features:


* Balances presentation of the mathematics with applications to signal
processing

* Algorithms and numerical examples are implemented in WaveLab, a MATLAB
toolbox

* Companion website for instructors and selected solutions and code available
for students


New in this edition


* Sparse signal representations in dictionaries

* Compressive sensing, super-resolution and source separation

* Geometric image processing with curvelets and bandlets

* Wavelets for computer graphics with lifting on surfaces

* Time-frequency audio processing and denoising

* Image compression with JPEG-2000

* New and updated exercises


**A Wavelet Tour of Signal Processing: The Sparse Way**, third edition, is an
invaluable resource for researchers and R\&D engineers wishing to apply the
theory in fields such as image processing, video processing and compression,
bio-sensing, medical imaging, machine vision and communications engineering.


Stephane Mallat is Professor in Applied Mathematics at \'{E}cole Polytechnique,
Paris, France. From 1986 to 1996 he was a Professor at the Courant Institute
of Mathematical Sciences at New York University, and between 2001 and 2007, he
co-founded and became CEO of an image processing semiconductor company.


* Includes all the latest developments since the book was published in 1999,
including its

application to JPEG 2000 and MPEG-4

* Algorithms and numerical examples are implemented in Wavelab, a MATLAB
toolbox

* Balances presentation of the mathematics with applications to signal
processing},
    author = {Mallat, Stephane},
    citeulike-article-id = {3974506},
    citeulike-linkout-0 = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20\&amp;path=ASIN/0123743702},
    citeulike-linkout-1 = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21\&amp;path=ASIN/0123743702},
    citeulike-linkout-2 = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21\&amp;path=ASIN/0123743702},
    citeulike-linkout-3 = {http://www.amazon.jp/exec/obidos/ASIN/0123743702},
    citeulike-linkout-4 = {http://www.amazon.co.uk/exec/obidos/ASIN/0123743702/citeulike00-21},
    citeulike-linkout-5 = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20\&path=ASIN/0123743702},
    citeulike-linkout-6 = {http://www.worldcat.org/isbn/0123743702},
    citeulike-linkout-7 = {http://books.google.com/books?vid=ISBN0123743702},
    citeulike-linkout-8 = {http://www.amazon.com/gp/search?keywords=0123743702\&index=books\&linkCode=qs},
    citeulike-linkout-9 = {http://www.librarything.com/isbn/0123743702},
    day = {25},
    edition = {3},
    howpublished = {Hardcover},
    isbn = {0123743702},
    keywords = {dictionary, sparse, wavelet},
    month = {December},
    posted-at = {2010-08-03 21:04:50},
    priority = {2},
    publisher = {Academic Press},
    title = {A Wavelet Tour of Signal Processing, 3rd ed., Third Edition: The Sparse Way},
    url = {http://www.worldcat.org/isbn/0123743702},
    year = {2008}
}

@inproceedings{Mairal2009Online,
    author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
    booktitle = {International Conference on Machine Learning},
    citeulike-article-id = {7522808},
    citeulike-linkout-0 = {http://www.di.ens.fr/willow/pdfs/icml09.pdf},
    keywords = {base, coding, dictionary, image, learning, online, sparse},
    location = {Montreal, Canada},
    posted-at = {2010-07-20 14:26:19},
    priority = {2},
    title = {Online Dictionary Learning for Sparse Coding.},
    url = {http://www.di.ens.fr/willow/pdfs/icml09.pdf},
    year = {2009}
}

@inproceedings{Yee2004Perceptual,
    abstract = {This paper describes a perceptually-based image comparison process that can be used to tell when images are perceptually identical even though they contain some numerical differences. The technique has shown much utility in the production testing of rendering software.},
    address = {New York, NY, USA},
    author = {Yee, Yangli H. and Newman, Anna},
    booktitle = {SIGGRAPH '04: ACM SIGGRAPH 2004 Sketches},
    citeulike-article-id = {7498930},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=1186374},
    citeulike-linkout-1 = {http://dx.doi.org/10.1145/1186223.1186374},
    doi = {10.1145/1186223.1186374},
    isbn = {1-59593-896-2},
    location = {Los Angeles, California},
    pages = {121+},
    posted-at = {2010-07-16 10:18:53},
    priority = {4},
    publisher = {ACM},
    title = {A perceptual metric for production testing},
    url = {http://dx.doi.org/10.1145/1186223.1186374},
    year = {2004}
}

@mastersthesis{Krizhevsky2009Learning,
    author = {Krizhevsky, Alex},
    citeulike-article-id = {7491128},
    citeulike-linkout-0 = {http://www.cs.toronto.edu/\~{}kriz/learning-features-2009-TR.pdf},
    keywords = {learning, sparse},
    posted-at = {2010-07-15 10:17:28},
    priority = {2},
    title = {Learning Multiple Layers of Features from Tiny Images},
    url = {http://www.cs.toronto.edu/\~{}kriz/learning-features-2009-TR.pdf},
    year = {2009}
}

@incollection{Bay2006SURF,
    abstract = {In this paper, we present a novel scale- and rotation-invariant interest point detector and descriptor, coined SURF (Speeded Up Robust Features). It approximates or even outperforms previously proposed schemes with respect to repeatability, distinctiveness, and robustness, yet can be computed and compared much faster. This is achieved by relying on integral images for image convolutions; by building on the strengths of the leading existing detectors and descriptors (in casu, using a Hessian matrix-based measure for the detector, and a distribution-based descriptor); and by simplifying these methods to the essential. This leads to a combination of novel detection, description, and matching steps. The paper presents experimental results on a standard evaluation set, as well as on imagery obtained in the context of a real-life object recognition application. Both show SURF's strong performance.},
    address = {Berlin, Heidelberg},
    author = {Bay, Herbert and Tuytelaars, Tinne and Van Gool, Luc},
    booktitle = {Computer Vision – ECCV 2006 },
    chapter = {32},
    citeulike-article-id = {2708013},
    citeulike-linkout-0 = {http://dx.doi.org/10.1007/11744023\_32},
    citeulike-linkout-1 = {http://www.springerlink.com/content/e580h2k58434p02k},
    doi = {10.1007/11744023\_32},
    editor = {Leonardis, Ale\v{s} and Bischof, Horst and Pinz, Axel},
    isbn = {978-3-540-33832-1},
    journal = {Computer Vision – ECCV 2006},
    pages = {404--417},
    posted-at = {2010-07-05 13:37:28},
    priority = {2},
    publisher = {Springer Berlin Heidelberg},
    series = {Lecture Notes in Computer Science},
    title = {SURF: Speeded Up Robust Features},
    url = {http://dx.doi.org/10.1007/11744023\_32},
    volume = {3951},
    year = {2006}
}

@inproceedings{Jojic2003Epitomic,
    abstract = {We present novel simple appearance and shape models that we call
 epitomes. The epitome of an image is its miniature, condensed
 version containing the essence of the textural and shape properties
 of the image. As opposed to previously used simple image models,
 such as templates or basis functions, the size of the epitome is
 considerably smaller than the size of the image or object it
 represents, but the epitome still contains most constitute elements
 needed to reconstruct the image (Fig. 1). A collection of images
 often shares an epitome, e.g., when images are a few consecutive
 frames from a video sequence, or when they are photographs of
 similar objects.A particular image in a collection is defined by
 its epitome and a smooth mapping from the epitome to the image
 pixels. When the epitomic representation is used within a
 hierarchical generative model, appropriate inference algorithms can
 be derived to extract the epitome from a single image or a
 collection of images and at the same time perform various inference
 tasks, such as image segmentation, motion estimation, object
 removal and super-resolution.},
    address = {Washington, DC, USA},
    author = {Jojic, Nebojsa and Frey, Brendan J. and Kannan, Anitha},
    booktitle = {ICCV '03: Proceedings of the Ninth IEEE International Conference on Computer Vision},
    citeulike-article-id = {7345820},
    citeulike-linkout-0 = {http://portal.acm.org/citation.cfm?id=946676},
    isbn = {0-7695-1950-4},
    keywords = {epitome},
    pages = {34+},
    posted-at = {2010-07-05 13:34:05},
    priority = {3},
    publisher = {IEEE Computer Society},
    title = {Epitomic analysis of appearance and shape},
    url = {http://portal.acm.org/citation.cfm?id=946676},
    year = {2003}
}

@inproceedings{Wang2008Factoring,
    abstract = {We reduce transmission bandwidth and memory space for images by factoring their repeated content. A transform map and a condensed epitome are created such that all image blocks can be reconstructed from transformed epitome patches. The transforms may include affine deformation and color scaling to account for perspective and tonal variations across the image. The factored representation allows efficient random-access through a simple indirection, and can therefore be used for real-time texture mapping without expansion in memory. Our scheme is orthogonal to traditional image compression, in the sense that the epitome is amenable to further compression such as DXT. Moreover it allows a new mode of progressivity, whereby generic features appear before unique detail. Factoring is also effective across a collection of images, particularly in the context of image-based rendering. Eliminating redundant content lets us include textures that are several times as large in the same memory space.},
    address = {New York, NY, USA},
    author = {Wang, Huamin and Wexler, Yonatan and Ofek, Eyal and Hoppe, Hugues},
    booktitle = {SIGGRAPH '08: ACM SIGGRAPH 2008 papers},
    citeulike-article-id = {3313606},
    citeulike-linkout-0 = {http://research.microsoft.com/en-us/um/people/hoppe/factorimage.pdf},
    citeulike-linkout-1 = {http://portal.acm.org/citation.cfm?id=1399504.1360613},
    citeulike-linkout-2 = {http://dx.doi.org/10.1145/1399504.1360613},
    doi = {10.1145/1399504.1360613},
    isbn = {978-1-4503-0112-1},
    keywords = {affine, compression, computer, deformation, epitome, vision},
    location = {Los Angeles, California},
    pages = {1--10},
    posted-at = {2010-07-05 13:15:51},
    priority = {0},
    publisher = {ACM},
    title = {Factoring repeated content within and among images},
    url = {http://research.microsoft.com/en-us/um/people/hoppe/factorimage.pdf},
    year = {2008}
}

@article{Mairal2010Online,
    abstract = {Sparse coding--that is, modelling data vectors as sparse linear combinations
of basis elements--is widely used in machine learning, neuroscience, signal
processing, and statistics. This paper focuses on the large-scale matrix
factorization problem that consists of learning the basis set, adapting it to
specific data. Variations of this problem include dictionary learning in signal
processing, non-negative matrix factorization and sparse principal component
analysis. In this paper, we propose to address these tasks with a new online
optimization algorithm, based on stochastic approximations, which scales up
gracefully to large datasets with millions of training samples, and extends
naturally to various matrix factorization formulations, making it suitable for
a wide range of learning problems. A proof of convergence is presented, along
with experiments with natural images and genomic data demonstrating that it
leads to state-of-the-art performance in terms of speed and optimization for
both small and large datasets.},
    archivePrefix = {arXiv},
    author = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro, Guillermo},
    citeulike-article-id = {5695195},
    citeulike-linkout-0 = {http://arxiv.org/abs/0908.0050},
    citeulike-linkout-1 = {http://arxiv.org/pdf/0908.0050},
    day = {11},
    eprint = {0908.0050},
    keywords = {coding, computer, dictionary, learning, sparse, vision},
    month = {Feb},
    posted-at = {2010-07-05 13:13:35},
    priority = {0},
    title = {Online Learning for Matrix Factorization and Sparse Coding},
    url = {http://arxiv.org/abs/0908.0050},
    year = {2010}
}

