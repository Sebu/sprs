\chapter{Conclusion}
%\thispagestyle{empty}

\section{Summary} 
We build framework to sparse code image signal with $\ell_0$ and
$\ell_1$ regularization. Learned image dictionaries for gray scale images. For
each separate color channel and for merge color channels. Dictionaries of
random natural images. In sub sets and in whole. Dictionaries on specific
groups of images. 
Tested image compression on images from the training set and other images from
the outside. Compared those compression results with other compression
algorithms.



\section{Discussion}
Convergence of the dictionary learning step.

OMP:
  start looks like DCT (random init)
  noisy
  only practical for OMP

LARS-lasso:
  slow 
  high quality dictionary

  Most common atoms in the dictionary 
  four major types of elements
  gradient, checkerboard (low color more b/w), spot, edge



With single images learning, initialization with elements from training sets
improbes quality. but with large training set random data is works fine.


Remark:
As it turns out the OMP, to its greedy nature, has a very aggressive selection
scheme. This very random/noisy selection strategy is very impractical
for the \trainDL learning step. But nevertheless the OMP is still good for
sparse coding. 
%\subsection{Observations}
Difference in the selection strategy.
Very noisy vs. smooth. 
How this different selection strategies affect the learning step will be
presented in the next section.

Big training sets tend to get to general.

If the images fit into the same category it is insignificant which images you
chose. The results will be the same?

Keeping the training sparse vectors too sparse can lead to dictionaries with
only low frequency elements. Using LARS-Lasso for training generates a good
dictionary for LARS and OMP. 
\Todo{ Training with OMP leads to a dictionary more with a lot of noise that
leads better coding results when using OMP.}




Not only the training data and task but also the right
sparse code algorithm plays a role for dictionary and reconstruction quality.


Compression 
JPEG and JPEG 2000 are highly optimized lossy image compression algorithms. 
Years of research went into observation human vision and the 
Especially the quantization and and the entropy encoding step are highly
optimized for the ... data.
For the short time available our results show promising results in visual
quality but still lack ...
But it is hard to compete with the market leader in this field.  Even JPEG 2000,
which leads to better results than JPEG especially in the area block artifacts,
did not catch up with the success of JPEG. 
The reason probably lies in the potential patent issues and higher computational
requirements of the algorithm compared
to the quality benefits. And the wide acceptance of JPEG in ...
\Todo{photo/Internet}.


Dictionaries to universal ... improved results to DCT .. but varying with field
of application. possible no universal solution but a good way for better
understanding of the key elements 

Just a first glimpse of that can de done to utilize sparse coding and
dictionary learning for large image databases.

Future of dictionaries
First moving away from structure induced by basis transform in analytical
dictionaries ... now realizing and moving back to structure ... promising
results\cite{?,?,?}

With the current development in computer technologie to more
parallization in form of multi-core CPUs and cloud computing. 
More indipendent learning strategies are a good direction to look at can
benefit from clustering. 



\section{Future work}
The amount of publications regarding in the last decade indicate strong ongoing
research in the field of sparse coding and dictionaries learning. Especially
brining structure to the learning stage is one of the most active topics in this
fields/area of research. 

\subsection{Framework improvements}
More speed and  sparse coding algorithms.
\paragraph{Speed}
Use SPAMS C++ interface.
\paragraph{Clustering}
\paragraph{Hierarchy}
\paragraph{Generate atoms}
\paragraph{Adaptive learning}
%\subsubsection*{Optimizations}
%\subsection{Adaptive}
%Thanks to the on-line approach of the training step an adaptive approach can be
%used. 


\subsection{Compression improvements}
\paragraph{Reducing block borders} The presented compression experiment
takes a very simple approach without taking correlation between adjancended
blocks into account. It is a copy of the JPEG block strategy. But other than
the JPEG basis transforms the learned atoms do not need to be periodic
Unable to giving
hard edges. 


\paragraph{Multi-scale approach}

%\Todo{OPTIONAL add structure for speed and model improvements Multi-scale,
%multi-channel, hierarchy}

