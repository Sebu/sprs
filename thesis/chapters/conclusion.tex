\chapter{Discussion}
%\thispagestyle{empty}

Convergence of the dictionary learning step.
Keeping the training sparse vectors too sparse can lead to dictionaries with
only low frequency elements. 

Using LARS-Lasso for training generates a good dictionary for LARS and OMP. 
\Todo{ Training with OMP leads to a dictionary more with a lot of noise that
leads better coding results when using OMP.}

Quality
With single images learning, initialization with elements from training sets
improbes quality. but with large training set random data is works fine.

OMP:
  start looks like DCT (random init)
  noisy
  only practical for OMP
  In the beginning OMP is very random

LARS-lasso:
  slow 
  high quality dictionary
  Most common atoms in the dictionary 

%\subsection{Observations}
Difference in the selection strategy.
Very noisy vs. smooth. 
How this different selection strategies affect the learning step will be
presented in the next section.

Big training sets tend to get to general.
If the images fit into the same category it is insignificant which images you
chose. The results will not much differ.

Learn basis similar to DCT and wavelets/bandelets(time and freq locality) with
increasing block size. Use to verify if it is a jpeg image?
Similar to natural images.
compression dicts
Dictionaries to universal ... improved results to DCT .. but varying with field
of application. possible no universal solution but a good way for better
understanding of the key elements
  four major types of elements
  gradient, checkerboard (low color more b/w), spot, edge


Better understanding the selection strategies of the algorithm and their
meaning for perceptional image quality. And evolution of the structure of
learned dictionaries. 

Compression
For the short time available the results show promising results in visual
quality but still lack ...

\section{Conclusion}
The experiments are just a first glimpse of that can de done to utilize sparse
coding and dictionary learning for large image databases.

Clustering learning strategies can help to learn faster and overcome
convergence in the reconstruction quality of big dictionaries.


It turn out that the training data and the right 
regularization play a big role for learning the right dictionaries and
for reconstruction quality. When extracting samples from JPEG images for
learning don't use non-overlapping $8 \times 8$ unless you want to learn DCT
atoms. 

The $\ell_1$ regularization of LARS has a selection strategy
which learns very smooth atoms. Good for reconstruction tasks 
But you need a certain amount of coefficients of about $L \ge 10$.
The $\ell_0$ regularization of the OMP, with its greedy nature, has a very
aggressive selection strategy. 
This very noisy selection strategy is less practical
than LARS for the ODL learning step under certain conditions. Such as a high
number of learning coefficients and bad initialization.

But when it comes to compression tasks the greedy OMP yields better results
than the Lasso with requirement for less coefficients. 
Our compression algorithm yields leads to similar compression ratio as JPEG
and surpasses it in certain situations. Both are highly optimized lossy image
compression algorithms.  Years of research went into observation human vision. 
Especially the quantization and and the entropy encoding step are highly
optimized for specific data. It is hard to compete with JPEG, the market leader
in this field.  Even JPEG 2000, which leads to better results than JPEG
especially in the area block artifacts, did not catch up with the success of
JPEG.  The reason lies in the potential patent issues and higher computational
requirements of the algorithm compared to the quality benefits annd the wide
acceptance of JPEG.



\section{Summary}
Over the course of this thesis we build a framework to sparse code and learn
dictionaries from big sets of different samples as found in large image
databases. We implemented the Batch-OMP and the LARS-Lasso sparse coding
algorithms for $\ell_0$ and $\ell_1$ regularization and the ODL by Mairal et
al. for learning dictionaries. 
In addition to this we added a cluster learning and merging strategy to the
framework to speed up learning of large dictionaries.
We proposed a simple sparse coding based compression algorithm that can compete
with and surpass JPEG compression in certain situations.


\section{Future work}
The amount of publications regarding sparse coding and machine learning in the
last decade indicate strong ongoing research in this field. Especially
brining structure to the learning stage is one of the most active topics in this
field of research. Be



\subsection{Framework improvements}
More speed and sparse coding algorithms.
\paragraph{Speed}
The implementation of the Batch-OMP and the OLD are already very fast while the
LARS-Lasso implementation is quiet slow compared to the KLT implmentation in the
SPAMS framework. In the near future the SPAMS framework will provide a C++
interface\footnote{\url{http://www.di.ens.fr/willow/SPAMS/faq.html}}. Adding
support for this interface could lead to speed improvements.

\paragraph{Clustering}
Taking the clustering strategy two even further is to learn smaller dictionaries
from different sets of images or characteristics. Such as paintings, gray-scale
sketches and smooth images animation. For example learning dictionaries for very
specific classes of images can be utilized for classification tasks. Something
that can come in handy when applying it to operations like search in large image
databases.
%We have shown that clustering can also lead to good dictionaries when the
%training samples are 
%With the current development in computer technologie to more
%parallization in form of multi-core CPUs and cloud computing online learning. 

\paragraph{Adaptive learning}
Thanks to the online approach of the training step an adaptive approach can be
used. 

\paragraph{Structure}
The results gained on the structure of the atoms can be used to generate new
ones.


\subsection{Compression improvements}

\paragraph{Multi-scale approach}
\paragraph{Reducing block borders} The presented compression experiment
takes a very simple approach without taking correlation between adjacent
blocks into account. Similar to the JPEG block strategy. But other than
the JPEG basis transforms the learned atoms are not locality in the frequency
domain but also in the time domain.
The framework can be easily extended to learn and code different
kinds of sub-images.









