\chapter{Conclusion and Future Work}

\section{Summary}
Over the course of this thesis we build a framework to sparse code and learn
dictionaries from big sets of different samples as found in large image
databases. We implemented the Batch-OMP and the LARS-Lasso sparse coding
algorithms for $\ell_0$ and $\ell_1$ regularization and the ODL by Mairal et
al. for learning dictionaries. 
In addition to this we added a cluster learning and merging strategy to the
framework to speed up learning of large dictionaries. We proposed a simple
sparse coding based compression algorithm that can compete with and surpass JPEG
compression in certain situations.


\section{Conclusion}
The right regularization and training data play a big role for learning 
dictionaries with good reconstruction quality. 

It turns out that the $\ell_1$ regularization of the LARS-Lasso has a selection
strategy which is very good at dictionary learning.
While the $\ell_0$ regularization of the OMP, with its greedy nature, has a very
aggressive selection strategy. This very noisy selection strategy is less
practical than LARS for the ODL learning step. Such as a high number of learning
coefficients and bad initialization. 


Using big image sets for learning can lead to very universal dictionaries. If
the images fit into the same category it is insignificant which images you
chose. The results will not much differ. When extracting samples from JPEG
images for learning keep in mind that we can only learn what the data provides.
Samples from JPEG especially non-overlapping $8 \times 8$ blocks will learn DCT
atoms.

%or want to verify if it is a JPEG image.
%Dictionaries to universal ... improved results to DCT .. but varying with field
%of application. possible no universal solution but a good way for better
%understanding of the key elements
% Better understanding the selection strategies of the algorithm and their
% meaning for perceptional image quality. And evolution of the structure of
% learned dictionaries.
% Similar to natural images.
% compression dicts

Dictionaries can be made much bigger than usual but the increased quality
comes with the price of computational complexity. Clustering can help to learn
more independent and still leads to equal reconstruction quality as single big
dictionaries.

When it comes to compression tasks the greedy OMP performs better
than the LARS-Lasso. As it sheers for less coefficients. Sparse coding 
compression algorithms can lead to similar compression quality as JPEG 
for low bit rates. And they can surpass it in certain situations. But
currently a lot of tweaking to the quantization step is required. 

Besides that JPEG and JPEG 2000 are highly optimized lossy image compression
algorithms. Years of research went into observation of human vision. Especially
the quantization and the entropy encoding step are highly optimized for specific
data. It is hard to compete with JPEG, the market leader in this field.  Even
JPEG 2000, which leads to better results than JPEG especially in the area block
artifacts, did not catch up with the success of JPEG. The reason lies in the
potential patent issues and higher computational requirements of the algorithm
compared to the quality benefits and the wide acceptance of JPEG.

Our experiments are just a first glimpse of what can be done to use sparse
coding and dictionary learning for large image databases.

\section{Future work}
The amount of publications regarding sparse coding and machine learning in the
last decade indicates active research in this field. Especially
adding hierarchy to dictionaries is one of the hot topics. 
%\Todo{The results gained on the atom structure can be used to think about 

\subsection{Different training sets}
One of our findings is the fact that we tend to learn DCT atoms from natural
JPEG images. Testing with larger sets of images that did not go through a JPEG
compression is one major topic to address in the future. Raw camera images or
TIFF images are a possible source. 

\subsection{Framework improvements}
To make further and more flexible experiments. The framework needs to be
extended in terms of speed and new dictionary learning strategies.

\paragraph{Speed}
While the implementation of the Batch-OMP and the ODL are already very fast
the LARS-Lasso implementation is quiet slow compared to the KLT implementation
in the SPAMS framework. In the near future the SPAMS framework will provide a
C++ interface\footnote{\url{http://www.di.ens.fr/willow/SPAMS/faq.html}}. Adding
support for this interface could lead to speed improvements. 

\paragraph{Clustering}
A different clustering strategy is to learn smaller dictionaries from different
sets of images or characteristics. Such as paintings, gray-scale sketches and
smooth animation images. For example learning dictionaries for very specific
classes of images can be used for classification tasks. Something that can come
in handy when applying it to operations like searching in large image databases.


\subsection{Compression improvements}
\paragraph{Reducing block borders} The presented compression experiments
uses a very simple approach without taking correlation between adjacent
blocks into account. This is similar to the JPEG block strategy. But unlike
JPEG basis transforms the learned atoms are not limited to locality in
the frequency domain. Splitting images in high and low pass sub images and
learning multi-scale dictionaries for each sub image could improve image
quality. The framework can be easily extended to learn and code different kinds
of sub-images.

\paragraph{Better entropy coding}
The coding of the coefficients already works quite well. But the coding of the
indices is currently limited as the sparse coding algorithm tends to select a
lot of different atoms. A joint sparsity approach as mentioned in
\prettyref{sec:sparse_related} could lead to less selected atoms and a better
entropy encoding. Also the required bits for each index need further
optimization.









