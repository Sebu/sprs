\chapter{Conclusion}
%\thispagestyle{empty}

The experiments are just a first glimpse of that can de done to use sparse
coding and dictionary learning for large image databases.

Clustering learning strategies can help to learn faster and overcome
convergence in the reconstruction quality of big dictionaries.

Big training sets tend to get general. If the images fit into the same category
it is insignificant which images you chose. The results will not much differ.

It turn out that the training data and the right 
regularization play a big role for learning the right dictionaries and
for reconstruction quality. When extracting samples from JPEG images for
learning don't use non-overlapping $8 \times 8$ unless you want to learn DCT
atoms. 

The $\ell_1$ regularization of LARS has a selection strategy
which learns very smooth atoms. Good for reconstruction tasks 
But you need a certain amount of coefficients of about $L \ge 10$.
The $\ell_0$ regularization of the OMP, with its greedy nature, has a very
aggressive selection strategy. 
This very noisy selection strategy is less practical
than LARS for the ODL learning step under certain conditions. Such as a high
number of learning coefficients and bad initialization.

But when it comes to compression tasks the greedy OMP yields better results
than the Lasso with requirement for less coefficients. 
Our compression algorithm yields leads to similar compression ratio as JPEG
and surpasses it in certain situations. Both are highly optimized lossy image
compression algorithms.  Years of research went into observation human vision. 
Especially the quantization and and the entropy encoding step are highly
optimized for specific data. It is hard to compete with JPEG, the market leader
in this field.  Even JPEG 2000, which leads to better results than JPEG
especially in the area block artifacts, did not catch up with the success of
JPEG.  The reason lies in the potential patent issues and higher computational
requirements of the algorithm compared to the quality benefits annd the wide
acceptance of JPEG.



\section{Summary}
Over the course of this thesis we build a framework to sparse code and learn
dictionaries from big sets of different samples as found in large image
databases. We implemented the Batch-OMP and the LARS-Lasso sparse coding
algorithms for $\ell_0$ and $\ell_1$ regularization and the ODL by Mairal et
al. for learning dictionaries. 
In addition to this we added a cluster learning and merging strategy to the
framework to speed up learning of large dictionaries.
We proposed a simple sparse coding based compression algorithm that can compete
with and surpass JPEG compression in certain situations.


\section{Future work}
The amount of publications regarding sparse coding and machine learning in the
last decade indicate strong ongoing research in this field. Especially
brining structure to the learning stage is one of the most active topics.

\subsection{Framework improvements}
To make further and more flexible experiments. The framework needs be be
extended in terms of speed and new dictionary learning strategies.

\paragraph{Speed}
While the implementation of the Batch-OMP and the OLD are already very fast
the LARS-Lasso implementation is quiet slow compared to the KLT implmentation in
the SPAMS framework. In the near future the SPAMS framework will provide a C++
interface\footnote{\url{http://www.di.ens.fr/willow/SPAMS/faq.html}}. Adding
support for this interface could lead to speed improvements.

\paragraph{Clustering}
Also taking the clustering strategy two even further is to learn smaller
dictionaries from different sets of images or characteristics. Such as
paintings, gray-scale sketches and smooth images animation. For example learning
dictionaries for very specific classes of images can be used for classification
tasks. Something that can come in handy when applying it to operations like
search in large image databases.
%We have shown that clustering can also lead to good dictionaries when the
%training samples are 
%With the current development in computer technologie to more
%parallization in form of multi-core CPUs and cloud computing online learning. 

\paragraph{Structure}
The results gained on the appearance of the atoms can be used to think
about more advanced ways to structure these atoms or generation of atoms.

%\paragraph{Adaptive learning}
%Thanks to the online approach of the training step an adaptive approach can be
%used. 


\subsection{Compression improvements}

\paragraph{Reducing block borders} The presented compression experiment
takes a very simple approach without taking correlation between adjacent
blocks into account. Similar to the JPEG block strategy. But other than
the JPEG basis transforms the learned atoms are not locality in the frequency
domain but also in the time domain. 
The framework can be easily extended to
learn and code different kinds of sub-images.

\paragraph{Better entropy coding}
The coding of the coefficients already works quiet good. 












