\chapter{Sparse coding}
\thispagestyle{empty}

\Todo{better intro}
The Cambridge Advanced Learner defines a code as ``..a system of numbers, letters or signals which is used to represent something in a shorter or more convenient form''
A sparse code is a sparse vector of coefficients that is used to linear combine a small selection of atoms from a dictionary.
Sparse code have their origin in neural science and machine learning. Olshausen and Field in 1996 \cite{Olshausen1996}

%copy
%In sparse modeling representation, a signal x Ã¢ÂÂ Rn is represented as a linear combination of basis column vectors dj Ã¢ÂÂ Rn (atoms) which form a dictionary D Ã¢ÂÂ RnÃÂK, such that x = DÃÂ±.

Consider $X \in \mathbb{R}^{m\times n}$  as a matrix with $n$ columns each column $x_{i}$ representing a signal described by a single vector of signal length $m$.
The dictionary $D\in\mathbb{R}^{m \times p}$ is another matrix with $p$ columns where each column represents an atom signal with the same dimension as a single signal $x_{i}$ from $X$.
The vector $\alpha$ is linear combination of a few non orthonormal atoms from $D$ that is close to the signal $X$.

We try to keep coefficient vector $\alpha$ sparse. 
Under-determined linear system. Not well-posed because there is more than one solution. 

%Sparse coding is the 
\begin{align}
x \approx D\alpha\notag\\
\underbrace{\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}}_{signal} \approx \underbrace{\begin{pmatrix} d_1  d_2 \cdots d_n \end{pmatrix}}_{\textrm{dictionary atoms}}
\underbrace{\begin{pmatrix} \alpha_1 \\ 0 \\ \vdots \\ \alpha_n \end{pmatrix}}_{\textrm{sparse vector}}
\end{align}

The solution to this problem is a least-squares solve under-determined linear system we want the sparsest solution.


\begin{align}
\min_{\alpha\in\mathbb{R}^{p}} \underbrace{\lVert x - D\alpha \rVert^{2}_{2}}_{reconstruction} \label{eq:problem}
\end{align}

To achieve the spare solution we add a constraint to the problem. ... over-fitting .. Such a constraint is the regularization.

\begin{align}
\min_{\alpha\in\mathbb{R}^{p}} \lVert x - D\alpha \rVert^{2}_{2} + \underbrace{\psi(\alpha)}_{regularization}
\end{align}


measure sparsity via        $\ell_0$ pseudo-norm       $\lVert\alpha\rVert_{0}$


$\lVert\alpha\rVert_{0}$ makes the problem NP-hard
to get best solution you need to test every combination
Solution:
use greedy approach 

%copy
% Following Tao et al., where it was shown that the L1 norm is equivalent to the L0 norm, 
%leads one to solve an easier problem. Finding the candidate with the smallest L1 norm can be expressed relatively
%easily as a linear program, for which efficient solution methods already exist. These solution methods have been refined over the past few years yielding enormous gain

Use $\ell_1$ or $\ell_2$ as a convex relaxation of $\ell_0$

\Todo{image/description of different regularizations}

\begin{figure}
\centering
%\includegraphics[width = 0.66\textwidth]{images/Da_x.pdf} % Or .pdf
\caption{Sparse Coding}
\label{fig:da_x}
\end{figure}


In the last 15 years several sparse coding algorithms have been proposed. 
Some that solve the initial problem <> greedily, the (orthogonal) matching pursuit, and others which modified the problem to become convex/linear. These primary derive from the numerical domain in the form of 
large linear system solvers with few optimization constraints. The LARS-Lasso, basis pursuit, FOCUSS?

the most common algorithms are the following ones.
BP/MP/OMP ...
Lasso/Ridge regression etc.




\section{$\ell_0$ regularization with greedy algorithms}

These algorithms calculate a coefficient vector $\alpha$ which is an greedy solution of the following NP-hard problem
\begin{align}
\min_{\alpha\in\mathbb{R}^{p}}  \lVert x - D\alpha \rVert^{2}_{2} \textrm{ s.t. } \lVert \alpha \rVert_{0} \leq L
\end{align}
respectivly
\begin{align}
\min_{\alpha\in\mathbb{R}^{p}}   \lVert \alpha \rVert_{0}   \textrm{ s.t. } \lVert x - D\alpha \rVert^{2}_{2} \leq \epsilon
\end{align}
\cite{Mallat1993}

\subsection{Matching-Pursuit}
\label{sec:mp}

The \prettyref{alg:mp} starts with the coefficent vector $\alpha$ set to zero. Then find the atom in the dictionary with the best reduction of the error.
This is done by selecting the coefficient that has the maximum correlation with the residual. 
Repeat this step $L$ times or until the error $\lVert x - D\alpha \rVert^{2}_{2}$ for the curernt $\alpha$ reaches $\lVert x - D\alpha \rVert^{2}_{2} \leq \epsilon$ respectifly the residual fullfills $\lVert r \rVert_2 \leq \epsilon$.


\begin{algorithm}
\caption{Matching Pursuit}
\label{alg:mp}
\begin{algorithmic}[1]
\REQUIRE $x \in \mathbb{R}^m, y \in \mathbb{R}^m, D \in \mathbb{R}^{m\times p}, \epsilon \in \mathbb{R}$
\STATE $\alpha_0 \gets 0$ (start with zero vector)
\STATE $r_0 \gets x-D\alpha_0 = x$ (residual) 
\WHILE {$\lVert \alpha \rVert_{0} \leq L$ \OR $\lVert r \rVert_2 \leq \epsilon$}
\STATE Select atom with maximum correlation with residual: 
\begin{equation*}
i \gets \argmax_{i=1,...,p} \lvert d_i^Tr \rvert
\end{equation*}
\STATE update coefficients: 
\begin{align}
a[i]  \gets a[i] + d_i^Tr \label{eq:mp_update}
\end{align}

\STATE update residual: $r \gets r - \left(d_i^Tr\right)d_i$

\ENDWHILE
\RETURN $\alpha$
\end{algorithmic}
\end{algorithm}
\subsection{Orthogonal-Matching-Pursuit}
\label{sec:omp}

An improved version of the \prettyref{sec:mp} the the Orthogonal-Matching-Pursuit \prettyref{alg:omp} was presented by in \cite{Pati1993}.
Rather than just updating the coefficent of $\alpha$ that is currently selected \prettyref{eq:mp_update} the algorithm re-evaluates all coefficent in the current active set $S$ 
\prettyref{eq:omp_update} $\alpha_S$ by solving a full least-squares solution in every iteration. This improves the quality of the solution. \cite{OMP}
The name Orthogonal-Matching-Pursuit comes from the fact that the residual $r$ is orthogonal to the previously selected atoms in $D$ which leads to the effect that every coefficent is only selected once.



\begin{algorithm}
\caption{Orthogonal Matching Pursuit}
\label{alg:omp}
\begin{algorithmic}[1]
\REQUIRE $x \in \mathbb{R}^m, y \in \mathbb{R}^m, D \in \mathbb{R}^{m\times p}, \epsilon \in \mathbb{R}$
\STATE $\alpha \gets 0, r \gets x $ (residual) $, S=\emptyset$
\FOR {$i = 1$ to $L$}
\STATE Select atom with maximum correlation with residual: 
\begin{equation*}
i \gets \argmax_{i \in S^C} \lvert d_i^Tr \rvert
\end{equation*}
\STATE update active set: $S \gets S \cup \{i\} $
\STATE update residual: $r \gets \left(I-D_S\left( D_S^T D_S \right)^{-1} D_S^T \right)x$
\STATE update coefficients: 
\begin{align}
a_S \gets \left( D_S^T D_S \right)^{-1} D_S^T x  \label{eq:omp_update}
\end{align}

\ENDFOR
\RETURN $\alpha$
\end{algorithmic}
\end{algorithm}

%\subsection*{Limitations}

All these gready approaches tend to find suitable solutions but because of the non-convex problem they can get stuck in local minimas.
As shown before an $\ell_1$ regularization version of \ref{eq:problem} also keeps the coefficent vector $\alpha$ sparse but elimantes the problem of a local minima.
The next section presents algorithms which solve  a $\ell_1$ regularization version of \ref{eq:problem}.



\section {$\ell_1$ regularization}



The Lasso (least absolute shrinkage and selection operator) is a regularized version of a least squares solution.
The regularized version is found by adding a constraint that induces the $L_1$-norm of the solution to be small. \cite{Tibshirani1998}

\begin{align}
\min_{\alpha\in\mathbb{R}^{p}}   \lVert \alpha \rVert_{1}   \textrm{ s.t. } \lVert x - D\alpha \rVert^{2}_{2} \leq \epsilon\label{eq:l1}
\end{align}

The lagrange multiplier version of the regularized problem \ref{eq:l1} is:

\begin{align}
\min_{\alpha\in\mathbb{R}^{p}}  \frac{1}{2} \lVert x - D\alpha \rVert^{2}_{2} + \lambda \lVert \alpha \rVert_{1}
\end{align}

There are several ways to compute the LASSO. .. ...... 

%\cite{} 
%basis pursuit\cite{} 
%FOCUSS \cite{}

We chose the LARS method as it is the fastest in low dimension or for high correlation. Both conditions are satisfied in our experiments.
\Todo{split into LAR and LARS-lasso, better illustrate than use complex formulas}

\subsection {LARS-Lasso}
\label{sec:lars}
The LARS-Lasso is a algorithm to solve the LASSO with the help of least angle regression (short LAR)
as described in \cite{Efron2004}. It is a modified version of the LAR where .... variables are removed when they cross zero ...


\begin{align}
\min_{\alpha\in\mathbb{R}^{p}}  \frac{1}{2} \lVert x - D\alpha \rVert^{2}_{2} + \lambda \lVert \alpha \rVert_{1}
\end{align}

\Todo{add algorithm}
\begin{algorithm}
\caption{LARS-lasso}
\begin{algorithmic}[1]
\REQUIRE $x \in \mathbb{R}^m$
\end{algorithmic}
\end{algorithm}

\subsubsection{Limitations}
The Lasso modified version of the LAR algorithm has the following limitations.
\begin{description}
 \item[Dimension] When the dimension $p$ of the signal $X$ is 
much higher than the the dimension $m$ of the dictionary $D$ the algorithm can only select $m$ columns.

\item[correlation] When the columns of the dictionary are highly correlated the algorithm
selects only one column.
\end{description}

Limitation one is irrelevant for our experiments as the dictionaries are over-complete 
, with respect to the dimension $m$ of signal $X$, and thus satisfy $p\leq n$.

\section{Application and Related Work}

%\begin{description}
\paragraph{noise reduction}
Remove noise from a signal. 
Using the fact that sparse coding is an approximation of signal that loses ... noise? ... in its encoding process. 
\begin{align*}
y = x + w
\end{align*}



\cite{Elad2006}

\paragraph{in-painting}
fill missing parts by removing rows from the dictionary
Train with the original image
\begin{align*}
x \approx D\alpha\\
x_s \approx D_s\alpha\\
Wx \approx WD\alpha\text{select subset}\\
\end{align*}

\cite{mairal08sparse}

\paragraph{compression} An example for this is the compression of facial images by Bryt and Elad \cite{Bryt2008}.

%Task driven dictionaries
\paragraph{classification} Examples for this can be found in \cite{Mairal2008b} and \cite{Bar2009}.
\paragraph{inverse half-toning} ongoing research by Mairal et al., 2010 \cite{Mairal2010b}
\paragraph{super resolution} \cite{Yang2010} \cite{Wright2008}  
Or the ongoing work by Couzinie-Devy et al., 2010 called digital zooming.
\paragraph{background subtraction} \cite{}

%\item[noise reduction]
%\item[in-painting]
%\item[classification] \Todo{usage:classification}

%\end{description}





