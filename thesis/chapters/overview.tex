\chapter{Overview}

\numberwithin{equation}{chapter}

\section{Problem statement}
Size of database/dictionary
Quality

In order to find 
1. coding signals 
2. find a good dictionary

analog problem video compression but with less corelation between images and still image 

\subsection{criteria}
Convergence of the dictionary learning
Quality increase with size increase 

\subsection{dictionaries for signal representation}
Dictionaries is a collection of ordered elements with an index.
Atom is am small unit of something big.
The Cambridge Advanced Learner defines a code as ``..a system of numbers, letters or signals which is used to represent something in a shorter or more convenient form''
A sparse code is a sparse vector of coefficents that is used to linear combie a small selection of atoms from a dictionarie.
As a matter of fact a good dictionary is essential for a good signal approximation. \cite{} Finding this dictionarie has become a major task/problem in the last 15 years.

\section{signal representation/analysis}
Signal transforms \cite{sparse intro} that far back in the early 60s.
FFT in 65s

approximate signals via combination of limited signal samples
Why?
signal analysis, compression, de-noise etc.

The early approaches in the 60s used combinations of cosine transformations. Coming from a continues representation this makes 
actually sense. The signal could be represented via ....

In 80s the search for better transformation basis became a major role in signal representation. \cite{}

\subsection{discrete signals}
Rather then using continues signal we concentrate on discrete signal representation.
Continues signals would be better for .... . But our signal (image etc.) will be discrete anyway becaus of their initial digital representation. 
Besides the problem of coding becomes different in the continues space \cite{} Because of ...

\subsection{splitting the problem}
\cite{Rubinstein2010}
In the last 15 years 

the idea came up to interpret the basis transforms as sparse linear combination of dictionary atoms.
see \cite{Olshausen1997} and \cite{}
The benefit from this approach/direction was that you could decouple signal coding and dictionary design.
Separating the problem into two distinct problems made the search for task specific and .... more flexible \cite{?}.

Two separate tasks.
Coding the signal. Design of the dictionary.

\subsection{dictionaries and application}
Signal representation via linear combination of few signal vectors from a dictionary.
Benefits of dictionaries


\section{sparse codes/coding}

%copy
%In sparse modeling representation, a signal x ∈ Rn is represented as a linear combination of basis column vectors dj ∈ Rn (atoms) which form a dictionary D ∈ Rn×K, such that x = Dα.

Consider $X$ as matrix with $n$ columns each column $x_{i}$ representing a signal described by a single vector of signal length $m$.
The dictionary $D$ is another matrix with $p$ columns where each column reprensents an atom signal with the same dimension and size as a single signal $x_{i}$ from $X$.
The sparse vector $\alpha$ is linear combination of a few non orthonormal atoms from D that is close to the signal X.
try to keep coefficient vector sparse
Sparse coding is the 
\[
\underbrace{\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}}_{signal} \approx \underbrace{\begin{pmatrix} d_1  d_2 \cdots d_n \end{pmatrix}}_{\textrm{dictionary atoms}}
\underbrace{\begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n \end{pmatrix}}_{\textrm{keep sparse}}
\]
solve under-determined linear system
we want the sparsest solution
\begin{align}
\min_{\alpha\in\mathbb{R}^{p}}  \lVert x - D\alpha \rVert^{2}_{2} + \underbrace{\psi(\alpha)}_{regularization}
\end{align}
measure sparsity via       l0-norm       $\lVert\alpha\rVert_{0}$

\begin{figure}
\centering
%\includegraphics[width = 0.66\textwidth]{images/Da_x.pdf} % Or .pdf
\caption{Sparse Coding}
\label{fig:da_x}
\end{figure}


In the last 15 years several sparse coding algorithms have been proposed. 
Some that solve the initial problem <> greedyly, the (orthogonal) matching pursuit, and others which modified the problem to become convex/linear. These primary derive from the numerical domain in the form of 
large linear system solvers with few optimization constraints. The LARS-Lasso, basis pursuit, FOCUSS?


$\lVert\alpha\rVert_{0}$ makes the problem NP-hard
to get best solution you need to test every combination
Solution:
use greedy approach 

or make problem convex (e.g. use $\lVert\alpha\rVert_{1}$)
the most common algorithms are the following ones.
BP/MP/OMP ...
Lasso/Ridge etc.





\subsection{matching pursuit}
The algorithm calculates a coefficient vector $\alpha$ which is an gready/approximate solution of the following NP-hard problem
\begin{align}
\min_{\alpha\in\mathbb{R}^{p}}  \lVert x - D\alpha \rVert^{2}_{2} \textrm{ s.t. } \lVert \alpha \rVert_{0} \leq L
\end{align}
or
\begin{align}
\min_{\alpha\in\mathbb{R}^{p}}   \lVert \alpha \rVert_{0}   \textrm{ s.t. } \lVert x - D\alpha \rVert^{2}_{2} \leq \epsilon
\end{align}
\cite{Mallat1993}
\begin{algorithm}
\begin{algorithmic}
\STATE i=0
\end{algorithmic}
\end{algorithm}

\subsection{orthogonal matching pursuit}
\cite{Pati1993}
\label{sec:omp}

%copy
%OMP to address the NP-hard sparse coding problem

\begin{algorithm}
\caption{Orthogonal-Matching-Pursuit}
\begin{algorithmic}[1]
\REQUIRE $x \in \mathbb{R}^m y \in \mathbb{R}^m  D \in \mathbb{R}^{mxk} \epsilon$
\STATE $\alpha_o \gets 0, r_0 \gets x $ (residual) $, S_0=\emptyset$
\FOR {$i = 1$ to $L$}
\STATE Select atom with maximum correlation with residual: 
\begin{equation*}
i \gets \arg\max_{i=1,...,p} \lvert d_i^Tr \rvert
\end{equation*}
\STATE update active set: $S \gets S \cup \{i\} $
\STATE update residual: $r \gets \left(I-D_S\left( D_S^T D_S \right)^{-1} D_S^T \right)x$
\STATE update coefficents: $a_S \gets \left( D_S^T D_S \right)^{-1} D_S^T x $

\ENDFOR
\RETURN $\alpha$
\end{algorithmic}
\end{algorithm}

\subsection {$L_1$ regularized}
\subsection{LARS-Lasso}
\label{sec:lars}

The LASSO (least absolute shrinkage and selection operator) is a regularized version of a least squares solution.
The regularized version is found by adding a constraint that induces the $L_1$-norm of the solution to be small. \cite{Tibshirani1998}
There are several ways to compute the LASSO. .. ...... \cite{} 
The LARS-Lasso is a algorithm to solve the LASSO with the help of least angle regression
as described in \cite{Efron2004}. It is a modified version of the LAR where .... variables are removed when they cross zero ...


\begin{align}
\min_{\alpha\in\mathbb{R}^{p}}  \frac{1}{2} \lVert x - D\alpha \rVert^{2}_{2} + \lambda \lVert \alpha \rVert_{1}
\end{align}

\begin{algorithm}
\begin{algorithmic}
\STATE i=0
\end{algorithmic}
\end{algorithm}

Some limitations of the LARS-Lasso
\begin{description}
 \item[Dimension] to high dimension of signal 
Es liegen deutlich mehr Parameter im Modell vor als Beobachtungen  p ( ) > n . Hier 
können durch die Lasso-Methode maximal  n Variablen ausgewählt werden

\item[correlation] When the columns of dictionary D a highly correlated the algori
Einige der Kovariablen weisen eine hohe  paarweise Korrelation auf. In diesem Fall 
wählt die Lasso-Methode lediglich eine der Kovariablen aus.
\end{description}


\subsection{usage/application}
\begin{description}
\item[noise reduction]
Remove noise from a signal. Using the fact that sparse coding 
is an approximation of signal that looses .... in its encoding process. 
\cite{Elad2006}

\item[inpainting]
fill missing parts by removing rows from the dictionary
Train with the original image
\cite{mairal08sparse}

\item[compression] An example for this is the compression of facial images by Bryt and Elad \cite{Bryt2008}.
\item[classification] Examples for this can be found in \cite{Mairal2008b} and \cite{Bar2009}.
\end{description}



\section{Dictionaries}
\subsection{analytical}
\begin{description}
 \item[cosine]
 \item[wavelets]
 \item[curvelets/contourlets/bandelets]
\end{description}

\subsection{learned over-complete}
recent research has shown that learnd dictionaries show better compression quality than small analytic dictionaries \cite{Aharon2006} \cite{Chen1998} 


In the last decade several learning algorithms have been proposed which try to a universal basis that 
can sparsly reconstruct a set of "trainig data" with minimal error. 
K-SVD
MOD
Online learning
Mairal2010

%\subsubsection{k-svd}
\subsection{online learning}
\label{sec:mairal}
Mairal .... \cite{Mairal2010}

\begin{algorithm}
\caption{Online dictionary learning \cite{Mairal2010}}
\begin{algorithmic}[1]
\REQUIRE $x \in \mathbb{R}^m  p \left( x \right) \lambda \in \mathbb{R} D_0 \in \mathbb{R}^{mxk} T$
\STATE $A_0 \in \mathbb{R}^{kxk} \gets  0, B_0 \in \mathbb{R}^{mxk}\gets 0$
\FOR {$t = 1$ to $T$}
\STATE Draw $x$ from p(x).
\STATE Sparse code:
\begin{align} 
\alpha_t \equiv \arg \min_{\alpha\in\mathbb{R}^{p}}  \lVert x_t - D_t\alpha \rVert^{2}_{2}  +  \lambda \lVert \alpha \rVert_{1}
\end{align}

\STATE $A_t \gets A_{t-1} + \alpha_t\alpha_t^T$
\STATE $B_t \gets B_{t-1} + x_t\alpha_t^T$
\STATE Compute $D_t$ using Algorithm \ref{lst:update}, with $D_{t-1}$ as warm restart 
\begin{align} 
D_t \equiv \arg\min_{D \in \mathbb{R}^{m x k}}  \frac{1}{t} \sum_{i=1}^t \left( \lVert x_i - D\alpha_i \rVert^{2}_{2}  +  \lambda \lVert \alpha_i \rVert_{1} \right) \label{eq:update}
\end{align}
\ENDFOR
\RETURN $D_T$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Dictionary Update}
\label{lst:update}
\begin{algorithmic}[1]
\REQUIRE $D=[d_1,...,d_k] \in \mathbb{R}^{mxk}, A=[a_1,...,a_k] \in \mathbb{R}^{kxk}, B=[b_1,...,b_k] \in \mathbb{R}^{mxk}$
\REPEAT
\FOR {$j = 1$ to $k$}
\STATE update j-th column to optimze for \ref{eq:update}:
\begin{align}
u_j \gets \frac{1}{A[j,j]}\left(b_j-Da_j\right)+d_j \\
d_j \gets \frac{1}{\max\left(\lVert u_j \rVert_2,1\right)} u_j
\end{align}

\ENDFOR
\UNTIL convergence 
\RETURN $D$
\end{algorithmic}
\end{algorithm}

\section{Learning for the Task}
It has been shown that learning basis specialy for certain tasks can lead to the best results\cite{}.  <>
Based on this discovery we will concentrate on a specific class. <> Join the basis for natural images and cartoon/line images.
We will also concentrate on real practical data. This means typically 3-channel data of 1+ megapixels images found on image hosting services like flickr, twitpic, etc.

Current research is primary concentrating on other tasks. 
Like ... related work :)

\subsection{improvements}

\begin{description}
\item[multi-scale]
Rather than encoding uniform segments of a bigger signal this technique encodes different sized 
segments (multiple times of the default segment). The idea is that aligned smooth regions with low variance can be 
can be combined an encoded as one signal rather than e.g. 4. \cite{saprio}

\item[multi-channel]
coding every channel separate or combine them
into single signal. can lead to color bleeding \cite{mairal08sparse}
fortunately this problem vanishes with big dictionaries and large training sets. \cite{mairal08sparse}

\item[double sparsity]
Rather then directly using a collection of basis/atom signals as a dictionary. This approach uses sparse coding
to encode the signals of a bigger dictionary with the help of a smaller on. \cite{double sparsity}

\item[group sparsity]
similar patches should admit similar patterns
\cite{double sparsity}

\item[hirarchy/structure]
Adding hirarchy/structure to dictionaries can lead to faster coding or use in structure analysis ( text documents )
\cite{Jenatton2010}

\end{description}




\section{Related work}
\begin{description}
\item[compression of facial images] In 2008 Bryt and Elad \cite{Bryt2008} 
\item[training with a neural network]
Learning Multiple Layers of Features from Tiny Images \cite{Krizhevsky2009}
\item[noise reduction]
\item[in-painting]
\end{description}



\section{goal of this thesis}
Evaluate the quality and size of learned big redundant dictionaries for 
optimal sparse coding of large image databases.

Observation of problems (encoding time, quality benefit, etc.)

Search for a universal dictionary for combination natural and artificial images
 
Convergence of different sizes and configurations of learned dictionaries.
how many elements for a 'good' sparse representation?

\Todo{OPTIONAL add structure for speed and model improvements Multi-scale, multi-channel, hierarchy}


Improve compression vs. jpeg/jepg2000 and possible usage as an image descriptor.
