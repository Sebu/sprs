\chapter{Overview}
%\thispagestyle{empty}
\numberwithin{equation}{chapter}

\section{Dictionaries for signal representation}
\label{sec:dicts}
%\subsection{Problem statement}
Signals like images, sound, mediacal data from EEG, genome sequences. 
Signal analysis of continues or discrete to performe operations on these
signals. Simplification of signals is a major field in signal processing as it
enables ... \Todo{}

Consider $x$ a signal that is represent by a
function $f(x)$ that approximates the signal $x$ in a sparse way.
\begin{equation*}
x \sim \tilde{x} = f\left(x\right)
\end{equation*}
When a signal is meant to be sparse it is either representable with a small
number of combined terms or a series of many terms where the majority of
coefficients is close to zero. 
\begin{equation*}
\begin{split}
\tilde{x} & = t_{1}(x) + t_{12}(x) + t_{312}(x)\\
& = 0t_{1}(x) + \cdots + 1.1t_{2}(x) + 0.01t_{12}(x) + \cdots +
2t_{312}(x)
\end{split}
\end{equation*}
We will call these basis terms $t_n$ \emph{atoms} as they are the basic units
which build up our representation of signals. So far consider the actual
appearance of those atoms as unknown. We 
organize all of them in a indexed set called \emph{dictionary} analog to a
dictionary of words where the indicies are the words and the atoms the
discription of them. 

Active research \cite{} in the last decades has shown that a good basis or
dictionary is essential for a good signal approximation. Dictionaries capture
the esence of the signals that they can represent. There are essential two major
distinct ways to construct the  desired dictionaries. First the construction of
model based on basis functions found via harmonic analysis that can represent
our signal in a sparse way or second via algorithms that learn a sparse
dictionary via a training process from a set of training signal samples. The
later approach of training good dictionaries has become a major task in the last
two decades\cite{Mairal2010}.



%Orthonormal (feature) vectors are orthogonal and have unit length.


\subsection{History}
\label{sec:history}
%Compressed sensing
The field of signal processing reaches far back in the early 60s.
\cite{Rubinstein2010}
In the early approaches in the 60s combinations of cosine 
transformations were used to approximate analog signals. For example the signal
could be represented via combinations of connected oscilloscopes with different
frequency configurations.  Later on approximation of signals via combination of
limited signal samples became a major task in signal analysis.  The Fourier
transform is one of the most widely used ones. Signals are split into basis
functions with 

Periodic signal in time represent as 
Several 
%application of such signal analysis and interpretation.
%\paragraph{Fourier transform}

%copy
%The most widely accepted analytic tool for such problems is the Fourier
%transform. The Fourier transform decomposes a signal into basis functions of
%different frequencies, known as frequency components, and gives us the
%amplitude of each frequency component in the signal. The waves are of the form:
\begin{equation*}
\mathcal{F}\left(v\right) = \int_{-\infty}^{\infty} \! f(t)e^{-i2\pi vt} \,
\mathrm{d}t
\end{equation*}
%\begin{align*}
%\end{align*}

\paragraph{Getting discrete}
With the beginning of the age of computer technologie the field of digital
signal processing emergerd. A move from continues signals of analog system to
the discrete representation common in the digital world happend. Based on this
move to digital signal processing new approaches for basis transforms of
discrete signals were required. The discrete Fourier transform (DFT) ermerged as
as special discrete version of the continuous Fourier transform.
\begin{equation*}
 x_k = \sum_{n=0}^{N-1}\alpha_ne^{\frac{-i2\pi kn}{N}}
\end{equation*}
In 1965 Cooley and Tukey presented\cite{Cooley1965} the \emph{fast Fourier
transform (FFT)} an fast algorithmic approach to calculation the discreat
Fourier transform. The fast Fourier transform is actualy a reinvention of a
similiar algorithm from 1805 by Carl Friedrich Gau√ü. The fast Fourier transform
was a major leap forward in the field of digital signal processing.

In the following decades two other major players in discrete basis
transformations emerged. The \emph{discrete cosine transform} and the
\emph{discrete wavelet transform}.

\paragraph{Discrete cosine transformation (DCT)} is a special case of the
discrete Fourier transform which only uses real part of the equation. The idea
is to make $f$ an odd function with $\forall x \in \mathbb{R} : f(x) = f(-x)$
which leads to the imaginary part of the equation becoming zero.
\begin{equation*}
x_k = \sum_{n=0}^{N-1}\alpha_n\cos \left[ \frac{\pi}{N} \left(
n+\frac{1}{2}\right) k\right]
\end{equation*}
An example for the usage of DCT basis is the coding of the image blocks in the
JPEG image compression algorithm.


\paragraph{Discrete wavelet transformation (DWT)}
Property of the basis functions 
$\int_{-\infty}^{\infty} \! f(t) \, \mathrm{d}t = 0$
\Todo{}
Other than the Fourier transform the wavelets transforms capture both frequency
and location information. For example these basis are used in JPEG 2000 image
compression and 
\Todo{move}

In 80s the search for better transformation basis started to become a major role
in field of signal representation.\cite{} 


In computer vision applications for digital signal processing would be
removing of image noise, reconstruction of missing and compression among others.


\subsection{Splitting the problem}
\cite{Rubinstein2010}
In the last two decades (see\cite{Olshausen1996,Mallat1993}) a concept emerged
to interpret basis transforms as a set of discrete signal atoms in a dictionary
and the signals that they reconstruct as sparse linear combination of these
atoms. The benefit of this approach is that you can decouple signal coding and
dictionary design and split the whole process of signal analysis into two tasks.
First coding of signals and second the design of dictionaries. Separating the
problem into two distinct problems made the search for efficient coding of
signals $x$ and construction of task specific dictionaries $D$ more
flexible\cite{?}. The initial problem becomes:
\begin{equation}
x = \sum_{n=0}^{N-1}d_n\alpha_n
\end{equation}

\begin{equation}
 x \sim \tilde{x} = D\alpha
\end{equation}
Ongoing research in this field emerged in order to:
\begin{itemize}
 \item Find efficient ways for coding signals in a sparse way.
 \item Design or learn a good dictionary that can code specific signal in sparse
way with low error.
\end{itemize}

%analog problem video compression but with less correlation between images and still image 


\paragraph{Notation}
$a[i]$ element i of a vector
alpha,X,x,D,d,L,epsilon error, L,lambda , A,I sets

\section{Goal of this thesis}
The main purpose of the thesis is to learn good dictionaries from large
image databases. %that can be used for specific tasks on. 
In 2009 Mairal et al. of the Willow
Project\footnote{\url{http://www.di.ens.fr/willow/}} presented in
\cite{Mairal2009,Mairal2010} an online dictionary learning algorithm for sparse
coding. The presented algorithm enables us to learn dictionary elements from
large training sets found in large image databases.

They also released the Matlab framework \emph{SPArse Modeling Software}
short SPAMS\footnote{\url{http://www.di.ens.fr/willow/SPAMS/}} with all
necessary functions to recreate their results and experiment with the algorithm.
The problem is that the framework is closed source, which is impractical for
modifications like usage of different coding algorithms in the training process.
Also Matlab bears/includes some limitations that make it hard to test the
algorithms with big dictionaries.

To address these problems we reimplemented all necessary functions of the
framework in C++ to experiment with the applicability of sparse coding for
learning large dictionaries for large image databases. The three major steps of
the following chapters are:

\begin{itemize}
 \item Fast sparse coding of many small signals under different constraints
 \item Efficient learning redundant dictionaries for large image databases from
a large set of training data
 \item Evaluation of the quality and usage of the learned dictionaries
\end{itemize}

This includes topics like ``When do dictionaries different sizes and
learning configurations show convergence?'' ``How many atoms do we need for a
'good' sparse representation of large image databases?''  and ``How does this
affect quality and usability for certain application?''. 

\Todo{einbauen/conclusion?}
Better understanding the selection strategies of the algorithm and their
meaning for perceptional image quality. And evolution of the structure of
learned dictionaries. 

All these topics are addressed in several experiments. Experiments with
small sets specific images like from sketches, still images of animations from
Disney and art styles like post-impressionistic images from Vincent van Gogh.
Evaluation of the quality and size of learned big redundant dictionaries for
optimal sparse coding of large image databases. Search for a universal
dictionary for databases of hundredths of thousands of images. Evaluation of
clustering of learning algorithms. Application of sparse coding for image
compression and observation of problems. Such as encoding time, quality benefit.
Comparison of compression with discrete cosine transformation approaches in JPEG
and discrete wavelets transformation of JPEG 2000 and possible usage as an image
descriptor.


%Convergence of the dictionary learning
%Quality increase with size increase 
%Comparison with JPEG,JPEG 2000 via MSE



%\Todo{OPTIONAL add structure for speed and model improvements Multi-scale, multi-channel, hierarchy}


