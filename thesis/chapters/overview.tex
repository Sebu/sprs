\chapter{Overview}
%\thispagestyle{empty}
\numberwithin{equation}{chapter}

\section{Sparse signal representation}
\label{sec:dicts}
Simplification of signals such as images, audio or video is a major
field of signal processing. It can help to speed up operations on signals and
allows us to capture the most significant basis elements that make up a signal.
Elements that are used in MP3 audio, JPEG image and MPEG4 video compression or
to reconstruct a signal in order to remove noise or recover missing information.

\paragraph{}
Consider $\vec{\tilde{x}}$ a signal that is represent by a function $f(\vec{x})$
that
approximates the signal $\vec{x}$ in a simplified sparse way.
\begin{equation*}
\vec{\tilde{x}} = f\left(\vec{x}\right)
\end{equation*}
We can split $f(vec{x})$ into a selection of elements from a large set of
basis functions. In a sparse way, as the signal gets either represented with
a small selection of basis functions or as a series of many basis
functions where the majority of coefficients, that weight the function, are
close to zero. 

\paragraph{}
We call these basis functions \emph{atoms} as they are the basic units
which build up our representation of signals. So far, consider the actual
structure of those atoms as unknown. We organize all of them in a
indexed set called a \emph{dictionary} analog to a dictionary of words, where
the indices are the words and the atoms the description of them. 

Such dictionaries capture the essence of the signals that they represent.
This fact calls for good dictionaries to achieve good signal representation.

There exist two major distinct ways to construct the desired dictionaries. 
First the construction of a model based on basis functions found by harmonic
analysis that can represent our signal in a sparse way. Second with
algorithms that find or learn the atoms through a training process from a set of
training signal samples. The latter approach of learning dictionaries is the
major topic of this thesis. The next section gives an insight into the history
of sparse sensing in digital signal processing. 



\section{History}
\label{sec:history}
%Compressed sensing
%\cite{Rubinstein2010}
The field of signal processing reaches far back in the early 60s.
Starting with combinations of sine and cosine transformations to approximate
analog signals. 
%For example the signal could be represented by combinations of
%connected oscilloscopes with different frequency configurations. 
Later on approximation of signals via combinations of limited signal samples
became a major task in signal analysis. This led to the Fourier transform as one
of the most widely used tools. A function $x(t)$ representing a signal depending
on time $t$ is transformed into a function $\alpha(v)$ depending on frequency
$v$. Describing the signal as a sum of sinusoids. 
%Resulting in a functionof frequency $v$ that represents $x(t)$ as the integral
%of basis function over time $t$.   
\begin{equation*}
\alpha\left(v\right) = \int_{-\infty}^{\infty} \! x(t)e^{-i2\pi
v t} \,
\mathrm{d}t\label{eq:ft}
\end{equation*}

\paragraph{Getting discrete}
With the rise of the computer the term \emph{digital} entered the
game of signal processing. To utilize these new powers a move from continues
to discrete signal representation was mandatory. This paradigm change called
for discrete basis transforms. The \emph{discrete Fourier transform (DFT)}
emerged as a special discrete version of the continuous Fourier transform. The
integral over time becomes a sum of the signal $\vec{x}$
with dimension $N$. 
\begin{equation}
 \alpha_k = \sum_{n=0}^{N-1}x_ne^{\frac{-i2\pi kn}{N}}\label{eq:dft}
\end{equation}
In 1965 Cooley and Tukey presented\cite{Cooley1965} the \emph{fast Fourier
transform (FFT)} a fast algorithmic approach to calculate the discrete
Fourier transform. The FFT was a major leap forward in the field of digital
signal processing. 
In the following decades two new major players in discrete basis
transformations emerged. The \emph{discrete cosine transform (DCT)} and the
\emph{discrete wavelet transform (DWT)}. 

The DCT is a special case of the discrete Fourier transform which only uses the
real part of the equation. Leading from \prettyref{eq:dft} to:
%The idea is to make $f$ an odd function with $\forall x \in \mathbb{R} : f(x)
%= f(-x)$ which leads to the imaginary part of the equation becoming zero. 
\begin{equation*}
\alpha_k = \sum_{n=0}^{N-1}x_n\cos \left[ \frac{\pi}{N} \left(
n+\frac{1}{2}\right) k\right]
\end{equation*}
An example for the usage of DCT is the coding of the $N\times N$ image blocks in
the JPEG image compression algorithm.

Other than the Fourier transform the DWT describes a signal as a series of
filter functions called wavelets, which are not only localized in frequency
(like the sinusoids of the Fourier transform) but also localized in time. 
%Localized in frequency and time. 
%(DWT) captures both frequency and location information. 
% Property of the basis functions ...
%$\int_{-\infty}^{\infty} \! f(t) \, \mathrm{d}t = 0$
For example this locality property is used in JPEG 2000 image compression to
remove the block artifacts that occur in JPEG. 

In the 80s the search for better transformation basis started to become a major
role in the field of signal representation. Leading to others such as Bandelets,
Curvelets, Contourlets, Wedgelets among others.

\paragraph{Splitting the problem}
%\cite{Rubinstein2010}
%\Todo{doppelt?}
In the last two decades (see\cite{Olshausen1996,Mallat1993}) a concept emerged
to interpret basis transforms as sets of discrete signal atoms in a dictionary
and the signals that they reconstruct as sparse linear combination of these
atoms. The benefit of this approach is that you can decouple signal coding and
dictionary design and split the whole process of signal analysis into two tasks.
First coding of signals and second the design of dictionaries. Separating the
problem into two distinct problems made the search for efficient coding of
signals $\vec{x}$ and construction of task specific dictionaries $\mat{D}$ more
flexible. The initial problem becomes:
\begin{equation}
 \vec{x} \sim \vec{\tilde{x}}  = \mat{D}\vec{\alpha}
\end{equation}
%Or for better understanding in relation to the previously presented
%equations.
%\begin{equation}
%\tilde{x} = \sum_{n=0}^{N-1}d_n\alpha_n\notag
%\end{equation}

Ongoing research in this field emerged in order to:
\begin{itemize}
 \item Find efficient ways for coding signals in a sparse way.
 \item Design or learn a dictionary that can code specific signals in
a sparse way with low error.
\end{itemize}
The next two chapters give an insight into both topics. 
%In computer vision applications for digital signal processing are for example
%removing of image noise, image reconstruction and compression among.



\section{Goal of this thesis}
The main purpose of the thesis is to investigate the usability of dictionary
learning to learn dictionaries for large image databases. 
%that can be used for specific tasks on. 
In 2009 Mairal et al. of the Willow
Project\footnote{\url{http://www.di.ens.fr/willow/}} presented in
\cite{Mairal2009,Mairal2010} an online dictionary learning algorithm for sparse
coding. The presented algorithm enables us to learn dictionary elements from
large training sets found in large image databases.

They also released the Matlab framework \emph{SPArse Modeling Software}
short SPAMS\footnote{\url{http://www.di.ens.fr/willow/SPAMS/}} with all
necessary functions to recreate their results and experiment with the algorithm.
The problem is that the framework is closed source, which is impractical for
modifications like usage of different coding algorithms in the training process.
Also Matlab has some limitations that make it hard to test the algorithms with
big dictionaries.

To address these problems we reimplemented all necessary functions of the
framework in C++ to experiment with the applicability of sparse coding for
learning large dictionaries for large image databases. The three major steps of
the following chapters are:

\begin{itemize}
 \item Fast sparse coding of many small signals under different constraints
 \item Efficient learning of redundant dictionaries for large image databases
from a large set of training data
 \item Evaluation of the quality and usage of the learned dictionaries
\end{itemize}

This includes topics like 
``When do dictionaries of different sizes and learning configurations show
convergence?'' 
``How many atoms do we need for a 'good' sparse representation of large image
databases?''  and 
``How can we use these universal dictionaries for applications like
image compression?''. 

All these topics are addressed in several experiments.
Evaluation of the quality and size of learned big redundant dictionaries
for optimal sparse coding of large image databases. 
%Search for a universal dictionary for databases of hundredths of thousands of
%images. 
Evaluation of clustering of the learning algorithm. Experiments with small sets
of specific images from sketches. 
%, still images of animations from Disney and art styles like
%post-impressionistic images from Vincent van Gogh.
Application of sparse coding for image compression and observation of problems
and comparison with discrete cosine transformation approaches in JPEG and
discrete wavelets transformation of JPEG 2000.

%Such as encoding time and quality benefit.




