\chapter{Overview}


\section{Problem}
\subsection{dictionaries and application}
Signal representation via linear combination of signal vectors from a dictionary.
Coding the signal. Design of the dictionary.

\section{signal representation}
approximate signals via combination of limited signal samples
Why?
signal analysis, compression, denoise etc.

\subsection{discrete signals}
Rather then using continues signal we concentrate on discret signal representation.
Continues signals would be better for .... . But the final signal (image etc.) will be discret anyway becaus of the 
digital representation. Besides the problem of coding becomes different in the continues space??

\subsection{basis transformation}
The early approaches in the 60s used combinations of cosine transformations. Coming from a continues representation this makes 
actually sense. The signal could be represented via ....

In 80s the search for better transformation basis became a major role in signal representation. \cite{}

\subsubsection{spliting the problem}
In the last 15 years 

the idea came up to interpret the basis transforms as sparse linear combination of dictionary atoms.
see \cite{Olshausen Field 1997} and \cite{}
The benefit from this approach/direction was that you could decouple signal coding and dictionary design.
Seperating the problem into two disctinct problems made the search for task specific and .... more flexible \cite{?}.


\subsection{sparse codes/coding}
Sparse coding is the 
\[
\underbrace{\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}}_{signal} \approx \underbrace{\begin{pmatrix} d_1  d_2 \cdots d_n \end{pmatrix}}_{\textrm{dictionary}}
\underbrace{\begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n \end{pmatrix}}_{\textrm{keep sparse}}
\]
solve under-determined linear system
we want the sparsest solution
\[
\min_{\alpha\in\mathbb{R}^{p}} \frac{1}{2} \lVert x - D\alpha \rVert^{2}_{2} + \underbrace{\psi(\alpha)}_{regularization}
\] 
measure sparsity via       l0-norm       ||a||0

Da=X
<>

In the last 15 years several sparse coding algorithms have been proposed. 
Some that solve the initial problem <> greedyly, the (orthogonal) matching pursuit, and others which modified the problem to become convex/linear. These primary derive from the numerical domain in the form of 
large linear system solvers with few optimization constraints. The LARS-Lasso, basis pursuit, FOCUSS?


blaa \footnote{test 123} blub

||a||0 makes the problem NP-hard
to get best solution you need to test every combination
Solution:
use greedy approach or make problem convex (e.g. use ||a||1)
the most common algorithms are the following ones.

\subsubsection{matching pursuit}
\begin{algorithmic}
\IF {$i\geq maxval$} 
        \STATE $i\gets 0$
\ELSE
        \IF {$i+k\leq maxval$}
                \STATE $i\gets i+k$
        \ENDIF
\ENDIF 
\end{algorithmic}

\subsubsection{orthogonal matching pursuit}
\label{sec:omp}
\begin{algorithm}
\caption{Wurst}
\begin{algorithmic}
\IF {$i\geq maxval$} 
        \STATE $i\gets 0$
\ELSE
        \IF {$i+k\leq maxval$}
                \STATE $i\gets i+k$
        \ENDIF
\ENDIF 
\end{algorithmic}

\subsubsection{LARS-Lasso}
\begin{algorithmic}
\IF {$i\geq maxval$} 
        \STATE $i\gets 0$
\ELSE
        \IF {$i+k\leq maxval$}
                \STATE $i\gets i+k$
        \ENDIF
\ENDIF 
\end{algorithmic}
\end{algorithm}

\section{Dictionaries/representation data}
\subsection{analytical}
\subsubsection{consine}
\subsubsection{wavelets}

\subsection{learned over-complete}
recent research has shown that learnd dictionaries show better compression quality than small analytic dictionaries \cite{Aharon2006} \cite{Chen1998} 


In the last decade several learning algorithms have been proposed which try to a universal basis that 
can sparsly reconstruct a set of "trainig data" with minimal error. 
K-SVD
MOD
Online learning
Mairal2010

\subsubsection{k-svd}
\subsubsection{online learning}
Mairal .... \cite{Mairal2010}

\section{Learning for the Task}
It has been shown that learning basis specicial for certain tasks can lead to the best results\cite{}.  <>
Based on this discovery we will concentrate on a specific class. <> Join the basis for natural images and cartoon/line images.
We will also concentrate on real pratical data. This means typically 3-channel data of 1+ megapixel images found on image hosting services like flickr, twitpic, etc.

Current research is primary concentrating on other tasks. 
Like ... related work :)

\section{Related work}
\subsection{denoising}
Remove noisy signal from image
\subsection{in-paiting}
fill missing parts by removing row from the dictionary
\subsection{better structure}
\subsubsection{multi-scale}
\subsubsection{multi-channel}
\subsubsection{double sparsity}
\subsubsection{hirarchy/structure}
Adding hirarchy/structure to dictionaries can lead to faster coding or use in structure analysis ( text documents )
\cite{Jenatton2010}
\subsubsection{training with a neural network}
Learning Multiple Layers of Features from Tiny Images \cite{Krizhevsky2009}


\section{Image database/Training data}
Task specific training data is a common way to solve the problem of finding the right dictionary. 
Here learning for the task comes into account. Denoising/Inpainting dictionaries directly learned from the initial
signal that gets denoised or restored from inpaiting.
If the task gets bigger it sounds logical to increase the size of training data and take a bigger veriety of signals to learn from.


\section{Goal (of this thesis)}
Evaluate the quality and size of learned big redundant dictionaries for 
optimal sparse coding of large image databases.!!!!!

Obeservation of problems (encoding time, quality benefit, etc.)

what about a universal dictionary for combination natural images and line graphics?
 
how many elements for a 'good' sparse representation?

add structure for speed and model improvements
Multi-scale, multi-channel, hirarchy

Improve compression and usage of coding for as a image descriptor.
