\chapter{Overview}
%\thispagestyle{empty}
\numberwithin{equation}{chapter}

\section{Dictionaries for signal representation}
%\subsection{Problem statement}

When a signal is meant to be sparse it is either representable with a small
number of combined terms or a series of many terms where the majority of
coefficients is close to zero. Consider $x$ a signal that is represent by a
function $f(x)$ that approximates the signal in a sparse way.
\begin{equation*}
x \sim \tilde{x} = f\left(x\right)
\end{equation*}

Simplification of signals in signal processing.
\begin{equation*}
\begin{split}
\tilde{x} & = b_{1}(x) + b_{12}(x) + b_{312}(x)\\
& = c_{1}b_{1}(x) + 0.001b_{2}(x) ... + 2b_{12}(x) + 0.021b_{13}(x) .. +
10b_{312}(x)
\end{split}
\end{equation*}



Those basis signal can be interpreted as atoms in a codebook or dictionary.

We define a \emph{dictionary} as a set of different atoms. We define an atom
elementary signal or the smallest unit in such a dictionary.

There are essential two major distinct ways to construct the 
desired dictionaries. First the construction of model based on basis functions
found via harmonic analysis that can represent our signal in a sparse way or
second via algorithms that learn a sparse dictionary via a training process from
a set of training data/signal samples. The later approach of training good
dictionaries has become a major task in the last decades\cite{Mairal2010}.

As a matter of fact a good basis or dictionary is essential for a good
signal approximation\cite{}.

%Orthonormal (feature) vectors are orthogonal and have unit length.


\subsection{History}
\label{sec:history}
%Compressed sensing
The field of signal transformation reaches far back in the early 60s.
\cite{Rubinstein2010}

In the early approaches in the 60s combinations of cosine
transformations were used to approximate analog signals coming from a continues
representation this makes actually sense. The signal could be represented via
combinations of connected oscilloscopes with different frequency
configurations. 

Later on approximation of signals via combination of limited signal samples
became a major task in signal analysis. Several 
%application of such signal analysis and interpretation.

\paragraph{Fourier Transform}

%copy
The most widely accepted analytic tool for such problems is the Fourier
transform. The Fourier transform decomposes a signal into basis functions of
different frequencies, known as frequency components, and gives us the amplitude
of each frequency component in the signal. The waves are of the form:

\begin{align*}
 \mathcal{F}\left(v\right) = \int_{-\infty}^{\infty} \! f(t)e^{-i2\pi vt} \,
\mathrm{d}t
\end{align*}





%\begin{align*}
%\end{align*}

\paragraph{Getting discrete}
%\subsection{Discrete signals}

%copy
With the increasing use of computers the usage of and need for digital signal
processing has increased.
A move from linear signals to discrete signals happened in 80s when ... 
Rather then using continuous signals concentrate on discrete
signal representation. Continuous signals would be better for .... .  But our
signal (e.g., images) will be discrete anyway because of their initial digital
representation. Besides the problem of coding becomes different in the continues
space \cite{} Because of ...

\begin{align*}
 X_k = \sum_{n=0}^{N-1}x_ne^{\frac{-i2\pi kn}{N}}
\end{align*}

\Todo{FFT}
FFT in 65s
FT or the later FFT as an fast algorithmic approach to calculate the discrete
Fourier transform.
The \emph{fast Fourier transform}  as an fast algorithmic approach to calculate
the discrete Fourier transform.



Stay in the frequency domain

Two of the major players in discrete 

\paragraph{Discrete cosine transformation}
Is special case of the discrete Fourier transform ( only real part ).
The idea is to make $f$ an odd function $f(x) = f(-x) $ which leads to the
imaginary part becoming 0.
\begin{align*}
X_k = \sum_{n=0}^{N-1}x_n\cos \left[ \frac{\pi}{N} \left(
n+\frac{1}{2}\right) k\right]
\end{align*}
\Todo{}

In image compression this basis is primary used in the JPEG compression.


\paragraph{Discrete wavelet transformation}

Property of the basis functions
$\int_{-\infty}^{\infty} \! f(t) \, \mathrm{d}t = 0$

\Todo{}
Other than the Fourier transform the wavelets transforms capture both frequency
and location information.

For example these basis are used in JPEG 2000 image compression and 

\Todo{move}
In computer vision such applications would be de-noising, in-paiting,
compression among others. In 80s the search for better transformation basis
started to become a major role in field of signal representation.\cite{}


\subsection{Splitting the problem}
\cite{Rubinstein2010}
In the last 15 years a concept emerged to interpret basis transforms as a set of
signal atoms in a dictionary and the signals that they reconstruct as sparse
linear combination of these atoms. see\cite{Olshausen1996,Mallat1993} The
benefit of this approach is that you can decouple signal coding and dictionary
design and split the whole process of signal analysis into two tasks. First
coding of signals and second the design of dictionaries. Separating the problem
into two distinct problems made the search for efficient coding of signals $z$
and construction of task specific dictionaries $D$ more flexible\cite{?}. The
initial problem becomes:


\Todo{	}
\begin{equation}
X_k = \sum_{n=0}^{N-1}d_nx_n
\end{equation}

\begin{equation}
 x \sim \tilde{x} = D\alpha
\end{equation}



Ongoing research in this field emerged in order to:
\begin{itemize}
 \item Find efficient ways for coding signals in a sparse way.
 \item Design or learn a good dictionary that can code specific signal in sparse
way with low error.
\end{itemize}

%analog problem video compression but with less correlation between images and still image 

\section{Goal of this thesis}
The main purpose of the thesis is to learn good dictionaries from large
image databases. %that can be used for specific tasks on. 
In 2009 Mairal et al. of the Willow
Project\footnote{\url{http://www.di.ens.fr/willow/}} presented in
\cite{Mairal2009,Mairal2010} an online dictionary learning algorithm for sparse
coding. The presented algorithm enables us to learn dictionary elements from
large training sets found in large image databases.

They also released the Matlab framework \emph{SPArse Modeling Software}
short SPAMS\footnote{\url{http://www.di.ens.fr/willow/SPAMS/}} with all
necessary functions to recreate their results and experiment with the algorithm.
The problem is that the framework is closed source, which is impractical for
modifications like usage of different coding algorithms in the training process.
Also Matlab bears/includes some limitations that make it hard to test the
algorithms with big dictionaries.

To address these problems we reimplemented all necessary functions of the
framework in C++ to experiment with the applicability of sparse coding for
learning large dictionaries from large image databases. The three major steps of
the following chapters are:

\begin{itemize}
 \item Fast sparse coding of many small signals under different constraints
 \item Efficient learning redundant dictionaries for large image databases from
a large set of training data
 \item Evaluation of the quality and usage of the learned dictionaries
\end{itemize}

This includes topics like ``When do dictionaries different sizes and
learning configurations show convergence?'' ``How many atoms do we need  for a
'good' sparse representation of large image databases?''  and ``How does this
affect quality and usability for different application?''. 

\Todo{einbauen/conclusion?}
Better understanding the selection strategies of the algorithm and their
meaning for perceptional image quality. And evolution of the structure of
learned dictionaries. 

All these topics are addressed in several experiments. Experiments with
small sets specific images like from sketches, still images of animations from
Disney and art styles like post-impressionistic images from Vincent van Gogh.
Evaluation of the quality and size of learned big redundant dictionaries for
optimal sparse coding of large image databases. Search for a universal
dictionary for databases of hundredths of thousands of images. Evaluation of
clustering of learning algorithms. Application of sparse coding for image
compression and observation of problems. Such as encoding time, quality benefit.
Comparison of compression with discrete cosine transformation approaches in JPEG
and discrete wavelets transformation of JPEG 2000 and possible usage as an image
descriptor.


%Convergence of the dictionary learning
%Quality increase with size increase 
%Comparison with JPEG,JPEG 2000 via RMSE



%\Todo{OPTIONAL add structure for speed and model improvements Multi-scale, multi-channel, hierarchy}


