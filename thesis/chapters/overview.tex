\chapter{Overview}
%\thispagestyle{empty}
\numberwithin{equation}{chapter}

\section{Dictionaries for signal representation}
%\subsection{Problem statement}

When a signal is meant to be sparse it is either representable with a small
numberof combined terms or a series of many terms where the majority of
coefficients is close to zero. The signal $x$ is represent by a function $f(x)$
that approximates the signal in a sparse way.
\begin{equation*}
x \approx f\left(x\right)
\end{equation*}

Simplification of signals in signal processing.
\begin{gather*}
x \approx b_{1} + b_{12} + b_{312}\\
 \text{v.s.}\\ x \approx 3b_{1} + 0.001b_{2} ... + 2b_{12} + 0.021b_{13} .. + 10b_{312}
\end{gather*}



Those basis signal can be interpreted as atoms in a codebook or dictionary.

We define a dictionary as a set of different atoms. We define an atom
elementary signal or the smallest unit in such a dictionary.

There are essential two major distinct ways to construct the 
desired dictionaries. First the construction of model based on basis functions
found via harmonic analysis that can represent our signal in a sparse way or
second via algorithms that learn a sparse dictionary via a training process from
a set of training data/signal samples. The later approach of training good
dictionaries has become a major task in the last decades\cite{Mairal2010}.

As a matter of fact a good basis or dictionary is essential for a good
signal approximation\cite{}.

%Orthonormal (feature) vectors are othogonal and have unit length.


\subsection{History}
\label{sec:history}
%Compressed sensing
The field of signal transformation reaches far back in the early 60s.
\cite{Rubinstein2010}

The early approaches in the 60s used combinations of cosine transformations. 
Coming from a continues representation this makes actually sense. The signal
could be represented via combinations of connected oscilloscopes with different
frequency configurations. 

Approximate signals via combination of limited signal samples Why? The
application of such signal analysis and interpretation.


\Todo{FFT}
FFT in 65s
%\begin{align*}
%\end{align*}

%\subsection{Discrete signals}


\paragraph{Discrete cosine transformation}
%\begin{align*}
%\end{align*}
\Todo{}

\paragraph{Discrete wavelet transformation}
%\begin{align*}
%\end{align*}
\Todo{}

In computer vision such applications would be de-noising, in-paiting,
compression among others. In 80s the search for better transformation basis
became a major role in signal representation.\cite{}

Rather then using continuous signals concentrate on discrete
signal representation. Continuous signals would be better for .... .  But our
signal (e.g., images) will be discrete anyway because of their initial digital
representation. Besides the problem of coding becomes different in the continues
space \cite{} Because of ...

\subsection{Splitting the problem}
\cite{Rubinstein2010}
In the last 15 years a concept emerged to interpret basis transforms as a set of
signal atoms in a dictionary and the signals that they reconstruct as sparse
linear combination of these atoms. see\cite{Olshausen1996,Mallat1993} The
benefit of this approach is that you can decouple signal coding and dictionary
design and split the whole process of signal analysis into two tasks. First
coding of signals and second the design of dictionaries. Separating the problem
into two distinct problems made the search for efficient coding of signals $z$
and construction of task specific dictionaries $D$ more flexible \cite{?}. The
initial problem becomes:

\begin{equation}
 x \approx D\alpha
\end{equation}


Ongoing research in this field emerged in order to:
\begin{itemize}
 \item Find efficient ways for coding signals in a sparse way.
 \item Design or learn a good dictionary that can code specific signal in sparse
way with low error.
\end{itemize}

%analog problem video compression but with less correlation between images and still image 

\section{Goal of this thesis}
The main purpose of the thesis is to learn good dictionaries from large
image databases. %that can be used for specific tasks on. 
In 2009 Mairal et al. of the Willow
Project\footnote{\url{http://www.di.ens.fr/willow/}} presented in
\cite{Mairal2009,Mairal2010} an online dictionary learning algorithm for sparse
coding. The presented algorithm enables us to learn dictionary elements from
large training sets found in large image databases.

They also released the Matlab framework ``SPArse Modeling Software''
short SPAMS\footnote{\url{http://www.di.ens.fr/willow/SPAMS/}} with all
necessary functions to recreate their results and experiment with the algorithm.
The problem is that the framework is closed source, which is impractical for
modifications like usage of different coding algorithms in the training process.
Also Matlab bears/includes some limitations that make it hard to test the
algorithms with big dictionaries.

To address theres problems we reimplemented all nessacary functions of the
framework in C++ to experiment with the applicability of sparse coding for
learning large dictionaries from large image databases. The three major steps of
the following chapters are:

\begin{itemize}
 \item Fast sparse coding of many small signals under different constraints
 \item Efficient learning redundant dictionaries for large image databases from a large set of training data
 \item Evaluation of the quality and usage of the learned dictionaries
\end{itemize}

This includes topics like ``When do dictionaries different sizes and
learning configurations show convergence?'' ``How many atoms do we need  for a
'good' sparse representation of large image databases?''  and ``How does this
affect quality and usability for different application?''. 

All these topics are addressed in several experiments. Experiments with
small sets specific images like from sketches, still images of animations from
Disney and art styles like post-impressionistic images from Vicent van Gogh.
Evaluation of the quality and size of learned big redundant dictionaries for
optimal sparse coding of large image databases. Search for a universal
dictionary for databases of hundredths of thousands of images. Evaluation of
clustering of learning algorithms. Application of sparse coding for image
compression and observation of problems. Such as encoding time, quality benefit.
Comparison of compression with discrete cosine transformation approaches in jpeg
and discrete wavelets transformation of jepg2000 and possible usage as an image
descriptor.


%Convergence of the dictionary learning
%Quality increase with size increase 
%Comparison with jpeg,jpeg2000 via RMSE



%\Todo{OPTIONAL add structure for speed and model improvements Multi-scale, multi-channel, hierarchy}


