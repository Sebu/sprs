\chapter{Overview}

\section{Problem/Idea}
\subsection{dictionaries and application}
Signal representation via linear combination of few signal vectors from a dictionary.
Benefits of dictionaries

\subsection{basis transformation}
The early approaches in the 60s used combinations of cosine transformations. Coming from a continues representation this makes 
actually sense. The signal could be represented via ....

In 80s the search for better transformation basis became a major role in signal representation. \cite{}

\subsection{splitting the problem}
In the last 15 years 

the idea came up to interpret the basis transforms as sparse linear combination of dictionary atoms.
see \cite{Olshausen Field 1997} and \cite{}
The benefit from this approach/direction was that you could decouple signal coding and dictionary design.
Separating the problem into two distinct problems made the search for task specific and .... more flexible \cite{?}.

Two separate tasks.
Coding the signal. Design of the dictionary.


\section{signal representation}
approximate signals via combination of limited signal samples
Why?
signal analysis, compression, de-noise etc.

\subsection{discrete signals}
Rather then using continues signal we concentrate on discrete signal representation.
Continues signals would be better for .... . But the final signal (image etc.) will be discrete anyway becaus of the digital representation. Besides the problem of coding becomes different in the continues space??


\section{sparse codes/coding}
a linear combination of a “few” non orthonormal atoms from
D that is “close” to the signal X
try to keep coefficient vector sparse
usage: reduction, classification, in-painting,  super-resolution
Sparse coding is the 
\[
\underbrace{\begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix}}_{signal} \approx \underbrace{\begin{pmatrix} d_1  d_2 \cdots d_n \end{pmatrix}}_{\textrm{dictionary}}
\underbrace{\begin{pmatrix} \alpha_1 \\ \alpha_2 \\ \vdots \\ \alpha_n \end{pmatrix}}_{\textrm{keep sparse}}
\]
solve under-determined linear system
we want the sparsest solution
\[
\min_{\alpha\in\mathbb{R}^{p}} \frac{1}{2} \lVert x - D\alpha \rVert^{2}_{2} + \underbrace{\psi(\alpha)}_{regularization}
\] 
measure sparsity via       l0-norm       ||a||0

Da=X
<>

In the last 15 years several sparse coding algorithms have been proposed. 
Some that solve the initial problem <> greedyly, the (orthogonal) matching pursuit, and others which modified the problem to become convex/linear. These primary derive from the numerical domain in the form of 
large linear system solvers with few optimization constraints. The LARS-Lasso, basis pursuit, FOCUSS?


blaa \footnote{test 123} blub

||a||0 makes the problem NP-hard
to get best solution you need to test every combination
Solution:
use greedy approach or make problem convex (e.g. use ||a||1)
the most common algorithms are the following ones.

\subsection{matching pursuit}
\begin{algorithmic}
\IF {$i\geq maxval$} 
        \STATE $i\gets 0$
\ELSE
        \IF {$i+k\leq maxval$}
                \STATE $i\gets i+k$
        \ENDIF
\ENDIF 
\end{algorithmic}

\subsection{orthogonal matching pursuit}
\label{sec:omp}
\begin{algorithm}
\caption{Wurst}
\begin{algorithmic}
\IF {$i\geq maxval$} 
        \STATE $i\gets 0$
\ELSE
        \IF {$i+k\leq maxval$}
                \STATE $i\gets i+k$
        \ENDIF
\ENDIF 
\end{algorithmic}

\subsection{LARS-Lasso}
The LASSO (least absolute shrinkage and selection operator) is a algorithm to find a regularized version of a least squares solution.
The regularized version is found by adding a constraint that induces the $L_1$-norm of the solution to be small.

The LARS algorithm ... \cite{least angle regression}
The LARS-Lasso is a algorithm to solve the LASSO with the help of least angle regression
as described in \cite{Least Angle Regression}. It is a modified version of the LARS where ....

\begin{algorithmic}
\IF {$i\geq maxval$} 
        \STATE $i\gets 0$
\ELSE
        \IF {$i+k\leq maxval$}
                \STATE $i\gets i+k$
        \ENDIF
\ENDIF 
\end{algorithmic}
\end{algorithm}

\section{Dictionaries/representation data}
\subsection{analytical}
\subsubsection{cosine}
\subsubsection{wavelets}

\subsection{learned over-complete}
recent research has shown that learnd dictionaries show better compression quality than small analytic dictionaries \cite{Aharon2006} \cite{Chen1998} 


In the last decade several learning algorithms have been proposed which try to a universal basis that 
can sparsly reconstruct a set of "trainig data" with minimal error. 
K-SVD
MOD
Online learning
Mairal2010

\subsubsection{k-svd}
\subsubsection{online learning}
Mairal .... \cite{Mairal2010}

\section{Learning for the Task}
It has been shown that learning basis specialy for certain tasks can lead to the best results\cite{}.  <>
Based on this discovery we will concentrate on a specific class. <> Join the basis for natural images and cartoon/line images.
We will also concentrate on real practical data. This means typically 3-channel data of 1+ megapixels images found on image hosting services like flickr, twitpic, etc.

Current research is primary concentrating on other tasks. 
Like ... related work :)

\section{Related work}
\subsection{application}
\subsubsection{de-noising}
Remove noisy signal from image
\subsubsection{in-painting}
fill missing parts by removing rows from the dictionary
Train with the original image

\subsection{multi-scale}
\cite{saprio}
\subsection{multi-channel}
coding every channel separate or combine them
into single signal. can lead to color bleeding \cite{mairal08sparse}
fortunately this problem vanishes with big dictionaries and large training sets. \cite{mairal08sparse}

\subsection{double sparsity}
Sparse code dictionaries. 

\subsection{group sparsity}
similar patches should admit similar patterns
  
\cite{double sparsity}
\subsubsection{hirarchy/structure}
Adding hirarchy/structure to dictionaries can lead to faster coding or use in structure analysis ( text documents )
\cite{Jenatton2010}
\subsubsection{training with a neural network}
Learning Multiple Layers of Features from Tiny Images \cite{Krizhevsky2009}


\section{Image database/Training set}
Task specific training data is a common way to solve the problem of finding the right dictionary. 
Here learning for the task comes into account. de-noising/in-painting dictionaries directly learned from the initial
signal that gets de-noised or restored from in-painting.
If the task gets bigger it sounds logical to increase the size of training data and take a bigger veriety of signals to learn from.


\section{object/aim/intention of this thesis}
Evaluate the quality and size of learned big redundant dictionaries for 
optimal sparse coding of large image databases.!!!!!

Observation of problems (encoding time, quality benefit, etc.)

Search for a universal dictionary for combination natural and artificial images
 
Size of 
how many elements for a 'good' sparse representation?

add structure for speed and model improvements
Multi-scale, multi-channel, hierarchy

Improve compression and usage as a image descriptor.
