\chapter{Dictionaries}
%\thispagestyle{empty}

\section{Structure}
Set of orthonormal signal vectors.
\subsection{Designed}

First presented in \ref{sec:history}

\paragraph{cosine}
\begin{figure}
%\centering
\includegraphics[width = 0.33\textwidth]{images/dct.png}
\caption{dct from wiki jpeg}
\label{fig:dct}
\end{figure}

\paragraph{wavelets}
\paragraph{curvelets/contourlets/bandelets}


\subsection{Learned}

%\paragraph{Machine Learning}

Besides the construction of dictionaries from mathematical functions, machine learning .. .


``A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E''


\paragraph{principal component analysis}


\paragraph{redundant}
Unlike the former designed dictionaries and the PCA approach. Dictionaries atoms learned with machine learning methods are not necessarily othogonal.
Infact the benefit of this new approach is to have overcomplete dictionies w
%The atoms are not orthonormal. 
%The dictionary is overcomplete. Overcomplete ...



Recent research has shown that learned dictionaries show better compression quality than small analytic dictionaries \cite{Chen1998} \cite{Aharon2006} \cite{Mairal2010}
Rather than selecting an appropriate mathematical construct that can reconstruct a certain group of signals close enough the data, a subset or similar data
is used as a training set to generate the dictionary itself.




%\subsubsection{k-svd}
\section{Learning}
In the last decade several learning algorithms have been proposed.
The basic concept of following algorithms for learning redundant dictionaries is to alter a initial start dictionary
in this way that it can sparsely reconstruct a set of training data with minimal error. 

\subsection{Batch}
MOD \cite{Engan1999a}
Generalized PCA \cite{}
ILS-DLA \cite{}
K-SVD \cite{Aharon2006}


\subsection{Online}
In 2010 new training algorithms were presented that enabled on-line learning of dictionaries. 
In contrast to batch learning algorithms with a fixed training set the new approaches enabled
This is good for very large training sets or training data that is unknown at the start of the training process.

... RLS-DLA \cite{Engan2010} ...

\Todo{find the Japanese one}

In 2010 Mairal et. al. \cite{Mairal2010} proposed an on-line approach which from now on will be called TrainDL

\subsubsection{\trainDL}
\label{sec:mairal}
The algorithm presented by Mairal et al. \cite{Mairal2010} is ...

\begin{algorithm}
\caption{Online dictionary learning \cite{Mairal2010}}
\begin{algorithmic}[1]
\REQUIRE $x \in \mathbb{R}^{m \times k},  p \left( x \right), \lambda \in \mathbb{R}, D_0 \in \mathbb{R}^{m \times p}, T \in \mathbb{N}$
\STATE $A_0 \in \mathbb{R}^{p \times p} \gets  0, B_0 \in \mathbb{R}^{m \times p}\gets 0$
\FOR {$t = 1$ to $T$}
\STATE Draw $x$ from p(x).
\STATE Sparse code:
\begin{align*} 
\alpha_t \equiv \argmin_{\alpha\in\mathbb{R}^{p}}  \lVert x_t - D_t\alpha \rVert^{2}_{2}  +  \lambda \lVert \alpha \rVert_{1}
\end{align*}

\STATE $A_t \gets A_{t-1} + \alpha_t\alpha_t^T$
\STATE $B_t \gets B_{t-1} + x_t\alpha_t^T$
\STATE Compute $D_t$ using \prettyref{alg:update}, with $D_{t-1}$ as warm restart 
\begin{align} 
D_t \equiv \argmin_{D \in \mathbb{R}^{m x k}}  \frac{1}{t} \sum_{i=1}^t \left( \lVert x_i - D\alpha_i \rVert^{2}_{2}  +  \lambda \lVert \alpha_i \rVert_{1} \right) \label{eq:update}
\end{align}
\ENDFOR
\RETURN $D_T$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Dictionary Update}
\label{alg:update}
\begin{algorithmic}[1]
\REQUIRE $D=[d_1,...,d_p] \in \mathbb{R}^{m \times p}, A=[a_1,...,a_p] \in \mathbb{R}^{p \times p}, B=[b_1,...,b_p] \in \mathbb{R}^{m \times p}$
\REPEAT
\FOR {$j = 1$ to $p$}
\STATE update j-th column to optimze for \prettyref{eq:update}:
\begin{align*}
u_j \gets \frac{1}{A[j,j]}\left(b_j-Da_j\right)+d_j \\
d_j \gets \frac{1}{\max\left(\lVert u_j \rVert_2,1\right)} u_j
\end{align*}

\ENDFOR
\UNTIL convergence 
\RETURN $D$
\end{algorithmic}
\end{algorithm}

In \cite{Mairal2010} Mairal et al. propose sereval modifikation to their algorith to improve convergence.



\section{Learning for the Task}
Guillermo Sapiro mentions in his talk \cite{sapiroSlides} at the MIT CSAIL that it has been shown that learning specific for certain tasks can lead to the best results.
``Please do not use the wrong dictionaries...''

Based on this discovery we will concentrate on a specific class. 
%<> Join the basis for natural images and cartoon/line images.
We will also concentrate on real practical data. This means typically 3-channel data of 1+ mega-pixels images found on image hosting services such as flickr, twitpic, imgurl, picasa.


\section{Related work}
%Current research is primary concentrating on other tasks. 
%Like ... related work :)



%\begin{description}

\paragraph{texture synthesis}

\paragraph{compression of facial images}
In 2008 Bryt and Elad \cite{Bryt2008} 

\paragraph{training with a neural network}
Learning Multiple Layers of Features from Tiny Images \cite{Krizhevsky2009}


\paragraph{multi-scale}
Rather than encoding uniform segments of a bigger signal this technique encodes different sized 
blocks (multiple times of the default block size). The idea is that aligned smooth regions with low variance can be 
can be combined an encoded as one signal rather than e.g. 4. \cite{saprioSlides}
\cite{Mairal2007}

\Todo{picture with different scales}

\paragraph{multi-channel}
When dealing with multi-channel data (e.g. RGB images) 
each channel can be coded separate or as a concatenation of each signal into a single one.
These approaches can lead to color bleeding \cite{mairal08sparse} \Todo{add pic}
Fortunately this problem vanishes with big dictionaries and large training sets. \cite{mairal08sparse}
Nevertheless a solution would be to add a constraint to the sparse coding that pays attention the correlation of the channels.
\Todo{add formula from paper}

\paragraph{double sparsity}
Instead of directly using a collection of trained atoms as dictionary elements. This approach uses sparse coding
to encode the signals of a bigger dictionary with the help of a smaller on. \cite{Rubinstein2009} 
\Todo{formula $B$ .. basis, $A$ sparse matrix, $D=BA$ ...}

\paragraph{structured sparsity}
similar patches should admit similar patterns \cite{Mairal2009}
\cite{group sparsity}
\Todo{picture with ditribution difference}

\paragraph{hierarchy}
Adding hierarchy to dictionaries can lead to faster coding of signals and capture additional information about the learned dictionaries, .
like in the use for structure analysis of text documents.  Examples for this approach can be found in \cite{Jenatton2010}.


\Todo{picture with hirarchy }



%\end{description}

