\chapter{Dictionaries}
%\thispagestyle{empty}

As defined in\label{sec:dicts} dictionaries in signal processing  
Dictionaries for sparse coding are indexed collection of discrete signal atoms
and a linear combination of a sparse selection of atoms can will reconstruct
signals.

\section{Structure}
As previously describes in\label{sec:dicts} sparse coding dictionaries can
capture the essential structures of signals. But to achieve this quality they
need the right structure. Finding the right structure for a specific signal is a
complex task. Implicit and explicit strategies to generate atoms are ... .
%Dictionaries for sparse coding 
%Set of orthonormal signal vectors.

\subsection{Designed}
The basis transformations first presented in\ref{sec:history} can also
be interpreted as discrete atoms 

\paragraph{cosine}


\paragraph{wavelets}

\begin{figure}
\centering
\subfloat[DCT]{\includegraphics[width =
0.3\textwidth]{images/dct.png}}
\hspace{5mm}
\subfloat[haar]{\includegraphics[width =
0.3\textwidth]{images/haar.png}}
\hspace{5mm}
\subfloat[gabor]{\includegraphics[width =
0.3\textwidth]{images/gabor.png}}
\caption{generated bases}

\label{fig:16_1000_lasso}
\end{figure}


\paragraph{curvelets/contourlets/bandelets}


\subsection{Learned}

%\paragraph{Machine Learning}

Besides the construction of dictionaries from mathematical functions, machine
learning .. .


``A computer program is said to learn from experience E with respect to
some class of tasks T and performance measure P, if its performance at tasks in
T, as measured by P, improves with experience E''


\paragraph{Vector quantization}

%\paragraph{Principal component analysis (PCA)}


\paragraph{redundancy}
Unlike the former designed dictionaries and the PCA approach. Dictionaries atoms
learned with machine learning methods are not necessarily orthogonal. In fact
the benefit of this new approach is to have over-complete dictionaries with
non-orthogonal basis atoms.
%The atoms are not orthonormal. 
%The dictionary is overcomplete. Overcomplete ...



Recent research has shown that learned dictionaries show better
compression quality than small analytic dictionaries\cite{Chen1998}.
\cite{Aharon2006} \cite{Mairal2010} Rather than selecting an appropriate
mathematical construct that can reconstruct a certain group of signals close
enough the data, a subset or similar data is used as a training set to generate
the dictionary itself.




%\subsubsection{k-svd}
\section{Learning}
\cite{Olshausen1997,Lewicki2000,Aharon2006}
In the last decade several learning algorithms have been proposed. The basic
concept of following algorithms for learning redundant dictionaries is to alter
a initial start dictionary in this way that it can sparsely reconstruct a set of
training data with minimal error. 

\begin{align} 
D = \argmax_{D} \sum_{i=0}^n \max_{x_i} P(x,D)
\end{align}
L loss function, E error function
\begin{align} 
D = \argmin_{D} \sum_{i=0}^n L(x_i,D)
\end{align}

\begin{align} 
\argmin_{D,\alpha} \sum_{i}
\lVert x_i - D\alpha_i \rVert^{2}_{2}  +  \lambda \lVert \alpha_i
\rVert_{0,1,2}
\end{align}


\subsection{Batch}
\paragraph{Method of optimal directions (MOD)}
MOD\cite{Engan1999a}
\paragraph{Generalized PCA}
\cite{?}
\paragraph{ILS-DLA}
\cite{Engan2007}


\subsubsection{K-SVD}
\label{sec:k-svd}

K-SVD\cite{Aharon2006}

\begin{algorithm}[H]
\caption{K-SVD}
\begin{algorithmic}[1]
\REQUIRE $X \in \mathbb{R}^{m \times k}, \lambda \in
\mathbb{R}, D_0 \in \mathbb{R}^{m \times p}, T \in \mathbb{N}$
\STATE $A \gets D_0$
\FOR {$t = 1$ to $T$}
\STATE sparse code X with D: $A$
\FOR {$i = 1$ to $p$}
\STATE $D_i \gets 0$
\STATE $I \gets \{\}$
\STATE $E \gets X_I - DA_I$
\STATE $bla$
\STATE $D_i \gets d$
\STATE $A_{i,I} \gets g^T$
\ENDFOR 
\ENDFOR
\RETURN $D$
\end{algorithmic}
\end{algorithm}

\subsection{Online}
In 2010 new training algorithms were presented that enabled on-line learning of
dictionaries. In contrast to batch learning algorithms with a fixed training set
the new approaches enabled This is good for very large training sets or training
data that is unknown at the start of the training process.

... RLS-DLA\cite{Engan2010} ...

\Todo{find the Japanese one}

In 2010 Mairal et. al.\cite{Mairal2010} proposed an on-line approach which from
now on will be called \trainDL

\subsubsection{\trainDL}
\label{sec:mairal}
The algorithm presented by Mairal et al.\cite{Mairal2010} is ...

\begin{algorithm}[H]
\caption{Online dictionary learning\cite{Mairal2010}}
\label{alg:trainDL}
\begin{algorithmic}[1]
\REQUIRE $x \in \mathbb{R}^{m \times k},  p \left( x \right), \lambda \in \mathbb{R}, D_0 \in \mathbb{R}^{m \times p}, T \in \mathbb{N}$
\STATE $A_0 \in \mathbb{R}^{p \times p} \gets  0, B_0 \in \mathbb{R}^{m \times p}\gets 0$
\FOR {$t = 1$ to $T$}
\STATE Draw $x$ from p(x).
\STATE Sparse code:
\begin{align*} 
\alpha_t \equiv \argmin_{\alpha\in\mathbb{R}^{p}}  \lVert x_t - D_t\alpha \rVert^{2}_{2}  +  \lambda \lVert \alpha \rVert_{1}
\end{align*}

\STATE $A_t \gets A_{t-1} + \alpha_t\alpha_t^T$
\STATE $B_t \gets B_{t-1} + x_t\alpha_t^T$
\STATE Compute $D_t$ using \prettyref{alg:update}, with $D_{t-1}$ as warm restart 
\begin{equation}
\begin{split}
D_t  & \equiv \argmin_{D \in \mathbb{R}^{m x k}}  \frac{1}{t} \sum_{i=1}^t
\left( \frac{1}{2} \lVert x_i - D\alpha_i \rVert^{2}_{2}  +  \lambda \lVert
\alpha_i \rVert_{1} \right) \\
& = \argmin_{D \in \mathbb{R}^{m x k}}  \frac{1}{t} \sum_{i=1}^t
\left( \frac{1}{2} Tr(D^TDA_t) - Tr(D^TB_t)\right) \label{eq:update}
\end{split}
\end{equation}
 
\ENDFOR
\RETURN $D_T$
\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{Dictionary Update}
\label{alg:update}
\begin{algorithmic}[1]
\REQUIRE $D=[d_1,...,d_p] \in \mathbb{R}^{m \times p}, A=[a_1,...,a_p] \in \mathbb{R}^{p \times p}, B=[b_1,...,b_p] \in \mathbb{R}^{m \times p}$
\REPEAT
\FOR {$j = 1$ to $p$}
\STATE update j-th column to optimize for \prettyref{eq:update}:
\begin{align*}
u_j \gets \frac{1}{A[j,j]}\left(b_j-Da_j\right)+d_j \\
d_j \gets \frac{1}{\max\left(\lVert u_j \rVert_2,1\right)} u_j
\end{align*}

\ENDFOR
\UNTIL convergence 
\RETURN $D$
\end{algorithmic}
\end{algorithm}

In \cite{Mairal2010} Mairal et al. propose several modification to their
algorithm to improve convergence.



\section{Learning for the Task}
\label{sec:learnForTheTask}
Guillermo Sapiro mentions in his talk\cite{sapiroSlides} at the \emph{MIT CSAIL}
that it has been shown that learning specific for certain tasks can lead to the
best results. <<Please do not use the wrong dictionaries...>>

Based on this discovery we will concentrate on a specific class. 
%<> Join the basis for natural images and cartoon/line images.
We will also concentrate on real practical data. This means typically 3-channel
data of 1+ mega-pixels images found on image hosting services such as flickr,
twitpic, imgurl, picasa.


\section{Related work}
%Current research is primary concentrating on other tasks. 
%Like ... related work :)
%\begin{description}

\paragraph{Texture synthesis}\cite{Peyre2008}

\paragraph{Vector quantization}

\paragraph{Epitomes} In 2008 Wang et al.\cite{Wang2008a} presented
an approach that factors repeated content in images. The algorithm generates 
a map of affine transformations and an epitome atlas map with ... . The
technique is used for real-time synthesis of textures with low memory
consumption and  fast reconstruction in hardware via pixel shaders. They also
presented some experiments for unified epitomes for collection of images and
possible application for larger sets of images. 
%The algorithm so caled eptimes
%Factoring repeated content\cite{factor_image}

\paragraph{Training with a neural network}
Learning Multiple Layers of Features from Tiny Images \cite{Krizhevsky2009}

\paragraph{Compression of facial images}
In 2008 Bryt and Elad \cite{Bryt2008} 

\paragraph{Multi-scale dictionaries}
Rather than encoding uniform segments of a bigger signal this technique
encodes different sized blocks (multiple times of the default block size). The
idea is that aligned smooth regions with low variance can be can be combined an
encoded as one signal rather than e.g. 4. \cite{saprioSlides}\cite{Mairal2007}

\Todo{picture with different scales}

\paragraph{Multi-channel}
When dealing with multi-channel data (e.g. RGB images) each channel can be coded
separate or as a concatenation of each signal into a single one. These
approaches can lead to color bleeding \cite{mairal08sparse} \Todo{add picture}
Fortunately this problem vanishes with big dictionaries and large training sets.
\cite{mairal08sparse} Nevertheless a solution would be to add a constraint to
the sparse coding that pays attention the correlation of the channels. \Todo{add
formula from paper}

\paragraph{Double sparsity}
Instead of directly using a collection of trained atoms as dictionary
elements. This approach uses sparse coding to encode the signals of a bigger
dictionary with the help of a smaller on.\cite{Rubinstein2009} 
\Todo{formula $B$ .. basis, $A$ sparse matrix, $D=BA$ ...}

\paragraph{Structured sparsity}
similar patches should admit similar patterns \cite{Mairal2009} 
\cite{group sparsity}
\Todo{picture with distribution difference}

\paragraph{Hierarchy}
Adding hierarchy to dictionaries can lead to faster coding of signals and
capture additional information about the learned dictionaries. like in the use
for structure analysis of text documents. Examples for this approach can be
found in\cite{Jenatton2010}.


\Todo{picture with hierarchy }



%\end{description}

