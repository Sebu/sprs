\chapter{Method}
\thispagestyle{empty}

\section{Signal representation}
As we use natural color images and graphics we require to use multi-channel data. Common 3-channel from a RGB color space.

%The initial signal data 
We use an approach to transform our initial signal into other color spaces like in jpeg encoding to account natural human reception of
color data. 
Experiment ....

%\subsection{color/signal representation}

\section{Sparse coding step}
\subsection{Batch-OMP}
A modified version of the OMP\footnote{\nameref{sec:omp}} that can effectively sparse codes multiple signals in parallel.
Pre-computation of gram matrix $G=D^TD$
\cite{Rubinstein2008}

\begin{algorithm}
\caption{Batch-OMP}
\begin{algorithmic}[1]
\REQUIRE $X =[d_1,...,d_k]  \in \mathbb{R}^{m \times k}, D \in \mathbb{R}^{m\times p}, \epsilon \in \mathbb{R}$
\STATE precompute gram matrix: $G=D^TD$
\FOR[all signals in parallel] {$j = 1$ to $k$ }
\STATE $\alpha_j \gets 0, r_j \gets x $ (residual) $, S_j=\emptyset$
\FOR {$i = 1$ to $L$}
\STATE Select atom with maximum correlation with residual: 
\begin{equation*}
i \gets \argmax_{i \in S_j^C} \lvert d_i^Tr_j \rvert
\end{equation*}
\STATE solve with cholesky: $G_{inv} \gets G_S^{-1}$
\STATE update active set: $S_j \gets S_j \cup \{i\} $
\STATE update residual: $r_j \gets \left(I-D_SG_{inv}D_S^T \right)x_j$
\STATE update coefficients: $a_{j_S} \gets G_{inv} D_S^T x_j $

\ENDFOR 
\ENDFOR 

\RETURN $\alpha$
\end{algorithmic}
\end{algorithm}

\subsubsection*{Optimizations}


\subsection{LARS-Lasso}
The complete regularization path based on \cite{Efron2004}
\subsubsection*{Optimizations}


\section{Training step}
Keeping the training sparse vectors too sparse can lead to dictionaries with only low frequency elements.
Using LARS-Lasso for training generates a good dictionary for LARS and OMP. 
Training with OMP leads to a dictionary more with a lot of noise 
that leads better coding results when using OMP 
% is LARS better?

\subsection{Mairal2010}
A more in-depth look at the Mairal on-line training algorithm \ref{sec:mairal}. 
Why use this algorithm? Robust, convergent, fast, on-line


\subsection{Clustering}
\subsubsection*{Optimizations}
%\subsection{Adaptive}
%Thanks to the on-line approach of the training step an adaptive approach can be used. 



\section{Training sets}
Task specific training data is a common way to solve the problem of finding the right dictionary. 
Here learning for the task comes into account. de-noising/in-painting dictionaries directly learned from the initial
signal that gets de-noised or restored from in-painting.
If the task gets bigger it sounds logical to increase the size of training data and take a bigger variety of signals to learn from.



\section{Experiments}
\subsection{Quality comparison}
PSNR, mean square error MSE, 


%\subsection{}

\subsection{Compression}
\subsubsection*{Image conversion}
color space, 
\subsubsection*{Signal coding}
block size, nr. of coefficients
\subsubsection*{Quantization}
Fixed quantization or learned from dictionary
The random distribution of the atoms in the dictionary 
fixed factor or analysis of dictionary
\subsubsection*{Entropy encoding}
RLE, huffman
\subsubsection*{Bits per pixel}
The sparse vector for every are encoded in 2 bytes index and a single byte quantized coefficient. Additional zero coefficients from the quantization step are removed.





\section{Implementation}
%\lstinputlisting[language=C++,caption=Training]{listings/test.cpp}
\Todo{add diagram}

\subsection*{Software}
The whole test suit is written in C++ with the use of OpenCV, LibEigen and OpenMP.
Eigen \cite{Eigen} is a template library for fast vector and matrix operations and includes some linear algebra algorithms.
It is mainly used for operations on dense and sparse matrices and solving of linear equation systems.
OpenCV \cite{OpenCV} as in Open Computer Vision Library is mainly used for image read and write operations and color space conversion.
OpenMP \footnote{www.openmp.org} \cite{OpenMP} is a preprocessor based application programming interface (API) for C,C++ and Fortran that enables 
the user to distribute code block and loops to multiple CPU cores. 
In situations where it is applicable OpenMP is used to utilize multi-core CPUs. 

The OMP implementation is based on the OMP implementation from \cite{Rubinstein}
The implementation of the LARS-Lasso algorithm is based on the implementation of \cite{Strand2005} and the original paper \cite{Efron2004}

Trainer
The Mairal2010 trainer is a straight forward implementation of the algorithm presented in \cite{Mairal2010}.
It is a implementation of basic version of the algorithm with the batch optimization. 
Both coding methods (OMP and LARS-Lasso) can be utilized for the coding step.


\subsection*{Hardware setup}
%Problems
%Big matrices
%Speed 
%Compression
%The 
Computer cluster with 100+ PCs with distribibuted batches of work.
Merge of results -> it's  map/reduce approach.
\Todo{add diagram}


