%\chapter{Method}
%\thispagestyle{empty}

\chapter{Framework}
To sparse code images and learn big dictionaries we designed a software
framework called emph{SPRS}. This section describes the functionality and
decisions made while developing it from idea to implementation.

\section{Signal representation}
\label{sec:signal_representation}
To be able efficiently sparse code images it is necessary to prepare the
image data. The signal preparation mainly consists of two sub steps.
\begin{itemize}
 \item Color space conversion
 \item Spatial separation
\end{itemize}

\paragraph{Color space conversion} As we mainly use sets of natural color images
and graphics we require to work with multi-channel data. Commonly presented in
the three channel RGB color space.
When dealing with multi-channel data (e.g. RGB images) each channel can be coded
separate or as a concatenation of each signal into a single one. These
approaches can lead to color bleeding \cite{mairal08sparse}. \Todo{add picture}
%Nevertheless 
A solution is to add a constraint to the sparse coding that pays
attention to correlation of the channels. \Todo{add formula from paper} 
Fortunately Mairal et al. also noticed that this problem vanishes with big
dictionaries and large training sets as present in our experiments.
\cite{mairal08sparse}


\paragraph{Spatial separation}
As the sparse coding process is operation on signal vectors it is required to
represent our 2-D image data in a set of 1-D vectors with size $m$. We extract
blocks of size $n \times n=m$ from the images. Typically dealing with block
sizes of $n=8,..,20$. The blocks get extracted in a raster either with
overlapping content or disjoint from each other. Figure \ref{fig:separation}
illustrates this separation step. How the each strategy can effect learning
show our experiments.
\begin{figure}[h]
\centering
\subfloat[]{\includegraphics[scale = 0.30]{images/segmentation.jpg}}
\hspace{5mm}
\subfloat[]{\includegraphics[scale =
0.50]{images/segmentation1.jpg}}
\hspace{5mm}
\subfloat[]{\includegraphics[scale =
0.50]{images/segmentation2.jpg}}
\caption[spatial separation]{spatial separation of(a) with (b) and without (c)
overlapping}
\label{fig:separation}
\end{figure}

\section{Sparse coding step}
When coding a large number of small or medium sized decomposition problems,
which is the case for small image blocks in large image sets, some
optimizations can be made on the sparse coding algorithms. 
For the framework we chose to include a batch modified versions of the OMP for
$\ell_0$ regularization and the LARS-Lasso for $\ell_1$ regularization in the
sparse coding step to address this circumstance. 

\subsection{Batch-OMP}
In 2008 Rubinstein et al.\cite{Rubinstein2008} presented a speed improved
version of the orthogonal-matching-pursuit(\ref{sec:omp}) called
\emph{Batch-OMP}. The speed gain of \prettyref{alg:batchOMP} is achieved
by modifying the OMP in a way that can effectively sparse code
multiple signals in parallel. One key element of the optimizations is the
pre-computation of the gram matrix $\mat{G}=\mat{D}^T\mat{D}$ which can
efficiently be reused for each signal. Another optimization to a single signal
coding step itself (\prettyref{alg:batchOMP}) is the cholesky factorization of
$\left( \mat{D}_A^T \mat{D}_A \right)^{-1}$ (line \ref{alg:OMP_DTD} of
\prettyref{alg:mp}). $\mat{D}_A^T \mat{D}_A$ is a symmetric and
positive-definite matrix and can be cholesky decomposed
into $\mat{L}\mat{L}^T$. Even better, due to the nature of the algorithm
composing $\mat{D}_A$ by adding additional row and columns to it, $\mat{L}$ can
be build up during the computation. This reduces the computational cost of the
algorithm drastically.

\begin{algorithm}[h]
\caption{Parallel coding}
\label{alg:parallel}
\begin{algorithmic}[1]
\REQUIRE $\mat{X} =[\vec{x}_1,...,\vec{x}_k]  \in \mathbb{R}^{m \times k},
\mat{D} =[\vec{d}_1,...,\vec{d}_p] \in
\mathbb{R}^{m\times p}, \epsilon \in \mathbb{R}$
%mat{A} = [\alpha_1,...,\alpha_p] \in \mathbb{R}^{p}
\STATE pre-compute $\mat{G} \gets \mat{D}^T\mat{D}$
\STATE pre-compute $\mat{DtX} \gets \mat{D}^T\mat{X}$
\FOR {$i = 1$ to $k$}
\STATE Compute $\vec{\alpha}_i$ of $\mat{A}$ using \prettyref{alg:batchOMP} or
\prettyref{alg:lars} for all $\vec{x}_i$ in $\mat{X}$
\ENDFOR
\RETURN $\mat{A}$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[h]
\caption{Batch-OMP}
\label{alg:batchOMP}
\begin{algorithmic}[1]
\REQUIRE $\vec{x} \in \mathbb{R}^{m}, \mat{G}  \in
\mathbb{R}^{m\times m}, \epsilon \in \mathbb{R}$
\STATE $A \gets \emptyset,\vec{\alpha} \gets 0,\vec{\gamma} \gets
\mat{D}^T\vec{x},\vec{\delta}_0 \gets
0, \epsilon_0\gets \vec{x}^T\vec{x},\mat{L}\gets[1],n\gets1$
\WHILE {$ n<L$ \AND $\epsilon_{n-1} > \epsilon $}
\STATE $k \gets \argmax_i\lvert \vec{\alpha}[i] \rvert$
\IF {$n>1$}
\STATE $\vec{w} \gets \mat{L}^{-1}\mat{G}_{A,i}$
\STATE
\begin{align}
\mat{L} \gets \left[
\begin{array}{ccc}
\mat{L} & 0\\
\vec{w}^T & \sqrt{1-\vec{w}^T\vec{w}}
\end{array}
\right]
\end{align}
\ENDIF
\STATE add variable to active set: $A \gets A \cup \{ i\}$
\STATE $\vec{\alpha}_A \gets (\mat{L}^T)^{-1}\mat{L}^{-1}\vec{\gamma}_A$
\STATE $\vec{\beta} \gets \mat{G}_A\vec{\alpha}_A$
\STATE $\vec{\gamma} \gets \vec{\gamma}-\vec{\beta}$
\STATE $\delta_{n} \gets \vec{\alpha}_A^T\vec{\beta}_A$
\STATE $\epsilon_n \gets \epsilon_{n-1} - \delta_n + \delta_{n-1}$
\STATE $n \gets n+1$
\ENDWHILE
\RETURN $\vec{\alpha}$
\end{algorithmic}
\end{algorithm}

All modification lead to significant speed increase when coding large sets of
signals. While $L$ is very small the main factor of the computation is
the multiplication of $\mat{D}$ with a vector. Batch-OMP reduces the number of
those multiplications compared to the ordinary OMP. Detailed information on the
complexity and speed gain can be found in the corresponding
paper\cite{Rubinstein2008} on the Batch-OMP. 

% with a complexity of $O(mp)$. 
%This modification reduces the complexity of the OMP from $O\left(L2mp + 2L^2m +
%2L(p+m) + L^3\right)$ to $O\left(2mp + L^2p + 3Lp + L^3\right)$ 
%with pre-computation $\left(mp^2\right)$ as shown in\cite{Rubinstein2008}. 
%more signals than dictionary elements.

\paragraph{LARS-Lasso} is chosen as it one of the fastest $\ell_1$
regularization and and it leads to good results for signals of low dimension and
with high correlation of dictionary elements.\cite{Mairal2010} Both conditions
are satisfied when coding small image blocks.

The same pre-computation of the matrix $\mat{D}^T\mat{D}$ and
$\mat{D}^T\mat{X}$ used in the Batch-OMP can also be applied to the LARS-Lasso
algorithm. This also helps to reduce the number of big matrix multiplications
when coding multiple signals in parallel. The framework implements a simple
combination of parallel coding (\prettyref{alg:parallel}) and the
LARS (\prettyref{alg:lars}) presented in \prettyref{sec:lars}.

\section{Learning step}
The majority of learning experiments in literature were made on
samples of sliding blocks from single images or small sets of images.
With samples of $n \ge 100.000$ and approximately about 10 non-zero coefficient.
A common strategy when the learned dictionaries are only used for operations on
those images. The dictionary atoms learned from those image have a high
correlation to the image content.  When looking at large sets of images and many
different blocks we start to talk about a different game. Now each dictionary
elements tends to captures more general structure of the signals.

For the learning step we use the ODL algorithm from \prettyref{sec:mairal}. As
an online learning algorithm it is able to learn dictionaries from very large
sets of training data $n\ge 10.000.000$.


\subsection{Initialization}
At the start of a training process it is requires to initial the
dictionary with start data. Otherwise the sparse coding step will only find
trivial solutions with all coefficient being zero. Which then will have no
effect in the learning process.

There are two common ways to initialize the dictionary using random data or
selecting random elements from the training data. The framework supports 
both ways of dictionary initialization. \Todo{optional: image}
%can be tested and the quality will be compared.


\subsection{Clustering}
\label{sec:clustering}
One reason not to learn big dictionaries in a single step is the fact that
learning such big dictionaries leads to huge memory consumption and coding time.
To address these problems we investigate strategies to distribute the workload
of the learning step onto multiple clients with a clustering approach similar to
MapReduce. 

One strategy would be to use the mini-batch
modification of the LARS-Lasso \prettyref{sec:lars}. But rather
than just coding a small batch of signals in one iteration of the training
step (\prettyref{alg:trainDL}) we code one batch on each client of the cluster
and merge all of them before applying (\prettyref{alg:update}). Keeping the
dictionary in each iteration fixed for every batch coding client. The problem is
that this modifications is only good at handling mini-batches in a range of one
to thousands samples. Impractical for our desired size of millions of samples.

Another strategy is to learn a set of smaller dictionaries $S =
(\mat{D}_1, ..., \mat{D}_k)$ on disjoint sets of training samples and when
the learning step ended, merge the dictionaries $\mat{D}_i$ into a bigger one
$\mat{D}_{s}$ . The merge process starts with an empty dictionary
$\mat{D}_{s}$  and successively adds atoms from the dictionaries of the
learning step to the new dictionary $\mat{D}_{s}$  unless the atom can be
reconstructed with only few atoms (e.g., 2) of the current state of
$\mat{D}_{s}$ . The drawback of this approach is fact that 
each atom will be less correlated to whole training set. But this will happen
anyway as we learn dictionaries from large sets of possibly very different
samples. 

\begin{algorithm}[H]
\caption{Dictionaries merging}
\label{alg:merging}
\begin{algorithmic}[1]
\REQUIRE $ S = (\mat{D}_1, ..., \mat{D}_k) \text{ set of dictionaries } \mat{D}
\in \mathbb{R}^{m\times p}$

\STATE $\mat{D}_{new} \gets 0, n \gets 0$
\FOR {$i = 1$ to $k$}
\FOR {$j = 1$ to $p$}
\STATE sparse code: $\vec{d}_j$ of $\mat{D}_i$ with $\mat{D}_{s}$
\IF {$\lVert\mat{D}_{s}\alpha-\vec{d}_j\rVert_2^2 < \epsilon$}
\STATE add $\vec{d}_j$: $\mat{D}_{s}[n] \gets \vec{d}_{j}$
\STATE $n \gets n+1$
\ENDIF
\ENDFOR
\ENDFOR
\RETURN $\mat{D}_s$
\end{algorithmic}
\end{algorithm}



\section{Application to image compression}
\label{sec:compression}
Experiment on sparse coding as a tool for compression have been done
before\cite{Lewicki1999,Murray2006} but mainly in theory with learned
dictionaries and generated generated dictionaries of wavelets bases. Leading to
better density with learned dictionaries over designed. For comparison of
compression quality only approximations for resulting bits-per-pixel were
made. Real entropy encoding was only applied by Bryt et al.\cite{Bryt2008} in
a very specific algorithm for facial compression. We add quantization and
entropy encoding after the sparse coding step to develop a simple real world
compression algorithm for sparse coding images.

Todays lossy image compression algorithms are primary based on scientific
findings about visual perception reaching far back in the 1970s.
They consits of the following main steps:
% in lossy image compression algorithms:
\begin{itemize}
 \item lossy color space conversion
 \item sub-sampling and spatial separation
 \item transformation of image signals
 \item quantization to reduce of the number of non-zero coefficient 
 \item lossless entropy coding of the coefficient 
\end{itemize}

\paragraph{Color conversion and sub-sampling} The human eye is good at sensing
difference in brightness but lacks the ability exactly differentiate small
changes in color. \emph{Chroma sub-sampling} takes advantage of this human flaw.
First the image gets converted into the $YC_bC_r$ color space, which describes
an image in a brightness/luminance (Y) and two color/chroma components (Cb and
Cr). Then the visual more relevant brightness/luminance channel will be coded in
the full resolution while the color/chroma components can be sub-sampled with
only minor loss in perceptual quality. This procedure is one key element of JPEG
and JPEG 2000 image compression to reduce the number of coefficients for each
color channel. But our atoms already capture color information. So we are
skipping this step. 
%Splitting the image

\paragraph{Spartial separation and cignal coding}
The actual coding step of the prepared signals. The signal vectors $\vec{x}_i$
of $\mat{X}$ consists of $n \times n$ non overlapping blocks from the image.
With $n$ in the range of $n=8,..,12$. A similar block size and separation
strategy as used in JPEG compression. After the signal separation we apply one
of the sparse coding algorithms present in our framework to all blocks. This
step is lossy as the number of coefficients for regularization chosen is lower
than the dimension of the signal. 

\paragraph{Quantization}
Quantized is the process of reducing informations from continuous set to a
discrete set. In the case of image compression from double precision to integer
values. In our framework the resulting coefficient from the coding step get
divided by a factor and rounded to the next integer.  

Quantization can either be applied by a fixed factor or by applying different 
factors to each coefficient based on frequency of the basis element.

JPEG quantization uses the fact that the human eye is not good at seeing
differences in high frequency brightness variations. Each DCT coefficient was
weighted by their perceptual relevance and weights were stored in quantization
matrix. This matrix can be applied to the coefficient after the DCT. 
Afterwards each coefficient gets rounded and zero coefficients can be removed.

We adopt the idea of a quantization matrix to our sparse coding approach and
weight our element by their frequencies. Learning dictionaries with the
algorithm we present in \prettyref{sec:mairal} leads to a random distribution of
atoms with yet unknown information of their structure and frequency. We analysis
the dictionary elements and sort them by their frequencies.
\prettyref{fig:sorted} illustrates an example of this. Weights are applied
in a simple linear way over the number of elements.

\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{images/sorted.png}
\caption{sorted learned dictionary and quantization weights}
\label{fig:sorted}
\end{figure}

\paragraph{Data encoding}
Compression of the remaining coefficient after quantization is the next step
of to reduce the amount of data that needs to be stored.

\begin{description}
 \item[Run-length encoding (RLE)] Is the process of encoding data in blocks of
data value and corresponding run-length of data with the same value. 
For example the sequence {\bf dddddddbbbddddd} becomes {\bf7d3b5d}.
The main requirement to work well is that the data consists of larger groups of
the same
data.
  \item[Huffman coding] Is an algorithm that uses the knowledge about the
entropy of elements of data. Elements that occur often in the data set are
encoded with less bits. Leading in a table of elements and a corresponding
symbol to represent this element and a block of symbols.
\end{description}

We apply something similar to RLE to the sparse matrices. We know that the
majority of coefficient of the spars matrices are zero. We code the sparse
matrices as a series of distances between coefficients and the actual
coefficients. We use the fact that it is more likely that the next value in our
matrix is zero. Afterwards the remaining data gets huffman coded.

Besides the raw pixel data, images formats usually contain a certain amount
of extra data from file headers and meta data. Fortunately we can ignore this
for simplification as it is only a few bytes and not being actual pixel data.

%In 1999 Lewicki et al.\cite{Lewicki1999} already made some on comparison 
%the bpp required for encoding of the sparse matrix data. Besides the estimation
%of the compression we also really applied some coding schemes to compare real
%world compression data.
%\cite{Murray2006}


\section{Implementation}
%\subsection*{Software}
The sofware framework consists of a library (sprscode) for sparse coding image
signals and learning dictionaries, a command line interface (sparsecli) to the
library and miscellaneous scripts for distribution of coding and learning jobs
and some a test suite.

The whole sparse coding and training library is written in C++ with
additional use of the OpenCV, Eigen and OpenMP libs. All operations use double
precision. 

Eigen\footnote{\url{http://eigen.tuxfamily.org/}}
is a template library for fast vector and matrix operations with vector
optimizations and includes some linear algebra algorithms. It is mainly used for
operations on dense and sparse matrices, cholesky factorization and solving of
linear equation systems. 

OpenCV\footnote{\url{http://opencv.willowgarage.com/}} as
in \emph{Open Computer Vision Library} is mainly used for
image read and write operations, image conversion and DCT for the
quantization matrix step. 

OpenMP \footnote{\url{http://www.openmp.org/}} is a preprocessor
based application programming interface (API) for C,C++ and Fortran that enables
the user to distribute code blocks and loops to multiple CPU cores. In
situations where it is applicable OpenMP is used to utilize multi-core CPUs. 


% \Todo{add diagram}
%Split image into sub images, convert sub image to samples from image blocks, 
%train dict with samples or just code samples - save dictionary/image
%Image block size, block selection strategy can be specified.

\paragraph{Coding}
The sparse coding step can be configured to use either the Batch-OMP and 
set a limitation to the maximum number of coefficients or the LARS-Lasso 
and set the regularization(\prettyref{eq:l1}) factor $\lambda$.
The Batch-OMP implementation is based on the implementation
of OMPBox by Ron
 Rubinstein\footnote{\url{http://www.cs.technion.ac.il/~ronrubin/}} with
technical modifications to fit into our
framework. The LARS-Lasso C++ implementation is based on the Matlab
implementation of\cite{Strand2005} and the original
paper\cite{Efron2004} on the algorithm with some additional optimizations
for coding of multiple signals in parallel. 

\paragraph{Learning}
The dictionary learning step is a straight forward C++ implementation of the
ODL algorithm from \prettyref{sec:mairal}. It is an
implementation of the
basic version of the algorithm with the mini-batch optimization applied.
Batch-OMP and the LARS-Lasso coding method can be used for the coding part.
Dictionary size and the number training signals for each iteration can be
speficied.

\paragraph{Command line interface}
The framework provides a command line interface to the library to unleash its
power.
%\lstinputlisting[language=C++,caption=Training]{listings/test.cpp}
\begin{table}[H]
\centering
\begin{tabular}{ |l | l |}
\hline
\multicolumn{2}{|c|}{sparsecli}\\
\hline
parameter & description \\
\hline
--dict <file> & save or load dictionary to \emph{file}\\
--dictSize <n> & size \emph{n} of the dictionary  \\
--train <file> & use images listed in \emph{file} as training set\\
--blockSize <n> & size \emph{n} of the image blocks \\
--winSize <n> & distance \emph{n} of selection window \\
--resume & resume an interrupted learning process \\
--merge <n> & merge dictionaries listed in \emph{file}  \\
--input <file> & input \emph{file} for reconstruction \\
--output & output \emph{file} of reconstruction \\ 
--coeffs <n> & maximum of coefficients \emph{n} \\
--error <n> & reconstruction or merge error \\

\hline
\end{tabular}
\end{table}

\paragraph{Miscellaneous}
Besides the actual coding software some other tools for
additional tasks were used. Several bash and ruby scripts for workload
distribution onto the clients, aggregation and evaluation of the results.
Also he ImageMagick\footnote{\url{http://www.imagemagick.org/}} toolkit was used
for image conversion into JPEG and JPEG 2000 and for some image comparison
tasks.

%\newpage

