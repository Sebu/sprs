\chapter{Method}
%\thispagestyle{empty}

\section{Performance comparison}
Before we present the experiments it is necessary to introduce some metrics to measure the performance of test results.



\paragraph{Root mean squared error (RMSE)} is error measurement that ...

Comparing the a signal $x$ with a reconstruction $y$ (noisy approxiamtion) of it. The root mean squared error is defined as:
\begin{align}
 RMSE = \left[\frac{1}{n} \sum_{i=0}^{n}{\lVert x_i - y_i\rVert^{2}} \right]^{\frac{1}{2}}
\end{align}
It is primarily used for ...


\paragraph{peak signal-to-noise ration (PSNR)}

\Todo{Normalized version of RMSE}

\begin{align}
 PSNR = 20 \cdot \log_{10} \left(\frac{MAX}{RMSE}\right)
\end{align}
Where $MAX$ is the maximum possible value of our signal. 
For an 8-bit image it would be 255. For a 32-bit normalized image it would be 1.
The PSRN is expressed in a logarithmic decibel scale.

Typical values lossy reconstruction are between 20dB and 50dB.


%e.g. relevant for de-noise


\paragraph{Bits per pixel (bpp)}
For comparison of compression ratio of images a well known practice is to measure the bits requires for a single pixel. 
For example an uncompressed RGB color image with 8-bit of color depth requires 24-bits per pixel, respectively 8-bit for a single channel gray scale image.

Looking at other well known compression algorithms such as jpeg or jpeg2000 a common ratio is about ~1.8 bits-per-pixel for average quality compression(Q=50) of a natural image. 
\Todo{example image?}

Besides the actual pixel data there is certain amount of extra data from file headers. 
%The sparse vector for every are encoded in 2 bytes index and a single byte quantized coefficient. Additional zero coefficients from the quantization step are removed.


\section{Signal representation}
As we use natural color images and graphics we require to use multi-channel data. Common 3-channel from a RGB color space.

%The initial signal data 
We use an approach to transform our initial signal into other color spaces like in jpeg encoding to account natural human reception of
color data. 
Experiment ....

%\subsection{color/signal representation}

\section{Sparse coding step}
\subsection{Batch-OMP}
A modified version of the OMP (\ref{sec:omp}) that can effectively sparse codes multiple signals in parallel.
Pre-computation of gram matrix $G=D^TD$
\cite{Rubinstein2008}

\begin{algorithm}
\caption{Batch-OMP}
\begin{algorithmic}[1]
\REQUIRE $X =[x_1,...,x_k]  \in \mathbb{R}^{m \times k}, D  =[d_1,...,d_p]  \in \mathbb{R}^{m\times p}, \epsilon \in \mathbb{R}$
\STATE precompute gram matrix: $G=D^TD$
\FOR[all signals in parallel] {$j = 1$ to $k$ }
\STATE $\alpha_j \gets 0, r_j \gets x_j $ (residual) $, S_j=\emptyset$
\FOR {$i = 1$ to $L$}
\STATE Select atom with maximum correlation with residual: 
\begin{equation*}
i \gets \argmax_{i \in S_j^C} \lvert d_i^Tr_j \rvert
\end{equation*}
\STATE solve with cholesky: $G_{inv} \gets G_S^{-1}$
\STATE update active set: $S_j \gets S_j \cup \{i\} $
\STATE update residual: $r_j \gets \left(I-D_SG_{inv}D_S^T \right)x_j$
\STATE update coefficients: $a_{j_S} \gets G_{inv} D_S^T x_j $

\ENDFOR 
\ENDFOR 

\RETURN $\alpha$
\end{algorithmic}
\end{algorithm}

\subsubsection*{Optimizations}


\subsection{LARS-Lasso}
The complete regularization path based on \cite{Efron2004}
\subsubsection*{Optimizations}


\section{Training step}
Keeping the training sparse vectors too sparse can lead to dictionaries with only low frequency elements.
Using LARS-Lasso for training generates a good dictionary for LARS and OMP. 
Training with OMP leads to a dictionary more with a lot of noise 
that leads better coding results when using OMP 
% is LARS better?

\subsection{\trainDL}
A more in-depth look at the Mairal on-line training algorithm \ref{sec:mairal}. 
Why use this algorithm? Robust, convergent, fast, on-line

\subsection{Initialization}
At the start of the training process it is requires to initial the dictionary with start data.
Otherwise the sparse coding step will only find trivial solutions with all coefficient being zero. Which then will have no effect in the learning process.

There are two common ways to initialize the dictionary using random data or selecting random elements from the training data. Both ways will be tested and 
the quality will be compared.

\section{Training sets}
Task specific training data is a common way to solve the problem of finding the right dictionary. 
Here learning for the task comes into account. de-noising/in-painting dictionaries directly learned from the initial
signal that gets de-noised or restored from in-painting.
If the task gets bigger it sounds logical to increase the size of training data and take a bigger variety of signals to learn from.
\Todo{data: USC-SIPI Image Database, flickr, gogh, disney }

\section{Experiments}

\subsection{Dictionary initialization}
\paragraph{Random data} 
\paragraph{Training elements}


\subsection{Learning specific dictionaries}


\subsection{Clustering the learning step}
\subsubsection*{Optimizations}
%\subsection{Adaptive}
%Thanks to the on-line approach of the training step an adaptive approach can be used. 


\subsection{Image compression}
\paragraph{Signal conversion}
color space, high/low pass filtering

\paragraph{Signal coding}
block sizes
Number of coefficients


\paragraph{Quantization}
Fixed quantization or learned from dictionary
The random distribution of the atoms in the dictionary 
We concentrated on two fixed quantization factor and the analysis of the dictionary elements.

%Copy
%The human eye is good at seeing small differences in brightness over a relatively large area, but not so good at distinguishing the exact strength of a high frequency brightness variation. This allows one to greatly reduce the amount of information in the high frequency components. This is done by simply dividing each component in the frequency domain by a constant for that component, and then rounding to the nearest integer. This rounding operation is the only lossy operation in the whole process if the DCT computation is performed with sufficiently high precision. As a result of this, it is typically the case that many of the higher frequency components are rounded to zero, and many of the rest become small positive or negative numbers, which take many fewer bits to represent.

The jpeg algorithm 
We adopt the idea of visual perception idea of jpeg to our approach and sort our element by their frequencies.

\paragraph{Data encoding}
As mentioned in in \ref{sec:headers} we are aware of the fact that besides the actual sparse matrix data we also have other files informations and meta data.
Coefficients
Indices
In 1999 Lewicki et al. \cite{Lewicki1999} already made some on comparison the bpp required for encoding of the sparse matrix data.
\cite{Murray2006}

\paragraph{Run-length encoding (RLE)}
\paragraph{Huffman coding}
\paragraph{Arithmetic coding}

%The sparse vector for every are encoded in 2 bytes index and a single byte quantized coefficient. Additional zero coefficients from the quantization step are removed.



\section{Implementation}
%\lstinputlisting[language=C++,caption=Training]{listings/test.cpp}
\Todo{add diagram}

\subsection*{Software}
The whole test suit is written in C++ with the use of OpenCV, LibEigen and OpenMP.
Eigen \cite{Eigen} is a template library for fast vector and matrix operations and includes some linear algebra algorithms.
It is mainly used for operations on dense and sparse matrices and solving of linear equation systems.
OpenCV \cite{OpenCV} as in Open Computer Vision Library is mainly used for image read and write operations and color space conversion.
OpenMP \footnote{www.openmp.org \cite{OpenMP}} is a preprocessor based application programming interface (API) for C,C++ and Fortran that enables 
the user to distribute code block and loops to multiple CPU cores. 
In situations where it is applicable OpenMP is used to utilize multi-core CPUs. 

\paragraph{Coding}
The OMP implementation is based on the OMP implementation from \cite{Rubinstein}
The implementation of the LARS-Lasso algorithm is based on the implementation of \cite{Strand2005} and the original paper \cite{Efron2004}

\paragraph{Trainig}
The Mairal2010 trainer is a straight forward implementation of the algorithm presented in \cite{Mairal2010}.
It is a implementation of basic version of the algorithm with the batch optimization. 
Both coding methods (OMP and LARS-Lasso) can be utilized for the coding step.

\paragraph{Misc}
\Todo{Other: imagemagick for conversion of test image -> jpeg,jp2 and comparison of images}


\subsection*{Hardware setup}
%Problems
%Big matrices
%Speed 
%Compression
%The 
Computer cluster with 100+ PCs with distribibuted batches of work.
Merge of results -> it's  map/reduce approach.
\Todo{add diagram}


