\chapter{Method}
%\thispagestyle{empty}

\section{Framework}
To sparse code images and learn big dictionaries we design a software framework

\subsection{Signal representation}
\label{sec:signal_representation}
To be able efficiently sparse code images it is necessary to prepare the
image data. The signal preparation mainly consists of two sub steps.
\begin{itemize}
 \item Color space conversion
 \item Spatial separation
\end{itemize}

\paragraph{Color space conversion} As we mainly use sets of natural color images
and graphics we require to work with multi-channel data. Commonly presented in
the three channel RGB color space.
When dealing with multi-channel data (e.g. RGB images) each channel can be coded
separate or as a concatenation of each signal into a single one. These
approaches can lead to color bleeding \cite{mairal08sparse} \Todo{add picture}
%Nevertheless 
A solution would be to add a constraint to the sparse coding that pays attention
the correlation of the channels. \Todo{add formula from paper} 
Fortunately Mairal et al. also noticed that this problem vanishes with big
dictionaries and large training sets.
\cite{mairal08sparse}




\paragraph{Spatial separation}
As the sparse coding process is operation on signal vectors it is required to
represent our 2-D image data in a set of 1-D vectors with size $m$. We extract
blocks of size $n \times n=m$ from the images. Typically dealing with block
sizes of $n=8,..,20$. The blocks get extracted in a raster either with
overlapping content or disjoint from each other. Figure \ref{fig:separation}
illustrates this separation step.
\begin{figure}[h]
\centering
\includegraphics[scale = 0.25]{images/segmentation.png}
\caption{spatial separation}
\label{fig:separation}
\end{figure}

\subsection{Sparse coding step}
When coding a large number of small or medium sized decomposition problems,
which is the case for small image blocks in large image sets, some
optimizations can be made on the sparse coding algorithms. 
For the framework we chose batch modified versions of the OMP for $\ell_0$
regularization and the LARS-Lasso for $\ell_1$ regularization in the sparse
coding step to address this circumstance. 

\subsubsection{Batch-OMP}
In 2008 Rubinstein et al.\cite{Rubinstein2008} presented a speed improved
version of the orthogonal-matching-pursuit(\ref{sec:omp}) called
\emph{Batch-OMP}. The speed gain of \prettyref{alg:batchOMP} is achieved
by modifying the OMP in a way that can effectively sparse code
multiple signals in parallel. One key element of the optimizations is the
pre-computation of the gram matrix $G=D^TD$ which can efficiently be reused
for each signal. Another optimization to a single signal coding step itself
(\prettyref{alg:batchOMP}) is the cholesky factorization of $\left( D_A^T D_A
\right)^{-1}$ (line \ref{alg:OMP_DTD} of \prettyref{alg:mp}). $D_A^T
D_A$ is a symmetric and positive-definite matrix and can be cholesky decomposed
into $LL^T$. Even better, due to the nature of the algorithm composing $D_A$
by adding additional row and columns to it, $L$ can be build up during the
computation. This reduces the computational cost of the algorithm drastically.

\begin{algorithm}[H]
\caption{Parallel coding}
\label{alg:parallel}
\begin{algorithmic}[1]
\REQUIRE $X =[x_1,...,x_k]  \in \mathbb{R}^{m \times k}, D  =[d_1,...,d_p]  \in
\mathbb{R}^{m\times p}, \epsilon \in \mathbb{R}, \alpha =
[\alpha_1,...,\alpha_p] \in
\mathbb{R}^{p}$
\STATE pre-compute $G \gets D^TD$
%\STATE $\epsilon \gets X^TX$
\FOR {$i = 1$ to $k$}
\STATE Compute $\alpha_i$ using \prettyref{alg:batchOMP} or
\ref{alg:lars} for all ${x_i}$ in $X$
\ENDFOR
\RETURN $\alpha$
\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
\caption{Batch-OMP}
\label{alg:batchOMP}
\begin{algorithmic}[1]
\REQUIRE $x \in \mathbb{R}^{m}, G  \in
\mathbb{R}^{m\times m}, \epsilon \in \mathbb{R}$
\STATE $A \gets \emptyset,\alpha \gets 0,\gamma \gets D^Tx,\delta^0 \gets
0, \epsilon^0\gets x^Tx,L\gets[1],n\gets1$
\WHILE {$ n<L$ \AND $\epsilon^{n-1} > \epsilon $}
\STATE $k \gets \argmax_k\lvert \alpha_k \rvert$
\IF {$n>1$}
\STATE $w \gets L^{-1}G_{A,k}$
\STATE
\begin{align}
L \gets \left[
\begin{array}{ccc}
L & 0\\
w^T & \sqrt{1-w^Tw}
\end{array}
\right]
\end{align}
\ENDIF
\STATE add variable to active set: $A \gets A \cup \{ k\}$
\STATE $\alpha_A \gets (L^T)^{-1}L^{-1}\gamma_A^0$
\STATE $\beta \gets G_A\alpha_A$
\STATE $\gamma \gets \gamma^0-\beta$
\STATE $\delta \gets \alpha_A^T\beta_A$
\STATE $\epsilon^n \gets \epsilon^{n-1} - \delta^n + \delta^{n-1}$
\STATE $n \gets n+1$
\ENDWHILE
\RETURN $\alpha$
\end{algorithmic}
\end{algorithm}


%\subsubsection*{Optimizations}
%\Todo{remove?}
All modification lead to significant speed increase when coding large sets of
signals. While $L$ is very small the main factor of the computation is
the multiplication of $D$ with a vector. Detailed information on the complexity
and speed gain can be found in the corresponding paper\cite{Rubinstein2008} on
the Batch-OMP. 

% with a complexity of $O(mp)$. 
%This modification reduces the complexity of the OMP from $O\left(L2mp + 2L^2m +
%2L(p+m) + L^3\right)$ to $O\left(2mp + L^2p + 3Lp + L^3\right)$ 
%with pre-computation $\left(mp^2\right)$ as shown in\cite{Rubinstein2008}. 

%more signals than dictionary elements.

\paragraph{LARS-Lasso} is chosen as it one of the fastest $\ell_1$
regularization and and it leads to good results for signals of low dimension and
with high correlation of dictionary elements. Both conditions are satisfied
when coding small image blocks.

The same pre-computation of the gram matrix used in the Batch-OMP can also
be applied to the LARS-Lasso algorithm. The framework uses a simple combination
of parallel coding\ref{alg:parallel} and the LARS\ref{alg:lars} presented
in\ref{sec:lars}.

\subsection{Learning step}
The majority of learning experiments in literature were made on
samples of sliding blocks from single images or small sets of images.
A good strategy when the learned dictionaries are only used for operations on
those images. The dictionary atoms learned from those image have a high
correlation to the image content.  When looking at large sets of images and many
different blocks we start to talk about a different game. Now each dictionary
elements tends to captures more general structure of the signals.

For the learning step we use the \trainDL learning algorithm. As an on-line
learning algorithm it is able to learn dictionaries from very large sets of
training data.

Section \ref{sec:signal_representation} already describes that the learning
process is not limited to gray-scale oder RGB image signals. The presented
algorithm can be applied to different kinds of signals. Such as images in other
color spaces, low/high pass filtered sub-images or signals from another domain
like audio or text. 


\subsubsection{Initialization}
At the start of a training process it is requires to initial the
dictionary with start data. Otherwise the sparse coding step will only find
trivial solutions with all coefficient being zero. Which then will have no
effect in the learning process.

There are two common ways to initialize the dictionary using random data or
selecting random elements from the training data. The framework supports 
both ways of dictionary initialization. \Todo{optional: image}
%can be tested and the quality will be compared.


\subsubsection{Clustering}
One reason not to learn big dictionaries in a single step is the fact that
learning such big dictionaries leads to huge memory consumption and coding time.
To address these problems we investigate strategies to distribute the workload
of
the learning step onto multiple clients with a clustering approach similar to
MapReduce. 

One strategy would be to utilize the batch\ref{sec:trainBatchOptimize}
modification of the LARS-Lasso algorithm. Rather than just coding a small batch
of signals in one iteration of the training step\ref{alg:trainDL} we code one
batch on each client of the cluster and merge all of them before
applying\ref{alg:update}. Keeping the dictionary in each iteration fixed
for every batch coding client. The problem is that it is still required 

Another strategy is to learn several smaller dictionaries on disjoint sets of
training data and when the learning step ended, merge the dictionaries
into a bigger one. The merge process starts with an empty dictionary and
successively adds atoms from the dictionaries of the learning step to the
new dictionary unless the atom can be reconstructed with few atoms of the
current state of the new dictionary. The drawback of this approach is fact that 
each atom will be less correlated to whole training set.

\begin{algorithm}[H]
\caption{Dictionaries merging}
\label{alg:merging}
\begin{algorithmic}[1]
\REQUIRE $ S = (D_1,...,D_k) \text{ set of dictionaries } D \in
\mathbb{R}^{m\times p}$

\STATE $D_{new} \gets D_{1}[1], n \gets 0$
\FOR {$i = 1$ to $k$}
\FOR {$j = 1$ to $p$}
\STATE sparse code 
\STATE $n \gets n+1$
\ENDFOR
\ENDFOR
\RETURN $D$
\end{algorithmic}
\end{algorithm}

Taking strategy two even further is to learn smaller dictionaries from different
sets of images or characteristics. Such as paintings, gray-scale sketches and
smooth images animation. For example learning dictionaries for very specific
classes of images can be utilized for classification tasks. Something that can
come in handy when applying it to operations like search in large image
databases.

%\paragraph{1. one dictionary per client}
% -> merge
% -> drawback: less idipendent learning
%\paragraph{2. one chunk of sample per client}
%  -> merge after evey train iteration
% drawback: many samples per train step

\subsection{Application to image compression}
Todays lossy image compression algorithms are primary based on scientific
findings about visual perception reaching far back in the 1970s\cite{?} and
good entropy encoding of the data. Both elements led to to the
following main steps in lossy image compression algorithms:
\begin{itemize}
 \item lossy color space conversion
 \item sub-sampling and spatial separation
 \item transformation of image signals
 \item quantization to reduce of the number of non-zero coefficient 
 \item lossless entropy coding of the coefficient 
\end{itemize}
%\paragraph{Chroma sub-sampling} is used to reduce the amount of data required
%by color information while preserving perceptual image quality. 
The human eye is good at sensing small difference in brightness but lacks the
ability exactly differentiate small changes in color. \emph{Chroma sub-sampling}
takes advantage of this human flaw. First the image gets converted into the
$YC_bC_r$ color space, which describes an image in a brightness/luminance (Y)
and two color/chroma components (Cb and Cr). Then the visual more relevant
brightness/luminance channel will be coded in the full resolution while the
color/chroma components can be sub-sampled with only minor loss in perceptual
quality. This procedure, illustrated in fig. \ref{fig:YCbCr}, is one key element
in lossy image compression. 
\begin{figure}[h]
\centering
%\includegraphics[scale = 0.25]{images/chroma_subsample.pdf}
\includegraphics[scale = 0.25]{images/segmentation.png}
\label{fig:YCbCr}
\caption{RGB $\rightarrow$ YCbCr}
\end{figure}
These steps can also be applied to sparse coding.

Experiment on sparse coding as a tool for compression have been done
before\cite{Lewicki1999,?} but mainly in theory with learned dictionaries and
generated generated dictionaries of wavelets bases. Leading to better density
with learned dictionaries over designed. But no real entropy encoding was
applied. For comparison of compression quality only approximations for
resulting bits-per-pixel were made. We add quantization and entropy encoding
after the sparse coding step to develop a simple real world compression
algorithm for sparse coding images.

%We already describe the first two steps in
%section\ref{sec:signal_representation}. So we will concentrate on the last
%three.
%As learned dictionary elements show also locality in time like , 
%But real practical application is an open question


\paragraph{Signal coding}
The actual coding step of the prepared signals. The signal vectors of $X$
consists of $8\times 8$ non overlapping blocks from the image. The same block
size and separation strategy as used in JPEG compression. After the signal
separation we apply one of the sparse coding algorithms present in our framework
to all blocks. This step is lossy as the number of coefficients for
regularization chosen is lower than the dimension of the signal. 
% an impact on reconstruction quality.


\paragraph{Quantization}
Quantized is the process of reducing informations from continuous set to a
discrete set. In the case of image compression from double precision to integer
values. In our framework the resulting coefficient from the coding step get
divided by a factor and rounded to the next integer.  

Quantization can either be applied by a fixed factor or by applying different 
factors to each coefficient based on frequency of the basis element.

JPEG quantization uses the fact that the human eye is not good at seeing
differences in high frequency brightness variations. Each DCT coefficient was
weighted by their perceptual relevance and weights were stored in quantization
matrix. This matrix can be applied to the coefficient after the DCT. 
Afterwards each coefficient gets rounded and zero coefficients can be removed.

We adopt the idea of a quantization matrix to our sparse coding approach and
weight our element by their frequencies. Learning dictionaries with the
algorithm we present in \ref{sec:mairal} leads to a random distribution of atoms
with yet unknown information of their structure and frequency. We analysis the
dictionary elements and sort them by their frequencies. Figure 
\ref{fig:sorted} illustrates an example of this.

%copy
%The human eye is good at seeing small differences in brightness over a
%relatively large area, but not so good at distinguishing the exact strength of
%a high frequency brightness variation. This allows one to greatly reduce the
%amount of information in the high frequency components. 
%Copy
%The human eye is good at seeing small differences in brightness over
%a relatively large area, but not so good at distinguishing the exact strength
%of a high frequency brightness variation. This allows one to greatly reduce the
%amount of information in the high frequency components. This is done by simply
%dividing each component in the frequency domain by a constant for that
%component, and then rounding to the nearest integer. This rounding operation is
%the only lossy operation in the whole process if the DCT computation is
%performed with sufficiently high precision. As a result of this, it is
%typically the case that many of the higher frequency components are rounded to
%zero, and many of the rest become small positive or negative numbers, which
%take many fewer bits to represent.

\begin{figure}[h]
\centering
\includegraphics[width = 0.75\textwidth]{images/sorted.png}
\caption{sorted learned dictionary and quantization weights}
\label{fig:sorted}
\end{figure}

\paragraph{Data encoding}
Compression of the remaining coefficient after quantization is the next step
of to reduce the amount of data that needs to be stored.
%The sparse vector for every are encoded in 2 bytes index and a single byte
%Coefficients Indices
%In 1999 Lewicki et al.\cite{Lewicki1999} already made some on comparison 
%the bpp required for encoding of the sparse matrix data. Besides the estimation
%of the compression we also really applied some coding schemes to compare real
%world compression data.
%\cite{Murray2006}
\begin{description}
 \item[Run-length encoding (RLE)] Is the process of encoding data in blocks of
data value and corresponding run-length of data with the same value. The main
requirement to work well is that the data consists of larger groups of the same
data.
  \item[Huffman coding] Is an algorithm that uses the knowledge about the
entropy of elements of data. Elements that occur often in the data set are
encoded with less bits. Leading in a table of elements and a corresponding
symbol to represent this element and a block of symbols.
%\item[Arithmetic coding] is ...
%While arithmetic coding often yields better compression results than Huffman
%coding. The higher processing requirements for coding and the current patent
%situation makes the algorithm unpopular for coding compared to the archived
%compression benefit.
\end{description}

We apply something similar to RLE to the sparse matrices. We know that the
majority of coefficient of the spars matrices are zero. We code the sparse
matrices as a series of distances between coefficients and the actual
coefficients. We use the fact that it is more likely that the next value in our
matrix is zero. Afterwards the remaining data gets huffman coded. 

%The sparse vector for every are encoded in 2 bytes index and a single byte 
%quantized coefficient. Additional zero coefficients from the quantization step
%are removed.

As mentioned in \ref{sec:headers} we are aware of the fact that besides
the actual sparse matrix data we also have other file informations and meta
data but ignore them for simplification.
  
\subsection{Implementation}
%\subsection*{Software}
The sofware framework consists of a library for sparse coding image signals 
and learning dictionaries, a command line interface to the library and
miscellaneous scripts for distribution of coding and learning jobs and
some a test suite.

The whole sparse coding and training library is written in C++ with
additional use of the OpenCV, Eigen and OpenMP libs. All operations use double
precision. 

Eigen\footnote{\url{http://eigen.tuxfamily.org/}}
is a template library for fast vector and matrix operations with vector
optimizations and includes some linear algebra algorithms. It is mainly used for
operations on dense and sparse matrices, cholesky factorization and solving of
linear equation systems. 

OpenCV\footnote{\url{http://opencv.willowgarage.com/}} as
in \emph{Open Computer Vision Library} is mainly used for
image read and write operations, image conversion and DCT for the
quantization matrix step. 

OpenMP \footnote{\url{http://www.openmp.org/}} is a preprocessor
based application programming interface (API) for C,C++ and Fortran that enables
the user to distribute code blocks and loops to multiple CPU cores. In
situations where it is applicable OpenMP is used to utilize multi-core CPUs. 

\Todo{add diagram}
%Split image into sub images, convert sub image to samples from image blocks, 
%train dict with samples or just code samples - save dictionary/image
%Image block size, block selection strategy can be specified.

\paragraph{Coding}
The sparse coding step can be configured to use either the Batch-OMP and 
set a limitation to the maximum number of coefficients or the LARS-Lasso 
and set the regularization(\ref{eq:l1}) factor $\lambda$.
The Batch-OMP implementation is based on the implementation
of OMPBox by Ron
 Rubinstein\footnote{\url{http://www.cs.technion.ac.il/~ronrubin/}} with
technical modifications to fit into our
framework. The LARS-Lasso C++ implementation is based on the Matlab
implementation of\cite{Strand2005} and the original
paper\cite{Efron2004} on the algorithm.

\paragraph{Learning}
The dictionary learning step is a straight forward C++ implementation of the
ODL algorithm from section \ref{sec:mairal}. It is an
implementation of the
basic version of the algorithm with the mini-batch optimization applied.
Batch-OMP and the LARS-Lasso coding method can be utilized for the coding part.
Dictionary size and the number training signals for each iteration can be
speficied.

\paragraph{Command line interface}
%\lstinputlisting[language=C++,caption=Training]{listings/test.cpp}

\paragraph{Miscellaneous}
Besides the actual coding software some other tools for
additional tasks were used. Several bash and ruby scripts for workload
distribution onto the clients, aggregation and evaluation of the results.
Also he ImageMagick\footnote{\url{http://www.imagemagick.org/}} toolkit was used
for image conversion into JPEG and JPEG 2000 and for some image comparison
tasks.


\section{Experiments}

\subsection{Performance measurement}
Before we go further and present the experiments and their results  it is
necessary to introduce some additional terms. Such as metrics to measure
their performance.

\paragraph{Mean squared error (MSE)} is an error measurement that
describes the error between an given or measured reference signal $x$
and and the reconstruction(noisy approximation) $\tilde{x}$ of it.
Mathematically it is defined as the average of the squared error between two
signals.
%The mean squared error is defined as:
\begin{align}
 MSE = \frac{1}{n} \sum_{i=0}^{n} \left( {\lVert x_i -
\tilde{x}_i\rVert^{2}}\right)
\end{align}
We primarily use it for testing dictionary learning convergence and as a
sub term for the measurement described in the next paragraph.

\paragraph{Peak signal-to-noise ration (PSNR)} describes the ratio between the
noise affecting a signal and the maximum possible signal amplitude. It is
expressed in a logarithmic decibel scale.
\begin{align}
 PSNR = 20 \cdot \log_{10} \left(\frac{MAX}{\sqrt{MSE}}\right)
\end{align}
Where $MAX$ is the maximum possible value of our signal. For an 8-bit
image it would be 255. For a 32-bit normalized image it would be 1. And $MSE$ is
the mean squared error between a reference signal and its reconstruction. The
PSNR is undefined for zero noise.

The PSNR is primarily used for comparison of the reconstruction quality of
lossy compression algorithms. Typical values for a lossy reconstruction lie in
a range between 30dB and
50dB.\footnote{\url{http://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio}}
%e.g. relevant for de-noise

\paragraph{Bits per pixel (bpp)} 
For the comparison of compression ratio of images another well known practice is
to measure the required \emph{bits per pixel} short bpp. The bbp are calculated
by dividing the raw image data by the image's dimensions. For example an
uncompressed RGB color image with 8-bit of color depth requires 24-bits per
pixel, respectively a gray scale image with 8-bit for a single channel requires
8-bits. While compression algorithms are able to encode multiple pixels with few
coefficient leading to much lower bpp rates.
Looking at other well known compression algorithms such as JPEG or
JPEG 2000 a common ratio is about $\sim1.8$ bits-per-pixel for average
quality compression( JPEG quality of 50) of an natural image. 

\Todo{example image?, lower bit rates, Lewicki estimate for sparse coding}

Besides the raw pixel data, images formats usually contain a certain amount
of extra data from file headers and meta data. Fortunately we can ignore this
for simplification as it is only a few bytes and not being actual pixel data.

\paragraph{Test data}
In addition to the introduced metrics it is common practice to use some image
sets to compare test results of the different algorithms and parameter
configurations. As the dictionaries are specificaly trained for
reconstruction of the data in the training sets it is mandatory to also use
these image to test the reconstruction and compression quality
(\prettyref{fig:database_images}).
\begin{figure}[h]
\centering
\subfloat{\includegraphics[width = 0.3\textwidth]{images/28979823.jpg}}
\hspace{5mm}
\subfloat{\includegraphics[width = 0.3\textwidth]{images/29018694.jpg}}
\hspace{5mm}
\subfloat{\includegraphics[width = 0.3\textwidth]{images/28874882.jpg}}
\hspace{5mm}
%\subfloat{\includegraphics[width = 0.3\textwidth]{images/28848380.jpg}}
%\hspace{5mm}
%\subfloat{\includegraphics[width = 0.3\textwidth]{images/28859439.jpg}}
%\hspace{5mm}
\subfloat{\includegraphics[width = 0.3\textwidth]{images/28803842.jpg}}
\hspace{5mm}
\subfloat{\includegraphics[width = 0.3\textwidth]{images/28894495.jpg}}
\hspace{5mm}
\subfloat{\includegraphics[width = 0.3\textwidth]{images/28952841.jpg}}
\caption{test images from training set}
\label{fig:database_images}
\end{figure}
In addition to this we want to know how good the dictionaries can reconstruct
and compress images outside of the database respectifly the training set. For
those comparisons we use a well known set of standard test images from the
\emph{USC-SIPI Image
Database}\footnote{\url{http://sipi.usc.edu/database/}}
(\prettyref{fig:USC-SIPI}). They are often used in image processing for
evaluation of compression algorithms. 
%Including pictures such as Lena, Mandrill and Peppers.
\begin{figure}[h]
\centering
\subfloat{\includegraphics[width = 0.3\textwidth]{images/4_1_05.jpg}}
\hspace{5mm}
\subfloat{\includegraphics[width = 0.3\textwidth]{images/4_2_03.jpg}}
\hspace{5mm}
\subfloat{\includegraphics[width = 0.3\textwidth]{images/4_2_04.jpg}}
\hspace{5mm}
\subfloat{\includegraphics[width = 0.3\textwidth]{images/4_2_05.jpg}}
\hspace{5mm}
\subfloat{\includegraphics[width = 0.3\textwidth]{images/4_2_06.jpg}}
\hspace{5mm}
\subfloat{\includegraphics[width = 0.3\textwidth]{images/4_2_07.jpg}}
\caption{test images from USC-SIPI Image Database}
\label{fig:USC-SIPI}
\end{figure}

\subsection{Hardware setup} 
Computations were made on Apple MacPro with ... \Todo{mac config}
and a cluster consisting of 100 nodes with each AMD Athlon X2 \Todo{athlon
config} running Debian 32-bit. Each client in the cluster calculated batches of
100 till 10,000 images based on the size of the training sets.

\subsection{Training sets}
%\subsection{Learning specific dictionaries}
Section \ref{sec:learnForTheTask} mentions that one of the key
elements of the learning algorithms is the learning of specialization
dictionaries. Task specific training data is a common way to solve the problem
of finding  the right dictionary. Here learning for the task comes into
account. Such as de-noising or in-painting dictionaries directly learned from
the initial signal that gets de-noised or restored from in-painting. If the task
gets bigger it sounds logical to increase the size of training data and take a
bigger variety of signals to learn from.  We have a closer look at differences
of learned elements from specific sets of images. These sets include sketches,
still images of animations from Disney and post-impressionistic images from
Vincent van Gogh.  

\subsection{Questions}
One of the main questions
is the OLD paper $\ell_1$ regularization is used does $\ell_0$ also performe
good? All test will be made with OMP and Lasso runs.

Unless we want to able to reconstruct a large variety of images.	

In the first itrations the learning algorithm can take big steps.
Most of the experiments in on small sets of 
See the differences in the selection sceme.


!Test element size inside outside reconstruction

\paragraph{Dictionary size}
!test
We want to know if reconstruction quality increases with dictionaries
size. Linear? (test via MSE)


\paragraph{Convergence}
Depended on block size, dict size, average nuber of coefficients in the
learning process.
Of block size, dict size, 
Reconstruction with different block size.
Rather than Convergence of the learning step. 
Block size of the atoms
Coefficients per signal code respectively $\lambda$
Dictionary size 
Can convergence be addressed by 
Overcomp reconstruction convergence by clustering.
Comparison of the quality of dictionaries learned via cluster strategy and
with normal single machine were made.

\paragraph{Structure}
How do the atoms look like? do they resemble known structures?
How do they change with size?
Are big image collections to general or specific enough to benefit from sparse
coding?
what show Other signals?

\paragraph{Compression}
How do we compare tor JPEG and JPEG 2000?
Can we also compress images from outside of the training data?


